--- 
title: "MATH 376: Algebraic Structures"
site: bookdown::bookdown_site
output:
  bookdown::gitbook:
    config:
      sharing:
        facebook: false
        twitter: false
documentclass: book
link-citations: yes
github-repo: Tom-Halverson/math376_s21
description: "This is the class activity manual for MATH 376 at Macalester College."
---


**Macalester College**    
**Spring 2021**

# Preface {-}



This is the class handbook for Math 376 Algebraic Structures at Macalester College. The content here was made by [Tom Halverson](https://www.macalester.edu/~halverson/) in the [Department  of Mathematics, Statistics and Computer Science](https://www.macalester.edu/mscs/) at [Macalester College](https://www.macalester.edu/).

<br>
<br>
<br>

![](https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png)

This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).

<!--chapter:end:index.rmd-->

# (PART) LineaR Algebra {-}

# Linear Systems in R

## Getting started with R

To use RStudio, you have two choices:

1) Use the cloud version by logging in to  [Rstudio.macalester.edu](https://rstudio.macalester.edu/). This is the easiest way to use RStudio and works great for our course. 

2) You can also download the free desktop version of RStudio. If you plan to go on to take more MSCS classes, especially in statistics and data science, you may want to use the desktop version. Download the desktop version following the instructions here: [rstudio.com/products](https://rstudio.com/products/rstudio/).

Now, let's learn how to use R to solve systems of linear equations! [Download this Rmd file.](https://github.com/mathbeveridge/math236_f20/blob/main/01-linear-systems.Rmd) First, we will create vectors and matrices  Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form.

Gaussian elimination is performed by the `rref()` command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the **practical math** package, which is known as `pracma`. 
So we need to run the following command once at the beginning of our session.
```{r, echo=TRUE}
require(pracma)
```

## Building Vectors and Matrices

A **vector** in R is a list of data. The simplest way to create a vector is to use the `c()` command. The letter 'c' is short for 'combine these values into a vector.' For example, we can make a vector `v` for the numbers 1,2,3 as follows:

```{r, echo=TRUE}
v=c(1,2,3)
v
```
Note that we had to ask R to display the value of `v`. This is because the assignment of `v` doesn't echo the value to the console. But can see the value of `v` in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of `v` gets updated in the environment.

```{r, echo=TRUE}
v=c(1,2,3,4,5,6)
```

It is interesting to note that `c()` returns a **dimensionless vector**. So 
you can treat a vector `c()` as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix
$$
A = \begin{bmatrix}
1 & 1 & 1 \\
2 & 4 & 8 \\
3 & 9 & 27
\end{bmatrix}.
$$
We could create this matrix by binding three row vectors:
```{r, echo=TRUE}
A = rbind(c(1,1,1), c(2,4,8), c(3,9,27))
A
```
or we could bind three column vectors:
```{r, echo=TRUE}
A = cbind(c(1,2,3), c(1,4,9), c(1,8,27))
A
```


## Solving a Linear System 

Suppose that we want to solve the linear system 
\begin{aligned}
x + y + z &= 7 \\
2x + 4y + 8z &= 6 \\
3x +9y+27z &=12
\end{aligned}
which has coefficient matrix 
$$
A = \begin{bmatrix}
1 & 1 & 1 \\
2 & 4 & 8 \\
3 & 9 & 27
\end{bmatrix}.
$$
and target (column) vector
$$
b = \begin{bmatrix}
4 \\ 6 \\ 12
\end{bmatrix}.
$$
This is the same matrix `A` we defined above. Let's define a vector b and use `cbind()` to create an augmented matrix which we will name `Ab`.
(We could have just made the full augmented matrix from the start, but using `cbind` to add a column to a matrix is a skill we will use later in the course!) 

```{r, echo=TRUE}
A
b = c(4,6,12)
Ab = cbind(A,b)
Ab
```

Now we use the `rref()` command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the `require(pracma)` command above.)

```{r, echo=TRUE}
rref(Ab)
```

We conclude that this is a consistent system no free variables. The unique solution is 
\begin{align}
x&=7\\
y&=-4\\
z&=1
\end{align}


We can **verify** that our answer works by multiplying $A$ by one of the solutions above. Matrix multiplication uses the funny operation `%*%`.
```{r, echo=TRUE}
A %*% c(7,-4,1)
#A %*% c(1,2,3,0,0)
```
Which matches our target 
$$
b = \begin{bmatrix}
4 \\ 6 \\ 12
\end{bmatrix}
$$ 
just as we had hoped.


## Solving another Linear System

Now let's find the solution set for the  linear system
$$
\begin{array}{rrrrrcr}
x_1 & & -x_3 & -x_4 & -x_5 & = & -2 \\
2x_1 & +x_2 & +2x_3 & -x_4 & -x_5 & = & 4 \\
-x_1 & +x_2 & +x_3 &  &  & = & 10 \\
x_1 & & -x_3 & -x_4 & -x_5 & = & -2 \\
\end{array}
$$
which corresponds to augmented matrix
$$
\left[
\begin{array}{rrrrr|r}
1 & & -1 & -1 & -1  & -2 \\
2 & +1 & +2 & -1 & -1  & 4 \\
-1 & +1 & +1 &  &  & 10 \\
1 & & -1 & -1 & -1 & -2 \\
\end{array}
\right]
$$


This time, let's just construct the augmented matrix direclty.

Then we define the coefficient matrix $A$. Here we use `cbind` to combine the vectors into the columns of a matrix named $A$. You can use `rbind` if you want to combine the vectors into the rows of a matrix.
```{r, echo=TRUE}
Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2))
Ab
```

And now let's row reduce to get RREF.

```{r, echo=TRUE}
rref(Ab)
```
So the set of solutions in parametric form is

$$
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix}
= 
\begin{bmatrix} 1 \\ 2 \\ 3 \\ 0 \\ 0 \end{bmatrix}
+ s \begin{bmatrix} 0 \\ 1 \\ -1 \\ 1 \\ 0 \end{bmatrix}
+ t \begin{bmatrix} -1 \\ 1 \\ -2 \\ 0 \\ 1 \end{bmatrix}
$$

and this is a "plane" in $\mathbb{R}^5$. It is in $\mathbb{R}^5$ because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 





##  Appendix: Dimensionless Vectors  in R

Let's revisit the vector constructed by `cbind`. Above we called this  a "dimensionless" vector because it can be used as a column vector or a row vector.  In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote `c()` to make an expression valid.

For example, let $A$ be an $n \times  n$ matrix, and let $b$ be a vector.  The expression $Av$ is only defined when $v$ is a $n \times 1$ **column vector** and that $wA$  is only defined when  $w$ is a $1 \times n$ ** row vector**. But let's look at what happens when we use a dimensionless vector instead.


```{r, echo=TRUE}
A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1))
A
b = c(2,5,11)
b

# A times b
A %*% b
# b times A
b %*% A

```


Both of these multiplications worked! So  R **treated `b` as a column vector** for the multiplication`A %*% b`. And then R **treated `b` as a row vector** for the multiplication `b %b% A`. 

So how do you make a true **column vector** or a true **row vector**? The answer is to use `cbind` and  `rbind`! Here are some examples:

```{r, echo=TRUE}
#  dimensionless
b = c(1,2,3,4)
b
# column vector
b.col = cbind(b)
b.col
# row vector
b.row = rbind(b)
b.row
```






<!--chapter:end:01-linear-systems.Rmd-->


# Linear Dependence

In this activity, we will explore linear dependence and independence in the context of solving nonhomogeneous $A x = b$ and homogeneous equations $A x = 0$. [Download this Rmd file.](https://github.com/Tom-Halverson/math236_s21/blob/main/02-linear-dependence.Rmd)


Remember that we will use the `pracma` package to get the `rref` function, so we first load it in:
```{r,echo=TRUE}
require("pracma")
```

## Example 1: a 7x9 integer matrix

Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back.
```{r, echo=TRUE}
A = cbind(
  c(3, 0, 0, 1, -2, -4, 1), 
  c(5, -5, 0, 3, 3, 1, 4), 
  c(3, 5, -1,  1, -3, -3, 5), 
  c(4, -1, -2, 0, -1, 2, -3), 
  c(0, 17, 3, 0, -17, -29,  8), 
  c(-4, -1, -5, -2, -1, -4, 3), 
  c(5, 3, -4, -5, -2, -3, -1), 
  c(0, 5, -3, -2, -1, -5, 0),
  c(37, -10, -27, -29, 4, 7, -24))
A
```

And here is a vector b that we hope to use in solving A x = b.

```{r,echo=TRUE}
b = c(382, 51, -321, -314, -86, -170, -153)
b
```

You can augment A with b, and call it Ab, using `cbind`:

```{r,echo=TRUE}
Ab = cbind(A,b)
Ab
```

And row reduce using `rref`
```{r,echo=TRUE}
rref(Ab)
```

### Solution to the nonhomogeneous equations Ax = b

Write out the solution to Ax=b in *parametric* form using the following formatting. You just need to fill in the correct values of the vectors:
$$
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8 \\ x_9 \end{bmatrix}
=
\begin{bmatrix} 8 \\ 10 \\ -19 \\ 21 \\ 0 \\ 6 \\ 61 \\ 8 \\ 0 \end{bmatrix}
+ s
\begin{bmatrix} 
-5 \\ 2 \\ -1 \\ 2 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 
\end{bmatrix}
+ t
\begin{bmatrix} 
2 \\ -2 \\ 3 \\ -3 \\ 0 \\ 0 \\ -6 \\ 0 \\ 1 
\end{bmatrix}
$$

Describe this solution space (by fixing up this sentence, which is incorrect right now): 


the set of solutions to A x= b  is a *plane* in $\mathbb{R}^9$.


### Solution to the nonhomogeneous equations Ax = 0

Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this:
$$
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8 \\ x_9 \end{bmatrix}
=
\begin{bmatrix} p1 \\ p2 \\ p3 \\ p4 \\ p5 \\ p6 \\ p7 \\ p8 \\ p9 \end{bmatrix}
+ s
\begin{bmatrix} 
u1 \\ u2 \\ u3 \\ u4 \\ u5 \\ u6 \\ u7 \\ u8 \\ u9 
\end{bmatrix}
+ t
\begin{bmatrix} 
v1 \\ v2 \\ v3 \\ v4 \\ v5 \\ v6 \\ v7 \\ v8 \\ v9 
\end{bmatrix}
$$

And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0.

your answer here

### Linearly dependent columns

The columns of the matrix A are *linearly dependent.* You can see that in rref(A). 

```{r,echo=TRUE}
rref(A)
```


Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation

$$
0 = 
c_1 \vec{a}_1 + 
c_2 \vec{a}_2 +
c_3 \vec{a}_3 +
c_4 \vec{a}_4 +
c_5 \vec{a}_5 +
c_6 \vec{a}_6 +
c_7 \vec{a}_7 +
c_8 \vec{a}_8 +
c_9 \vec{a}_9.
$$
**Challenge**: give a dependency relation that none of the other groups in the class have.

This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are *linearly independent.* You can do this by removing the appropriate columns from the code below:

```{r,echo=TRUE}
M = cbind(    # you need to edit this matrix
  c(3, 0, 0, 1, -2, -4, 1), 
  c(5, -5, 0, 3, 3, 1, 4), 
  c(3, 5, -1,  1, -3, -3, 5), 
  c(4, -1, -2, 0, -1, 2, -3), 
  c(0, 17, 3, 0, -17, -29,  8), 
  c(-4, -1, -5, -2, -1, -4, 3), 
  c(5, 3, -4, -5, -2, -3, -1), 
  c(0, 5, -3, -2, -1, -5, 0), 
  c(37, -10, -27, -29, 4, 7, -24)
  )
M
rref(M)
```

Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, `solve`, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. 

```{r,echo=TRUE}
# solve(M,b)
```

Now, you should get a *unique* solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?).

Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it.

```{r,echo=TRUE}
# solve(A,b)
```

## A 5 x 6 Numerical Matrix

So far, all of the matrices we've worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might  find when dealing with real-world data. 

```{r,echo=TRUE}
B = cbind(
  c(0.717, -0.274, 0.365, 0.482, -0.362), 
  c(0.587, -0.545, 0.5, -0.407, -0.597), 
  c(-0.441, 0.886, 0.784, -0.831, -0.594), 
  c(0.923, -0.466, 0.222, 0.867, 0.493), 
  c(-0.42, -0.745, -0.02, -0.44, 0.209), 
  c(0.621, 0.049, -0.134, -0.844, -0.31)
  )
B
```
and here is a vector d in $\mathbb{R}^5$.

```{r,echo=TRUE}
d = c(5.886, -4.001, 3.701, -6.621, -2.199)
d
```

Try answering some of these questions:

a. Are the columns of B linearly independent?
b. Do the columns of B span $\mathbb{R}^5$?
c. Give the parametric solution to B x = d.
d. What is the geometric form of this solution (e.g., a plane in $\mathbb{R}^4$)?
e. Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get.


## Random Matrices

The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. 

```{r,echo=TRUE}
R1 = matrix(runif(5*5), nrow = 5, ncol = 5)
R1
```

Try the same using the following code that generates a random 5 x 6 matrix. 

```{r,echo=TRUE}
R2 = matrix(runif(5*6), nrow = 5, ncol = 6)
R2
```

Try the same using the following code that generates a random 5 x 4 matrix. 

```{r,echo=TRUE}
R3 = matrix(runif(5*4), nrow = 5, ncol = 4)
R3
rref(R3)
```

In each of these cases, how likely is it that the columns of the matrix spans all of $\mathbb{R}^4$?

<!--chapter:end:02-linear-dependence.Rmd-->


# 2D Linear Transformations

Let's explore linear transformations of the plane! [Download this Rmd file.](https://github.com/mathbeveridge/math236_f20/blob/main/02-linear-transformations.Rmd)

We know that a linear transformation $T$ satisfies
$$
T(c \mathsf{u} + d \mathsf{v}) = c\, T( \mathsf{u}) +d \, T( \mathsf{v}).
$$
But what do linear transformations look like? Let's start to answer this question by considering linear transformations of the plane. 

We will look at mappings $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ where $T(\mathsf{x}) = \mathsf{A} \mathsf{x}$ for a $2 \times 2$ matrix $\mathsf{A}$.
$$
T \left( \begin{bmatrix} x \\ y \end{bmatrix} \right) = 
\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}.
$$
The linear transformation $T(\mathsf{x})$ maps the plane to itself. This nicely allows us to compare vectors before the mapping with their images after the mapping. We can then describe the effect of the mapping on the plane.


## Functions to plot our vectors

Here are a couple of helper functions to plot our "before" and "after" vectors as arrows in the plane. 


```{r, echo=TRUE}

plot_four_vectors <- function(before1, before2, after1, after2){
  a0 = c(0,0, before1[1], before2[1])
  b0 = c(0,0, before1[2], before2[2])
  a1 = c(before1[1],before2[1],before1[1]+before2[1], before1[1]+before2[1])
  b1 = c(before1[2],before2[2],before1[2]+before2[2], before1[2]+before2[2])
  c0 = c(0,0, after1[1], after2[1])
  d0 = c(0,0, after1[2], after2[2])
  c1 = c(after1[1],after2[1],after1[1]+after2[1], after1[1]+after2[1])
  d1 = c(after1[2],after2[2],after1[2]+after2[2], after1[2]+after2[2])
  max_val = max(a0,b0,a1,b1,c0,d0,c1,d1)
  min_val = min(a0,b0,a1,b1,c0,d0,c1,d1)
  
  plot(NA, xlim=c(min_val*1.5,max_val*1.5), ylim=c(min_val,max_val), xlab="X", ylab="Y", )
  abline(v=0, col="gray")
  abline(h=0, col="gray")
  vecs <- data.frame(vname=c("a","b","a+b", "transb"), 
                     x0=a0, y0=b0, x1=a1 ,y1=b1, 
                     col=1:4)
  with( vecs, mapply("arrows", x0, y0, x1,y1,col=col, lty=2) )
    vecs <- data.frame(vname=c("a","b","a+b", "transb"), 
                     x0=c0, y0=d0, x1=c1, y1=d1, 
                     col=1:4)
  with( vecs, mapply("arrows", x0, y0, x1,y1,col=col) )

}
```


This function has four parameters. The parameters `before1` and `before2` are two vectors before the mapping, and `after1` and `after2` are their images after the mapping. The function creates a dashed plot of the  parallelogram with corners (0,0), `before1`, `before2` and `before1+before2`. It also creates a solid plot of the image of this parallelogram. These are plotted on the same plane so taht we can easily compare "before" and "after."

Here is an example. Let's start with vectors $\begin{bmatrix} 3 \\ 0 \end{bmatrix}$ and $\begin{bmatrix} 0 \\ 2 \end{bmatrix}$. It's nice to start with vectors of different lengths. Let's consider the mapping correspoding to multiplication by the matrix
$$
A = \begin{bmatrix} 2 & 3 \\ 4 & -1 \end{bmatrix}.
$$
Here is some example code showing how to make our plot. Note: `A %*% x` is the syntax for matrix multiplication in R. 


```{r, example-2d-transformation, results = FALSE, fig.cap="An example 2D linear transformation", echo=TRUE}

vec1 = c(3,0)
vec2 = c(0,2)

A = cbind(c(2,1), c(4,-1))

newvec1 = A %*% vec1
newvec2 = A %*% vec2

plot_four_vectors(vec1, vec2, newvec1, newvec2)
```

This visualization uses different colors for the vectors so that you can match up the original vector with its image. There is a lot going on in this mapping, so let's start making some observations.

* The original rectangle mapped to a parallelogram. So the shape is ``squished'' a bit.
* Both the black vector and the red vector have gotten larger. But we can see that the red vector has grown much more than the black one. So there is expansion, but it is not uniform.
* The black vector and the red vector have flipped! This means that there is some sort of reflection happening. 

There isn't a simple description for what's happening here. It's a combination of effects, so that the image of the rectangle is a warped version of the original.


## Your Turn

Now it's your turn. Investigate the effect of each of the following families of mappings.

* Using the previous code snippet as a guide, create a "before and after plot" for the black vector $\begin{bmatrix} 3 \\ 0 \end{bmatrix}$ and the red vector $\begin{bmatrix} 0 \\ 2 \end{bmatrix}$
* **Describe** the effect of the mapping as best you can. Be sure to look at the different effect on the black vector and the red vector. Use words like, expansion, contraction, rotation, reflection and [shear](https://en.wikipedia.org/wiki/Shear_force#/media/File:Simple_shear_in_2D.svg).
* Once you have looked at the effect of the family, look back at the form of the matrix $A$. Can you explain why it leads to the outcome you see?

### Family 1

Explore matrices of the form 
$$A=\displaystyle{ \begin{bmatrix} a & 0 \\ 0 & b \end{bmatrix}}.$$
Once again, try various combinations of positive, negative, small and large  numbers. Here is some sample code that would create such a matrix for $a=1$ and $b=1$.

```{r, echo=TRUE}
a = 1
b = 1

A = cbind(c(a,0), c(0,b))
```


### Family 2

Explore matrices of the form 
$$A=\displaystyle{ \begin{bmatrix} 0 & b \\ c & 0 \end{bmatrix}}.$$
Try various combinations of positive and negative numbers. Also try numbers of small magnitude (less than 1) and large magnitude. 


### Family 3

Explore matrices of the form 
$$A=\displaystyle{ \begin{bmatrix} \cos(t) & -\sin(t) \\ \sin(t) & \cos(t) \end{bmatrix}}$$
where $t$ is in radians. Here is a function that will create such a matrix.

```{r, echo=TRUE}
create_angle_matrix <- function(t) {
  A = cbind(c(cos(t), sin(t)), c(-sin(t), cos(t)))
  return(A)
}

A = create_angle_matrix(pi/2)
A
```
**Note**: R is numerical software. You'll note that ``sin(pi/2)`` returns a value of ``6.123234e-17`` or something similar. This value is given in scientific notation, and is $\approx 6.12 \times 10^{-17}$. **You should treat this tiny number as equal to 0.** Be sure to keep an eye out for return values like this that are ``numerically equivalent to 0.''


### Family 4

Now explore matrices of the form 
$$A=\displaystyle{ \begin{bmatrix} a & -b \\ b & a \end{bmatrix}}$$

Once again, consider a wide variety of such matrices. 


### Some Other Matrices

Now try these matrices. For each one, try to make other matrices that have a similar effect. (What is the relationship between the entries that leads to this particular kind of image?)


$$
A=\displaystyle{ \begin{bmatrix} 1 & -3 \\  2 & -6 \end{bmatrix}},
\qquad
B=\displaystyle{ \begin{bmatrix} 1 & 1 \\  0 & 1 \end{bmatrix}},
\qquad
C=\displaystyle{ \begin{bmatrix} 0 & 1 \\  0 & 0 \end{bmatrix}},
\qquad
D=\displaystyle{ \begin{bmatrix} 3 & 0 \\  3 & 2 \end{bmatrix}},
\qquad
$$





## Exploration

Now it's time to explore. Try one or more of the following:

* Can you find a mapping that will turn the rectangle with corners ``(0,0), (3,0), (3,2), (0,2)`` into a square?
* Look at some different starting vectors. 
* Try some other matrices.
* What is the relationship between the rank of the matrix and the image of the transformation?

## Even More Exploration

For a more interactive experinece, head over to [Section 2.6.2 of Understanding Linear Algbra](http://merganser.math.gvsu.edu/david/linear.algebra/ula/ula/sec-transforms-geom.html). This short section on the geometry of $2 \times 2$ matrix transformations has an interactive activity where you can change the entries using slider controls. It is illuminating to see the progressive impact of your choices!

<!--chapter:end:02-linear-transformations.Rmd-->

# Matrix Multiplication

[Download this Rmd file from GitHub](https://github.com/Tom-Halverson/math236_s21/blob/main/03-matrix-multiplication.Rmd)


Here we will practice multiplying matrices in R. First, let's define a few matrices.I'm using a trick here. By putting the assignment in parentheses, it assigns the matrix and displays it.s
```{r,echo=TRUE}
(A = cbind(c(1,2,3),c(4,5,6),c(1,1,-1)))
(B = cbind(c(1,-1,1),c(1,1,1),c(0,2,1)))
(C = cbind(c(2,1,1),c(1,0,1),c(1,-3,1),c(3,2,1)))
```
We multiply using the `%*%` command. As seen here:

```{r,echo=TRUE}
A %*% B
```

Note that 
```{r,echo=TRUE}
B %*% A
```
1. What do these last two multiplications say about the matrix product AB and BA? This is a very important property (or, perhaps, lack of property) of matrix multiplication.

2. Try multiplying BC and CB. What happens? And why?

```{r,echo=TRUE}

```

3. The *transpose* of a matrix is computed by `t(A)`. Compute the transpose of the matrices A, B, C and be sure that you all understand what it does. 

```{r,echo=TRUE}

```

4. The command `diag(n)` gives the n x n *identity matrix*. This is denoted $I_n$. For example, here is $I_3$. 
```{r,echo=TRUE}
diag(3)
```
Compute $I_2$, $I_4$, and $I_5$ and be sure you all agree on what the identity matrix is. 

5. Multiply the matrices A and B by the appropriately-sized identity matrix. Multiply  both ways, A I and I A, and agree upon what multiplying by the identity does.

```{r,echo=TRUE}

```

6. Multiply C by an identity matrix I C and C I. You might need a different size on the left and on the right.

```{r,echo=TRUE}

```

7. Our topic for Thursday (tomorrow) is the inverse of a matrix. You  compute the inverse of the matrix A  with `solve(A)`. Try this
```{r,echo=TRUE}
solve(B)
B %*% solve(B)
```

8. Multiply A by its inverse and look closely at the answer you get. 

```{r,echo=TRUE}
solve(A)
A %*% solve(A)
```

```{r,echo=TRUE}

```

9. Some matrices do not have inverses. Try computing the inverse of the following matrices. We will discuss this tomorrow!

```{r,echo=TRUE}
(M1 = cbind(c(3,5),c(-2,1)))
(M2 = cbind(c(4,3),c(5,4)))
(M3 = cbind(c(4,2),c(10,5)))
```

10. Enter the matrix A in problem 3.7 in the homework. Then compute the matrix G which is A times its transpose. Discuss its meaning.




<!--chapter:end:03-matrix-multiplication.Rmd-->


# Linear Transformations of a House


[Download this Rmd file from GitHub](https://github.com/Tom-Halverson/math236_s21/blob/main/04-house-transformations.Rmd)


Here is a plot of my house. You will need to run this chunk of code each time you re-start R to get the house back in memory.

```{r,house1,fig.height=4,fig.width=4, echo=TRUE}
house = cbind(c(0,0), c(0,3/4), c(1/2,3/4), c(1/2,0), c(1,0), c(1,1), c(5/4,1), c(0,2), c(-5/4,1), c(-1,1), c(-1,0), c(0,0));

plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
```

Here we explore linear transformations on the plane by looking at their effect on my house.


We give a series of examples of 2D linear transformations. After each example, it's your turn to play with variations from the same family of transformations. 

## Rotations

Suppose that we wish to *rotate* my house by pi/3 radians. As we've seen, a 2D rotation matrix by $t$ radians, counter-clockwise, is given by
$$A=\displaystyle{ \begin{bmatrix} \cos(t) & -\sin(t) \\ \sin(t) & \cos(t) \end{bmatrix}}.$$
Here is the code to display this transformation.
Observe that I apply the matrix `A` to the house, call it `house2` and plot both the original house and the new house in the same plot.

```{r,house2,fig.height=4,fig.width=4, echo=TRUE}
# define the matrix A. This is the only part you should need to edit.
t = pi/3
A = cbind(c(cos(t),sin(t)),c(-sin(t),cos(t)))
A # display the matrix A

#----------------- this code applies the transformation and plots
# create a plot that we will add more layers to
house2 = A %*% house
plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # add  grid lines
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```

Try changing the angle above to achieve different rotations. How can you rotate it clockwise?


## Expansion and contraction
Next, we scale the house by 2 in the $x$-direction and by 3 in the $y$-direction.

```{r,house3,fig.height=4,fig.width=4, echo=TRUE}
# define the matrix A. This is the only part you should need to edit.
A = cbind(c(2,0),c(0,3))
A # display the matrix A

#----------------- this code applies the transformation and plots
# create a plot that we will add more layers to
house2 = A %*% house
plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # add  grid lines
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```

Your turn: try some different scale factors. What if you use negative scale factors.


## Reflection

Now we reflect over the line y = x. 

```{r,house4,fig.height=4,fig.width=4, echo=TRUE}
# define the matrix A. This is the only part you should need to edit.
A = cbind(c(0,1),c(1,0))
A # display the matrix A

#----------------- this code applies the transformation and plots
# create a plot that we will add more layers to
house2 = A %*% house
plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # add  grid lines
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```

Your turn: try the reflections (1)  over the x-axis; (2) over the y-axis; and (3) through the origin, i.e., sending (x,y) to (-x,-y).


## Shear Transformations

A shear transformation is of the form
$$
A=\displaystyle{ \begin{bmatrix} a & b \\ 0 & c \end{bmatrix}}
\quad
\mbox{and}
\quad
A=\displaystyle{ \begin{bmatrix} a & 0 \\ b & c \end{bmatrix}}
$$
For example:

```{r,house5,fig.height=4,fig.width=4, echo=TRUE}
# define the matrix A. This is the only part you should need to edit.
A = cbind(c(1,0),c(1,1))
A # display the matrix A

#----------------- this code applies the transformation and plots
# create a plot that we will add more layers to
house2 = A %*% house
plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # add  grid lines
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```

You try: try to get the house to slant in the other direction.


## Dimension Reduction


Here we perform the transformation that sends  $\mathsf{e}_1$ to $(-1,1/2)$ and $\mathsf{e}_2$ to $(2,-1)$. Notice that they are the same line and the transformation projects the house onto this line.

```{r,house6,fig.height=4,fig.width=4, echo=TRUE}
# define the matrix A. This is the only part you should need to edit.
A = cbind(c(-1,1/2),c(2,-1))
A # display the matrix A

#----------------- this code applies the transformation and plots
# create a plot that we will add more layers to
house2 = A %*% house
plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # add  grid lines
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```



## Your Turn

See if you can do the transformations in problem 3.3.

```{r,fig.height=4,fig.width=4, echo=TRUE}
# define the matrix A. This is the only part you should need to edit.
A = cbind(c(1,0),c(0,1))
A # display the matrix A

#----------------- this code applies the transformation and plots
# create a plot that we will add more layers to
house2 = A %*% house
plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # add  grid lines
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```




<!--chapter:end:04-house-transformations.Rmd-->



# Fractals 

```{r}
require(pracma)
```


Fractals are mathematical sets with a self-similar structure. Each smaller part is a miniature copy of the whole structure. Today, we will see how to use linear transformations and homogeneous coordintes to generate a fractal.


We will generate a fractal by repeatedlyapplying the following 3D linear transformations. 

```{r}
(A1 = cbind(c(.86,-.03,0),c(.03,.86,0),c(0,1.5,1)))
(A2 = cbind(c(.2,.21,0),c(-.25,.23,0),c(0,1.5,1)))
(A3 = cbind(c(-.15,.27,0),c(.25,.26,0),c(0,.45,1)))
(A4 = cbind(c(0,0,0),c(.0,.17,0),c(0,0,1)))
```
Observe that, via homogenous coordinates, each of them does some transformation in $\mathbb{R}^2$ and some translation.

We choose among these four transformations using some randomness: 83% of the time we use A1, 8% of the time we use A2, 8% of the time we use A3, and 1% of the time we use A4. This is done in the following function. 


```{r}
get_matrix <- function(x) {
  val = runif(1) # generate a random number between 0 an 1
  if (val < .83) {
    A1  # return A1
  } else if (val < .91) {
    A2  # return A2
  } else if (val < .99) {
    A3  # return A3 
  } else {
    A4   # return A4 
  }
}
```

Now, we start with the point (0,0,1) in homogeneous coordinates. And apply this function over and over again to the previous point that was just computed, plotting the new point each time. 

```{r,fig.height=5,fig.width=5}
pt = c(0,0,1) # starting point

plot(pt[1], pt[2], pch='.', xlim=c(-5,5), ylim=c(-1,12)) # set up the plot

for (i in 1:2000) # loop
  {
  points(pt[1], pt[2], pch='.',col='forest green')
  pt = get_matrix() %*% pt
}

```


## Transforming My House.

So what is happening? Let's look at the effect of each of these linear transformations on my house

Here is what A1 does to the house.  This happens 91% of the time.
```{r,fig.height=5,fig.width=5}

house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1));

house2 = A1 %*% house

plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "forest green", border = "black")
```

Here is what A2 does to the house. This happens 8% of the time.
```{r,fig.height=5,fig.width=5}

house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1));

house2 = A2 %*% house

plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "forest green", border = "black")
```
Here is what A3 does to the house. This happens 8% of the time.
```{r,fig.height=5,fig.width=5}

house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1));

house2 = A3 %*% house

plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "forest green", border = "black")
```
Here is what A4 does to the house. This happens 1% of the time.
```{r,fig.height=5,fig.width=5}

house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1));

house2 = A4 %*% house

plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-5,5),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "forest green", border = "forest green")
```
```{r,fig.height=5,fig.width=5}

house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1));

plot(house[1,],house[2,],type="n",xlim=c(-5,5),ylim=c(-.5,10),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted") # set up the plot

for (i in 1:1000) # loop
  {
  polygon(house[1,], house[2,], col = "forest green", border = "black")
  house = get_matrix() %*% house
}

```

##  Your Turn

Make some small adjustments to one of these matrices. Explore the impact of the fractal. Can you make it spikier?  Bushier?

```{r}
A1 = cbind(c(.86,-.03,0),c(.03,.86,0),c(0,1.5,1)) 
A2 = cbind(c(.2,.21,0),c(-.25,.23,0),c(0,1.5,1)) 
A3 = cbind(c(-.15,.25,0),c(.27,.27,0),c(0,.45,1)) 
A4 = cbind(c(0,0,0),c(.0,.17,0),c(0,0,1)) 
```

```{r,fig.height=5,fig.width=5}
pt = c(0,0,1)

plot(pt[1], pt[2], pch='.', xlim=c(-5,5), ylim=c(-1,12))
for (i in 1:1000) {
  points(pt[1], pt[2], pch='.')
  pt = get_lt() %*% pt
}

```


## Here are some from http://paulbourke.net/fractals/ifs/ 

             a     0.2020    0.1380
             b    -0.8050    0.6650
             c    -0.6890   -0.5020
             d    -0.3420   -0.2220
             e    -0.3730    0.6600
             f    -0.6530   -0.2770

```{r,fig.height=5,fig.width=5}

B1 = cbind(c(0.202,-.689,0),c(-.805,-0.342,0),c(-0.3730,-.653,1))
B2 = cbind(c(0.138,-.502,0),c(.665,-0.222,0),c(.66,-.2777,1))


get_bourke <- function() {
  val = rand()
  if (val < .6) {
    A = B1
  } else {
    A = B2
  }
  
  A
}


pt = c(0,0,1)

plot(pt[1], pt[2], pch='.', xlim=c(-.5,1), ylim=c(-1.25,.5))
for (i in 1:1000) {
  points(pt[1], pt[2], pch='.')
  pt = get_bourke() %*% pt
}
```

```{r}
get_list_ifs <- function(ifs) {
  vec <- list()
  for (i in 1:dim(ifs)[2]) {
    vec[[i]] = cbind(c(ifs[1,i],ifs[3,i],0),c(ifs[2,i],ifs[4,i],0),c(ifs[5,i],ifs[6,i],1))
  }
  vec
}

get_rand_idx <- function(p ) {
  ret_val  = 1
  q =  0
  x  = rand()
  for (i in 1:length(mat_list)) {
    q = q + p[i]
    if (x < q) {
      ret_val =  i
      break
    }
  }

  ret_val
}

bourke_dat = rbind(c(0.2020, 0.1380), c(-0.8050, 0.6650),
             c(-0.6890,-0.5020),
             c(-0.3420,-0.2220),
             c(-0.3730,0.6600),
             c(-0.6530,-0.2770))

bourke_list = get_list_ifs(bourke_dat)
bourke_p  =  c(.5,.5)


```

```{r}
dragon_list = rbind(c(0.824074,0.088272),
c( 0.281428,0.520988),
c(-0.212346,-0.463889),
c(0.864198,-0.377778),
c(-1.882290,0.785360),
c(-0.110607,8.095795))
dragon_p = c(0.8, 0.2)

dragon_list
```




```{r,fig.height=5,fig.width=5}
pt = c(0,0,1)
mat_list = get_list_ifs(dragon_list)
p = dragon_p

plot(pt[1], pt[2], pch='.', xlim=c(-7,7), ylim=c(0,12))

for (i in 1:1000) {
  points(pt[1], pt[2], pch='.')
  mat = mat_list[[get_rand_idx(p)]]
  pt = mat %*% pt
}
```
```{r,fig.height=3,fig.width=3}
spiral_data = rbind(c(0.787879,-0.121212, 0.181818),
c(-0.424242,0.257576,-0.136364),
c(0.242424,0.151515, 0.090909),
c(0.859848,0.053030, 0.181818),
c(1.758647,-6.721654, 6.086107),
c(1.408065,1.377236, 1.568035))



spiral_p = c(0.90,0.05, 0.05)

pt = c(0,0,1)
mat_list = get_list_ifs(spiral_data)
(mat_list)
p = spiral_p

plot(pt[1], pt[2], pch='.', xlim=c(-7,7), ylim=c(0,12))

for (i in 1:1000) {
  points(pt[1], pt[2], pch='.')
  mat = mat_list[[get_rand_idx(p)]]
  pt = mat %*% pt
}

```

<!--chapter:end:05-fractal.Rmd-->


# Homogeneous Coordinates

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/05-homogeneous.Rmd)

A **translation** of the plane shifts every vector by a constant vector. For example, the mapping
$$
S \left( \begin{bmatrix} x \\ y \end{bmatrix} \right)
= \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix} 3 \\ -4 \end{bmatrix} 
= \begin{bmatrix} x +3 \\ y - 4 \end{bmatrix}
$$
translates every vector in the plane by $\begin{bmatrix} 3 \\ -4~ \end{bmatrix}$. 

**The bad news:** This is a simple and natural mapping, but it is **not a linear transformation**! We know that a linear transformation must map $\mathbb{0}$ to $\mathbb{0}$, and that is certainly not the case when we translate! This restriction is rather limiting for computer graphics: we can never move our image away from the origin. 

**The good news:** We can work around this problem by creating a 3D linear transformation $T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ and then retricting our attention to a plane in this larger space.
As discussed in the Homogeneous Coordinates video, we do the following:

1. Embed the $xy$-plane $\mathbb{R}^2$ into the plane $z = 1$ in $\mathbb{R}^3$. 
2. Translate in $\mathbb{R}^3$ using a mapping $T$ that maps this horizontal plane to itself. That is:
$$
T \left( \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} \right)
= \begin{bmatrix} u \\ v \\ 1 \end{bmatrix}. 
$$
3. When we create our plot, we use **only the first two coordinates** and ignore the third coordinate (which is still 1).

In summary, during our calculations, we replace the vector $\begin{bmatrix} x \\ y  \end{bmatrix}$ in $\mathbb{R}^2$ with the **homogeneous coordinate vector** $\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}$ in $\mathbb{R}^3$.

## Translation 


Here is my house once again. Note that we have add $z=1$ as the third coordinate to each point. However, when plotting, we only use the first two coordinates.

```{r,homogeneous1,fig.height=4,fig.width=4, echo=TRUE}
# the third entry always = 1
house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1));

# only plot the first two coordinates
plot(house[1,],house[2,],type="n",xlim=c(-6,6),ylim=c(-6,6),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
```


Next, we translate by $\begin{bmatrix} 3 \\ - 4 \end{bmatrix}$  by using the linear transformation
$$
T \left( \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} \right)
= \begin{bmatrix} 1 & 0 & 3  \\ 0 & 1 & -4 \\ 0 & 0 & 1 \end{bmatrix} 
\begin{bmatrix} x \\ y \\ 1 \end{bmatrix}=
\begin{bmatrix} x+3 \\ y - 4 \\ 1 \end{bmatrix}.
$$


Now, let's do this calculation in R and plot the first two coordiantes:

```{r,homogeneous2,fig.height=4,fig.width=4, echo=TRUE}

A = cbind(c(1,0,0),c(0,1,0),c(3,-4,1))

house2 = A %*% house

# only plot the first two coordinates
plot(house[1,],house[2,],type="n",xlim=c(-6,6),ylim=c(-6,6),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house2[1,], house2[2,], col = "pink", border = "black")
```

## Rotation and then Translation

We know plenty of interesting 2D linear transformations, including rotation, reflection  and shear mappings. We can turn any of them into a 3D transformation by appending a row and a column with a 1 in the lower right corner and  zero everywhere else. For example, the 2D rotation
$$
\begin{bmatrix} \cos \theta & -\sin \theta~   \\ \sin \theta & \cos \theta \end{bmatrix} 
$$
becomes the 3D transformation
$$
\begin{bmatrix} \cos\theta & -\sin\theta~ & 0   \\ \sin\theta & \cos\theta  &0 \\ 0 & 0 & 1 \end{bmatrix}. 
$$
This mapping rotates 3D space around the $z$-axis.

So let's combine two operations: a rotation and a translation  First, let's rotate  counterclockwise by $2 \pi/3$ and then translate by $\begin{bmatrix} -2 \\ 4 \end{bmatrix}$. And remember: the matrix **closest** to the vector acts first. So if we want to translate first, the translation matrix needs to be to the right of the rotation matrix:


$$
T \left( \begin{bmatrix} x \\ y \\ z \end{bmatrix} \right)
= 
\begin{bmatrix} 1 & 0 & -2  \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{bmatrix} 
\begin{bmatrix} \cos \frac{2\pi}{3} & -\sin\frac{2\pi}{3}~ & 0  \\ \sin\frac{2\pi}{3} & \cos\frac{2\pi}{3} & 0 \\ 0 & 0 & 1 \end{bmatrix} 
\begin{bmatrix} x \\ y \\ z \end{bmatrix}.
$$

```{r,homogeneous3,fig.height=5,fig.width=5, echo=TRUE}

t = 2*pi/3
rot = cbind(c(cos(t),sin(t),0),c(-sin(t),cos(t),0),c(0,0,1)) 
rot

translate = cbind(c(1,0,0),c(0,1,0),c(-2,4,1))
translate


A =   translate %*% rot 
A 
house3 = A %*% house

plot(house[1,],house[2,],type="n",xlim=c(-6,6),ylim=c(-6,6),xlab="x",ylab="y")
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house[1,], house[2,], col = "gray", border = "blue")
polygon(house3[1,], house3[2,], col = "green", border = "black")
```

 






## Your Turn  

### Translation and then Rotation

Let's **reverse the order** of these matrices in the previous example and see that we get a **different transformation**. Remember, order matters: matrix multiplication is not commutative. So try changing the problem to first translating and then rotating. But first, in your group, try guessing where the house will go and then do it in R. Does the house end up where you expected it to be?


### House of Orange

Here is a picture of a gray house and a larger, upside-down orange house. **Work as a group** to reproduce this image using homogeneous coordinates. 



![](https://raw.githubusercontent.com/Tom-Halverson/math236_s21/main/images/04-house-orange.png){width=70%}

You will have to use a combination of translation, rotation, and expansion.  You will do this by multiplying three matrices. **Think carefully and experiment.** Remember that the order of your matrices matters, and the rightmost one happens first. 

```{r,homogeneous5,fig.height=5,fig.width=5,echo=TRUE}
#############################
# your code defining the 3x3 matrices A1 and A2 
A1 = cbind(c(1,0,0), c(0,1,0), c(0,0,1))
A2 = cbind(c(1,0,0), c(0,1,0), c(0,0,1))
A3 = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A = A3 %*% A2 %*% A1


#############################
# you do not need to change this code
plot(house[1,],house[2,],type="n",xlim=c(-6,6),ylim=c(-6,6),xlab="x",ylab="y")
house2 = A %*% house
abline(h=-6:6, v=-6:6, col="gray", lty="dotted")
polygon(house2[1,], house2[2,], col = "orange", border = "green")
polygon(house[1,], house[2,], col = "gray", border = "blue")
```

### House Party

This problem is on PS5.

Here is a plot of the grey house and four other houses, colored cyan, red, gold, and purple Reproduce this image using homogeneous coordinates. 


![](https://raw.githubusercontent.com/Tom-Halverson/math236_s21/main/images/04-house-party.png){width=70%}

```{r,homogeneous6,fig.height=5,fig.width=5,echo=TRUE}

#############
# your code for 3x3 matrices that create the transformed houses goes here 
A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1))


####################
# you do not need to change this code
house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1));


plot(house[1,], house[2,], type = "n", xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab="x",ylab="y")
abline(h=-4:4, v=-4:4, col="gray", lty="dotted")

house.gold = A.gold %*% house
polygon(house.gold[1,], house.gold[2,], col = "gold", border = "blue")

house.cyan = A.cyan %*% house
polygon(house.cyan[1,], house.cyan[2,], col = "cyan", border = "blue")

house.red = A.red %*% house
polygon(house.red[1,], house.red[2,], col = "red", border = "blue")

house.purple= A.purple %*% house
polygon(house.purple[1,], house.purple[2,], col = "purple", border = "blue")

polygon(house[1,], house[2,], col = "gray", border = "blue")

```


<!--chapter:end:05-homogeneous.Rmd-->


# Eigenvectors

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/06-eigenvectors.Rmd)


## Computing Eigenvectors and Eigenvalues

To compute eigenvalues and eigenvectors in `R` we use the `eigen` command. For example if our matrix is
```{r,echo=TRUE}
(A =  cbind(c(-14,-20,-23),c(13,19,19),c(-2,-2,1)))
```
Then we compute its eigenvalues and eigenvectors as
```{r,echo=TRUE}
eigen(A)
```
One thing to notice about the eigenvectors is that they are scaled to have length one (they are **unit vectors**). So they often do not look like what we expect. Note for example that the first vector above is a multiple of $(1,2,3)^T$, the second is a multiple of $(-1,-1,2)^T$ and the third is a multiple of $(1,1,1)^T$.

We can extract the eigenvectors and eigenvalues as follows
```{r,echo=TRUE}
vals = eigen(A)$values
vecs = eigen(A)$vectors
```
Then, for example, we can see if a vector is an eigenvector as follows. Here I will check the first eigenvalue and first eigenvector:
```{r,echo=TRUE}
lambda1 = vals[1]
v1 = vecs[,1]
A %*% v1
lambda1 * v1
```
From this, we see that $A v_1 = \lambda_1 v_1$.

Recall that every scalar multiple of an eigenvector is also an eigenvector of that same eigenvalue. The vectors are currently scaled to have length 1. Another useful scaling is to have them sum to 1. You can accomplish this by dividing them by the sum of their entries. For example,
```{r,echo=TRUE}
v1 = v1/sum(v1)
v1
```


## Diagonalization

In class we diagonalized a few matrices. Here we show how to do this in `R`. 

(a) Here is the first matrix from the checkpoint question CP-5.3.

```{r,echo=TRUE}
(A = cbind(c(-5,-6,-7),c(6,7,8),c(-2,-2,-2)))
vals = eigen(A)$values
vals
vecs = eigen(A)$vectors
vecs
solve(vecs) %*% A %*% vecs
```
Here, we are diagonalizing $A$ by multiplying $P^{-1} A P = D$ where $P$ is the matrix of eigenvectors and $D$ is the diagonal matrix of eigenvalues. 

We can use `zapsmall` to round or "zap" very small numbers to 0, and it then looks more like what we are expecting.
```{r,echo=TRUE}
zapsmall(solve(vecs) %*% A %*% vecs)
```


(b) Now we diagonalize the second matrix from CP-5.3 You will recall that this one has a repeated eigenvalue (algebraic multiplicity 2), but it has a 2-dimensional eigenspace (geometric multiplicity 2), so it is digonalizable.
```{r,echo=TRUE}
B = cbind(c(3,-1,2),c(-1,3,2),c(2,2,0))
eigen(B)
vals = eigen(B)$values
vecs = eigen(B)$vectors
zapsmall(solve(vecs) %*% B %*% vecs)
```

(c) The third matrix from CP-5.3 is not diagonalizable. It has an eigenvalue of algebraic multiplicity 2 and geometric multiplicity 1. Note that it gives the same two eigenvectors for $v_2$ and $v_3$, because the eigenspace $E_2$ is only 1 dimensional.
```{r,echo=TRUE}
C = cbind(c(3,-1,1),c(2,2,1),c(1,1,2))
eigen(C)
```

## Rental Car Example

In problem PS 3.8, you studied a transition matrix for where cars got returned if they are rented from one of three rental locations: St. Paul, Rochester, and Duluth. Note that the columns of this matrix are probablilities, and as such, the are nonnegative and sum to 1. Such a matrix is called a **Markov** or **stochastic** matrix. 

```{r,echo=TRUE}
StP = c(.85,.09,.06)
Roch = c(.30,.60,.10)
Dul = c(.35,.05,.60) 
M = cbind(StP,Roch,Dul)
rownames(M) <- c("StP","Roch","Dul")
M
```

In this assignment you imagined that there were 20 cars at each location, i.e., `v = cbind(20,20,20)` and you applied the matrix over and over again to this vector watching it converge to a steady state. 

```{r,echo=TRUE}
v = c(20,20,20)
for (i in 1: 100) { v = M %*% v }
v
```

Stochastic matrices always have eigenvalue $\lambda = 1$. As can be seen here:
```{r,echo=TRUE}
eigen(M)
```
Note that the first eigenvalue is 1, and that the second and third eigenvalues (and eigenvectors) are complex and have both real and imaginary parts. If some of the eigenvalues have imaginary parts, then it outputs them all in complex form.

Here we extract just the real part of the first eigenvector (since its imaginary part is 0), and we rescale it both to sum to 1 and to sum to 60.
```{r,echo=TRUE}
v = Re(eigen(M)$vectors[,1])
(v = v /sum(v))
60*v
```
Notice that this is the *exact same* as the steady-state vector that we got by iterating. The steady-state that this system wants to be in --- with 40.97 cars in St. Paul, 10.31 cars in Rochester, and 8.72 cars in Duluth. It makes sense that a steady-state vector is an eigenvector with eigenvalue $\lambda = 1$. That the system converges to this state is eigen-magic that we will learn about soon.


## Northern Spotted Owl

This is the opening example in Chapter 5 of the textbook on page 265. It comes from a 1992 study of the northern spotted owl, which was threatened with extinction due to the loss of forest habitat due to logging in the Pacific Northwest. This is currently a story featured in an NPR Podcast called [Timber Wars](https://www.npr.org/podcasts/906829608/timber-wars).


### The Dynamical System

The vector
$$
x_n = \begin{bmatrix} j_n \\ s_n \\ a_n \end{bmatrix}
$$
is an *age-stage* vector in which $j_n, s_n$, and $a_n$ are the number of female owls in the juvenile (up to 1 year), subadult (1-2 year), and adult (over 2 year) age groups in year $n$.

The dynamics that take us from one year to the next is given by, the recursive relation $x_{n+1} = A x+n$, where $A$ is the matrix shown here. This is an age-stage matrix model that was published in [*Conservation Biology*](https://www.fs.usda.gov/treesearch/pubs/3519).
$$
\begin{bmatrix} j_{n+1} \\ s_{n+1} \\ a_{n+1} \end{bmatrix}
= 
\begin{bmatrix} 0 & 0 & 0.33 \\ 0.18 & 0 & 0 \\ 0 & 0.71 & 0.94 \end{bmatrix}
\begin{bmatrix} j_n \\ s_n \\ a_n \end{bmatrix}
$$
If we multiply this system out, we get
$$
\begin{array} {rcl}
j_{n+1} &=& 0.33 a_n    \\
s_{n+1} &=& 0.18 j_n     \\
a_{n+1} &=& 0.71 s_n + 0.94 a_n    
\end{array}
$$
We see that, in this model, 0.33 represents the fertility or *fecundity* rate. That is, it is the proportion of new juveniles next year to adults this year (the proportion of offspring the adult population is producing). The 0.18 is the survival rate from juvenile to subadult, 0.71 is the survival rate from subadult to adult, and 0.94 proportion of adults that survive from one year to the next.


To see the dynamics play out over time, we will start with an original population of owls is distributed into age groups as follows.
$$
x_0 = \begin{bmatrix} 100 \\ 76 \\ 502 \end{bmatrix}
$$
We will write a loop to apply the matrix $A$ over and over again. This time we will make a table and store each value in the table.

```{r,echo=TRUE}
A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix
x0 = c(100,76,502) # the inital value
N = 10  # iterate N=10 times
X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in
X[,1] = x0 # the first column is the initial population
for (i in 2:(N+1)) {  # loopn from 2 to N+1
  X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i
}
X # display the table
```

Having saved the information, we can now plot the data, Note that it appears to support the claim the claim that the owls are threatened with extinction.
```{r,echo=TRUE}
tot = X[1,] + X[2,] + X[3,]
t = seq(1,N+1)
plot(t,X[1,],type='l',col='blue',ylim=c(0,1000),ylab="population",xlab="year",main="Spotted Owl Population")
points(t,X[1,],col='blue',pch=20,cex=.8)
lines(t,X[2,],col='orange')
points(t,X[2,],col='orange',pch=20,cex=.8)
lines(t,X[3,],col='red')
points(t,X[3,],col='red',pch=20,cex=.8)
points(t,tot,col='black',pch=20,cex=.8)
lines(t,tot,col='black')
legend(8, 1050, legend=c("juvenile", "subadults", "adults","total"), col=c('blue','orange','red','black'), lty=1)
```
Let's run the iteration further. This time, we won't display the table (gets too big), and we will just show the plot of 100 iterations

```{r,echo=TRUE}
A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix
x0 = c(100,76,502) # the inital value
N = 100  # iterate N=10 times
X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in
X[,1] = x0 # the first column is the initial population
for (i in 2:(N+1)) {  # loopn from 2 to N+1
  X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i
}
tot = X[1,] + X[2,] + X[3,]
t = seq(1,N+1)
plot(t,X[1,],type='l',col='blue',ylim=c(0,1000),ylab="population",xlab="year",main="Spotted Owl Population")
points(t,X[1,],col='blue',pch=20,cex=.8)
lines(t,X[2,],col='orange')
points(t,X[2,],col='orange',pch=20,cex=.8)
lines(t,X[3,],col='red')
points(t,X[3,],col='red',pch=20,cex=.8)
points(t,tot,col='black',pch=20,cex=.8)
lines(t,tot,col='black')
legend(8, 1050, legend=c("juvenile", "subadults", "adults","total"), col=c('blue','orange','red','black'), lty=1)
```

They do seem to be dying out. 

### Eigenanalysis 

Now we check the eigenvectors and eigenvalues to see if they help us understand what is going on. 

```{r,echo=TRUE}
eigen(A)
```

The first eigenvalue is $\lambda_1 = 0.98$, and the other two are complex. R always lists the eigenvalues from largest to smallest, so in this case the largets eigenvalue is less than one. That means that in that direction, the population is dying off by 2% each year. 

If we extract the corresponding eigenvector, and scale it to sum to 1, we get
```{r,echo=TRUE}
v1 = eigen(A)$vectors[,1] # get the first eigenvector
v1 = Re(v1) # drop the imaginary part
v1/sum(v1)  # scale it to sum to 1
```
What this is telling us that as the population dies off, it does so in this eigenvector direction with 24.0% of the population being juveniles, 4.4% subadults, and 71.5% adults. 

The owls were going extinct because of the logging in the Pacific Northwest. Suppose that we make the case that by stopping logging we will increase the survival rate from juvenile to subadult from 0.18 to 0.26 (by improving the habititat the juvinile owls have a better chance of surviving the first year). In this case, the eigenvalues and eigenvectors becomes:
```{r,echo=TRUE}
A = cbind(c(0,0.26,0),c(0,0,.71),c(0.33,0,0.94))
eigen(A)
```

Notice that the largest eigenvalue now becomes 1. And if we iterate, we see that the population does not die off (it even grows slightly).
```{r}
A = cbind(c(0,0.26,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix
x0 = c(100,76,502) # the inital value
N = 100  # iterate N=10 times
X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in
X[,1] = x0 # the first column is the initial population
for (i in 2:(N+1)) {  # loopn from 2 to N+1
  X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i
}
tot = X[1,] + X[2,] + X[3,]
t = seq(1,N+1)
plot(t,X[1,],type='l',col='blue',ylim=c(0,1000),ylab="population",xlab="year",main="Spotted Owl Population")
points(t,X[1,],col='blue',pch=20,cex=.8)
lines(t,X[2,],col='orange')
points(t,X[2,],col='orange',pch=20,cex=.8)
lines(t,X[3,],col='red')
points(t,X[3,],col='red',pch=20,cex=.8)
points(t,tot,col='black',pch=20,cex=.8)
lines(t,tot,col='black')
legend(8, 1050, legend=c("juvenile", "subadults", "adults","total"), col=c('blue','orange','red','black'), lty=1)
```

Finally, we go back to the original system, which is dying out, but start with a totally different age distribution. You can see the non-dominant eigevectors dying out quickly at the beginning and the dominant eigenvector, of eigenvalue 0.98, taking over. 

```{r,echo=TRUE}
A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix
x0 = c(70,600,8) # the inital value
N = 20  # iterate N=100 times
X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in
X[,1] = x0 # the first column is the initial population
for (i in 2:(N+1)) {  # loopn from 2 to N+1
  X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i
}
tot = X[1,] + X[2,] + X[3,]
```


```{r,echo=TRUE}
t = seq(1,N+1)
plot(t,X[1,],type='l',col='blue',ylim=c(0,1000),ylab="population",xlab="year",main="Spotted Owl Population")
points(t,X[1,],col='blue',pch=20,cex=.8)
lines(t,X[2,],col='orange')
points(t,X[2,],col='orange',pch=20,cex=.8)
lines(t,X[3,],col='red')
points(t,X[3,],col='red',pch=20,cex=.8)
points(t,tot,col='black',pch=20,cex=.8)
lines(t,tot,col='black')
legend(8, 1050, legend=c("juvenile", "subadults", "adults","total"), col=c('blue','orange','red','black'), lty=1)
```



s


<!--chapter:end:06-eigenvectors.Rmd-->



# Dynamical Systems in 2D

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/07-dynamical-system.Rmd)


Let $A$ be a square $n \times n$ matrix and let $\mathsf{x}_0 \in \mathbb{R}^n$. A **dynamical system** is a sequence of vectors $\mathsf{x}_0,\mathsf{x}_1,\mathsf{x}_2, \ldots, \mathsf{x}_t, \ldots$ where
$$
\mathsf{x}_{t} = A \mathsf{x}_{t-1} = A^t \mathsf{x}_0 \quad \mbox{for} \quad t \geq 1.
$$
The sequence $\mathsf{x}_0,\mathsf{x}_1,\mathsf{x}_2, \ldots, \mathsf{x}_t, \ldots$ is called the **trajectory** for initial vector $\mathsf{x}_0$.

A dynamical system evolves over time. The long-term behavior is governed by the eigenvalues of matrix $A$. We will look at visualizations of some $2 \times 2$ dynamical systems to develop some intuition about eigensystems.

## Helper Function to Plot Dynamical Systems

Here is some special code,  written by Professor Beveridge, that makes helpful plots. You need to  **execute this code chunk** before the others.

```{r, echo=TRUE}
get_traj <- function(mat, x0, num) {
  traj = cbind(x0) 
  num
  for (i in 1:num) {
    traj = cbind(traj, mat %*% traj[,dim(traj)[2]])
    traj
  }
  return(traj)
}

plot_traj <- function(mat, x0, num) {
  traj = get_traj(mat,x0,num)
  points(traj[1,],traj[2,], pch=20, col=rainbow(length(traj)))
}

trajectory_plot <- function(mat, t=20, datamax=5, plotmax=10, numpoints=10, showEigenspaces=TRUE) {
  # initialize plot
  par(pty = "s")
  plot(c(0),c(0),type="n",
       xlim=c(-plotmax,plotmax),ylim=c(-plotmax,plotmax),
       xlab='x', ylab='y')
  abline(h=-plotmax:plotmax, v=-plotmax:plotmax, col="gray")
  mygrid <- expand.grid(x=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1),
                      y=seq(from =  -datamax, by = 2*datamax/numpoints, l = numpoints+1))
  for (i in 1:dim(mygrid)[1]) {
    plot_traj(mat,c(mygrid[i,1],mygrid[i,2]),t)
  }
  if (showEigenspaces) {
    eigen = eigen(mat)
    #mylabel = cat('lambda=', eigen$values[1], 'and lambda=', eigen$values[2])
    #title(xlab=mylabel)
    v1 = zapsmall(eigen$vectors[,1])
    v2 = zapsmall(eigen$vectors[,2])
    if (! class(v1[1]) == "complex") {
      if (v1[1] == 0) {
        abline(v=0)
      } else {
        abline(a=0,b=v1[2]/v1[1], col="blue")
      } 
      
      if (v2[1] == 0) {
        abline(v=0)
      } else {
        abline(a=0,b=v2[2]/v2[1], col="blue")
      }   
    }
    
  }
}
```



## Our first example

Let's start by looking at the example from the video
$$
A = \frac{1}{30}
\begin{bmatrix}
31 & 4 \\ 2 & 29
\end{bmatrix}.
$$
We get the most complete picture when we plot multiple trajectories at once. So we use the helper function `trajectory_plot` to plot the trajectories of a grid of points. It also plots the eigenspaces for the matrix. You can specify 

* the matrix A
* the number of iterations
* the size of the square where the initial points lie
* the size of the plot
* the number of points along the side of the grid


```{r, dynsys1, fig.height=7, fig.width=7,echo=TRUE}
A = 1/30 * cbind(c(31,2),c(4,29))
trajectory_plot(A, t=30, datamax=5, plotmax=15, numpoints=5)
```

This system is best understood by comparing what we see with the eigenvector and eigenvalues.
```{r, echo=TRUE}
eigen(A)
```
We can see that we have slight expansion along $[ 2, 1]^{\top}$ and slight contraction along $[-1,1]$. The long term behavior is an expansion in the direction of $[2, 1]^{\top}$.

## CheckPoint Question for today

Here is the checkpoint question for today (for which you found a closed-form solution).
```{r,echo=TRUE}
A = 1/110 * cbind(c(97,-8),c(6,123))
eigen(A)
```

And here is the corresponding plot of the dynamical system:
```{r, dynsys2, fig.height=7, fig.width=7,echo=TRUE}
A = 1/110 * cbind(c(97,-8),c(6,123))
trajectory_plot(A, t=10, datamax=15, plotmax=25, numpoints=5)
```

## Discussion Question 1

In class, we looked at a matrix with eigenvalues 1 and 1/2, and we plotted a trajectory starting at $(8,7)$ by hand. Its eigensystem is shown here:

```{r, dynsys3, fig.height=7, fig.width=7,echo=TRUE}
A = rbind(c(0.4, 0.4), c(-0.15, 1.1))
eigen(A)
```

And here is a trajectory plot
```{r, dynsys4, fig.height=7, fig.width=7,echo=TRUE}
trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5)
```

## Discussion Question 2

Here the  matrix has the same eigenvectors, but now the eigenvalues are 1.0 and 0.9. It's a little easier to see when the smaller eigenvalue converges more slowly.

```{r, dynsys5, fig.height=7, fig.width=7,echo=TRUE}
A = rbind(c(0.88, 0.08), c(-0.03, 1.02))
eigen(A)
trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5)
```

## Discussion Question 3

Here the  matrix has the same eigenvectors, but now the eigenvalues are 1.1 and 0.9. 

```{r, dynsys6, fig.height=7, fig.width=7,echo=TRUE}
A = rbind(c(0.86, 0.16), c(-0.06, 1.14))
eigen(A)
trajectory_plot(A, t=30, datamax=10, plotmax=15, numpoints=5)
```

## Discussion Question 4

Finally, here again the  matrix has the same eigenvectors, but now the eigenvalues are 0.99 and 0.9. 


```{r, dynsys7, fig.height=7, fig.width=7,echo=TRUE}
A = rbind(c(0.882, 0.072), c(-0.027, 1.008))
eigen(A)
trajectory_plot(A, t=50, datamax=10, plotmax=15, numpoints=5)
```


## Your Turn

Now it's your turn to explore some dynamical systems. 

* Create trajectory plots for each of these dynamical systems.
* Characterize the long-term behavior. 
    + What direction to vectors converge to?
    + Do magnitudes increase? decrease? stabilize?
* Calculate the eigenvectors and eigenvalues and compare them to your plot.
    + The eigensystem should tell the same story as your plot.


If your original plot is confusing, **try changing the parameters** (initial square size, plot size, number of grid points). 

Here is some code for you to adapt for the examples.

```{r,fig.height=7, fig.width=7, echo=TRUE}
A = cbind(c(1,0),c(0,1))
trajectory_plot(A, t=30, datamax=5, plotmax=10, numpoints=10)
eigen(A)

```

### Example 1

$$
A =  \frac{1}{60}
\begin{bmatrix}
55& -8 \\
-1 & 53
\end{bmatrix}
$$

### Example 2

$$
A =  \frac{1}{20}
\begin{bmatrix}
24& -6 \\
1 & 19
\end{bmatrix}
$$



### Example 3

$$
A =  \frac{1}{110}
\begin{bmatrix}
106& 12 \\
6 & 92
\end{bmatrix}
$$

### Example 4

$$
A =  \frac{1}{16}
\begin{bmatrix}
17& -15 \\
15 & 17
\end{bmatrix}
$$






<!--chapter:end:07-dynamical-system.Rmd-->



# Complex Eigenvalues

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/08-complex-eigenvalues.Rmd)

Now we will explore what happens if the matrix has complex eigenvalues.

## Rotation-Dilation

First we explore a special case of 2x2 matrices with complex eigenvalues of the following form:
$$
R=\begin{bmatrix}
a & -b \\
b & a \\
\end{bmatrix}
$$
As we see in the image below, this matrix rotates by angle of $\theta$ and expands (dilates) or contracts by a factor of $r$ where
$$
\begin{align}
\theta &= \arctan(b/a) \\
r &= \sqrt{a^2 + b^2}
\end{align}
$$
![](images/rotation-dilation.png){width=75%}

Furthermore the eigenvalues of this matrix are the complex values
$$
\lambda_1 = a + b i \qquad \lambda_2 = a - b i
$$
where $i = \sqrt{-1}$. These eigenvalues are conjugate pairs and are often written as
$\lambda = a \pm b i$. They come from applying the quadratic formula to the characteristic polynomial and getting a negative discriminant under the square root. It is important to note that both **the angle of rotation $\theta$ and the dilation factor $r$ are contained in the eigenvalues.** The fact that these are the eigenvalues is derived in the video. We will illustrate it here in three examples.

### Example 1

Our first example has $a = .9$ and $b = .2$.
```{r}
(A = cbind(c(.9,.2),c(-.2,.9)))
```

We look at its eigenvectors and eigenvalues and see that $\lambda = .9 \pm .2 i$:
```{r,echo=TRUE}
eigen(A)
```
Notics that the eigenvectors also come in conjugate pairs, with a real and a complex part. This always happens.
$$
\vec{\mathsf{v}} = 
\begin{bmatrix}0.707 \\ 0.000 \end{bmatrix} \pm 
\begin{bmatrix} 0.000 \\ .707 \end{bmatrix}  i
$$
Now, let's find the **angle of rotation**. We will use the `Arg` command which finds the angle (in radians) of a complex number. We also convert it to degrees here. 

```{r,echo=TRUE}
vals = eigen(A)$values
v1 = vals[1]
Arg(v1)  # gives the argument, or angle, of a complex number (in radians)
Arg(v1) / (2*pi) * 360 # convert to degrees
```

For good measure, we can compare with using the `arctan` function.

```{r,echo=TRUE}
atan(.2/.9)  
```

Next we find the **dilation/contraction factor**. We can do so using the `Mod` command, which finds the "modulus" or absolute value or length of a complex number.

```{r,echo=TRUE}
Mod(v1) # gives the length of a complex number
```

And, again for good measure, we compare with using the Pythagorean theorem:
```{r,echo=TRUE}
sqrt(.9^2 + .2^2)  
```

Now, we observe the trajectory of a single point $(0,1)^T$ under this matrix. In this picture you can see that it is contracting and rotating by 12.5 degrees. Note that 360/12.5 is about 29, and it takes 29 applications to go once around the circle. You can count them in the plot below. Furthermore, $(0.9219544)^29 =0.095$ and after 29 applications the vector is about 1/10 of its original length.

```{r,fig.width=6, fig.height=6}

N = 50
start = c(1,0)
X = matrix(0,nrow=2,ncol=N)
X[,1] = start
for (i in 2:N) {X[,i] = A %*% X[,i-1]}

plot(X[1,],X[2,],col='blue',xlim=c(-1,1),ylim=c(-1,1),xlab='x',ylab='y',pch=20,cex=1)
points(start[1],start[2],col="red")
grid(nx=10,ny=10)
```
We can also view this by looking at a plot of the x and y coordinates over time as the point (x,y) circles around in the xy-plane. Key point: complex eigenvalues lead to oscillating values of the individual coordinates

```{r}
t = seq(1,N)
plot(t,X[1,],type='l',col='blue',ylim=c(-1,1),ylab="x and y coordinates",xlab="t (time) ")
lines(t,X[2,],col='darkorange')
legend(15, 0.8, legend=c("x", "y"), col=c("blue", "darkorange"), lty=1)
```


```{r, echo=FALSE}
get_traj <- function(mat, x0, num) {
  traj = cbind(x0) 
  num
  for (i in 1:num) {
    traj = cbind(traj, mat %*% traj[,dim(traj)[2]])
    traj
  }
  return(traj)
}

plot_traj <- function(mat, x0, num) {
  traj = get_traj(mat,x0,num)
  points(traj[1,],traj[2,], pch=20, col=rainbow(length(traj)))
}

trajectory_plot <- function(mat, t=20, datamax=5, plotmax=10, numpoints=10, showEigenspaces=TRUE) {
  # initialize plot
  par(pty = "s")
  plot(c(0),c(0),type="n",
       xlim=c(-plotmax,plotmax),ylim=c(-plotmax,plotmax),
       xlab='x', ylab='y')
  abline(h=-plotmax:plotmax, v=-plotmax:plotmax, col="gray")
  mygrid <- expand.grid(x=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1),
                      y=seq(from =  -datamax, by = 2*datamax/numpoints, l = numpoints+1))
  for (t in 1:dim(mygrid)[1]) {
    plot_traj(A,c(mygrid[t,1],mygrid[t,2]),t)
  }
  if (showEigenspaces) {
    eigen = eigen(A)
    #mylabel = cat('lambda=', eigen$values[1], 'and lambda=', eigen$values[2])
    #title(xlab=mylabel)
    v1 = zapsmall(eigen$vectors[,1])
    v2 = zapsmall(eigen$vectors[,2])
    if (! class(v1[1]) == "complex") {
      if (v1[1] == 0) {
        abline(v=0)
      } else {
        abline(a=0,b=v1[2]/v1[1], col="blue")
      } 
      
      if (v2[1] == 0) {
        abline(v=0)
      } else {
        abline(a=0,b=v2[2]/v2[1], col="blue")
      }   
    }
    
  }
}
```

We can also use `trajectory_plot` from [Dynamical Systems in 2D] to watch what happens to a whole grid of points under this transformation. It is beautiful!

```{r,fig.height=7, fig.width=7, echo=TRUE}
trajectory_plot(A, t=30, datamax=5, plotmax=5, numpoints=10)
```


### Example 2

Here is a second example of a rotation-dilation matrix, this time with $a = .96$ and $b = .28$.

```{r,echo=TRUE}
(A = cbind(c(.96,.28),c(-.28,.96)))
eigen(A)
```

We check the angle of rotation and the dilation factor

```{r,echo=TRUE}
vals = eigen(A)$values
v1 = vals[1]
Arg(v1)  # gives the argument, or angle, of a complex number (in radians)
Arg(v1) / (2*pi) * 360 # convert to degrees
Mod(v1)  # gives the length of a complex number
```
Notice that the dilation factor is 1, which is seen in the following plots. Here are 21 iterations:

```{r,fig.height=4, fig.width=6}
# plot
N = 21
start = c(1,0)
X = matrix(0,nrow=2,ncol=N)
X[,1] = start
for (i in 2:N) {X[,i] = A %*% X[,i-1]}

plot(X[1,],X[2,],col='blue',xlim=c(-1,1),ylim=c(-1,1),xlab='x',ylab='y',pch=20,cex=1)
points(start[1],start[2],col="red")
grid(nx=10,ny=10)
```

And here are 200 iterations
```{r,fig.height=4, fig.width=6}
# plot
N = 200
start = c(1,0)
X = matrix(0,nrow=2,ncol=N)
X[,1] = start
for (i in 2:N) {X[,i] = A %*% X[,i-1]}

plot(X[1,],X[2,],col='blue',xlim=c(-1,1),ylim=c(-1,1),xlab='x',ylab='y',pch=20,cex=1)
points(start[1],start[2],col="red")
grid(nx=10,ny=10)
```

And a trajectory plot:
```{r,fig.height=7, fig.width=7}
trajectory_plot(A, t=30, datamax=5, plotmax=5, numpoints=10)
```

### Example 3

A third and final example.

```{r,echo=TRUE}
(A = cbind(c(.99,.16),c(-.16,.99)))
eigen(A)
vals = eigen(A)$values
v1 = vals[1]
Arg(v1) / (2*pi) * 360 # convert to degrees
Mod(v1)  # gives the length of a complex number
```

We see that the dilation factor is $r = 1.0028$ and the angle of rotation is $9.18$ degrees. Here are 100 iterations.


```{r,fig.width=6, fig.height=4}
# plot
N = 100
start = c(1,0)
X = matrix(0,nrow=2,ncol=N)
X[,1] = start
for (i in 2:N) {X[,i] = A %*% X[,i-1]}

plot(X[1,],X[2,],col='blue',xlim=c(-2,2),ylim=c(-2,2),xlab='x',ylab='y',pch=20,cex=1)
points(start[1],start[2],col="red")
grid(nx=10,ny=10)
```

And a trajectory plot:
```{r,fig.height=7, fig.width=7}
trajectory_plot(A, t=100, datamax=5, plotmax=5, numpoints=10)
```


## General 2x2 Matrices with Complex Eigenvalues

Now suppose we have a 2x2 matrix with complex eigenvalues $\lambda = a \pm b i$ and complex eigenvectors $\mathsf{v} = \mathsf{u } \pm \mathsf{w} i$ that is not in rotation-dilation form. Here is an example:
$$
A = \begin{bmatrix} 1.19 & -0.38 \\ 0.29 & 0.78 \end{bmatrix}
$$
It has eigenvalues and eigenvectors
$$
\lambda = 0.985 \pm 0.261 i \qquad \mathsf{v} = 
\begin{bmatrix} 0.753 \\ 0.406 \end{bmatrix} \pm 
\begin{bmatrix} 0.000 \\ -0.517 \end{bmatrix} i
$$
as seen here:

```{r}
(A = cbind(c(1.19,0.29),c(-.38,.78)))
eigen(A)
```

The angle of rotation and factor of dilation are $\theta = 14.8$ degrees and $r = 1.019$ as we see from these computations:
```{r}
vals = eigen(A)$values
v1 = vals[1]
Arg(v1) / (2*pi) * 360 # convert to degrees
Mod(v1)  # gives the length of a complex number
```

A trajectory plot shows us that it is still rotating by 14.8 degrees and dilating by 1.019, but it is taking more of an elliptical pattern.
```{r,fig.width=6, fig.height=4}
A = cbind(c(1.19,0.29),c(-.38,.78))
vecs = eigen(A)$vectors
u = Re(vecs[,1])
w = Im(vecs[,1])
N = 50
start = c(1,0)
X = matrix(0,nrow=2,ncol=N)
X[,1] = start
for (i in 2:N) {X[,i] = A %*% X[,i-1]}

plot(X[1,],X[2,],col='blue',xlim=c(-3,3),ylim=c(-3,3),xlab='x',ylab='y',pch=20,cex=1)
points(start[1],start[2],col="red")
lines(c(10*u[1],-10*u[1]),c(10*u[2],-10*u[2]),col="purple")
lines(c(10*w[1],-10*w[1]),c(10*w[2],-10*w[2]),col="purple")
grid(nx=10,ny=10)
```

To see precisely what happens, we change to basis $\{\mathsf{w}, \mathsf{u}\}$ where $\mathsf{w}$ and $\mathsf{u}$ are the imaginary and real parts of the eigenvector $\mathsf{v} =\mathsf{u}  +  \mathsf{w} i$. In this case the eigenvalues and eigenvectors are
$$
\lambda = 0.985 \pm 0.261 i \qquad \mathsf{v} = 
\begin{bmatrix} 0.753 \\ 0.406 \end{bmatrix} \pm 
\begin{bmatrix} 0.000 \\ -0.517 \end{bmatrix} i
$$
So if we make the change of basis matrix $P = [\mathsf{u},\mathsf{w}]$
$$
P = \begin{bmatrix}
0.000 &  0.753 \\
-0.517 & 0.406 \end{bmatrix}
$$
then we can factor $A$ as
$$
A = \begin{bmatrix} 1.19 & -0.38 \\ 0.29 & 0.78 \end{bmatrix} = 
\underbrace{\begin{bmatrix}0.000 &  0.753 \\-0.517 & 0.406 \end{bmatrix}}_P
\underbrace{\begin{bmatrix} 0.985 & - 0.261 \\ 0.261 & 0.985 \end{bmatrix}}_R
\underbrace{\begin{bmatrix}0.000 &  0.753 \\-0.517 & 0.406 \end{bmatrix}^{-1}}_{P^{-1}}
$$
We have not diagonalized $A$. Rather we have **rotation-dilationalized** (made up term) the matrix $A$. At is *core* $A$ is a rotation-dilation matrix whose angle and dilation factor come from the eigenvalue. The matrix $P$ is a change of basis matrix. It is rotating and dilating in this new coordinate system, which are the vectors in the plot above.

If we multiply the other way, we get
$$
P^{-1} A P = \begin{bmatrix} 0.985 & - 0.261 \\ 0.261 & 0.985 \end{bmatrix} = R
$$
Which we can see using R
```{r}
vecs = eigen(A)$vectors
P = cbind(Im(vecs[,1]),Re(vecs[,2]))
P
solve(P) %*% A %*% P
```


## A 3D example

Here is the Northern Spotted Owl matrices from the [Eigenvalues] examples. We saw that it has one real and two complex eigenvalues:

```{r}
A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94))
eigen(A)
```

The eigenvalues are always listed in descending order by magnitude as we see here when we compute their modulus and arguments
```{r}
Mod(eigen(A)$values)
Arg(eigen(A)$values)/(2 *pi)*360
```



We can diagonalize this over the complex numbers. 
```{r}
P = eigen(A)$vectors
zapsmall(solve(P) %*% A %*% P)
```
It has complex eigenvalues on the diagonal and requires using the complex vectors in our basis. Often, this is not what we want to do if our matrix has real entries and comes from a real-valued problem. 

Instead, let's rotation-dilationalize this matrix. The block diagonalization consists of a 1x1 block of the real eigenvalue and a 2x2 block of the rotation-dilation part of the complex eigenvalues.

```{r}
vecs = eigen(A)$vectors
P = Re(cbind(vecs[,1],Im(vecs[,2]),Re(vecs[,2])))
P
zapsmall(solve(P) %*% A %*% P)
```
In 3 dimensions, there the geometry is as follows:
* there is one basis vector, corresponding to the real eigenvalue $\lambda = 0.985$
* there is a plane in which the system rotates by $\theta = 96$ degrees and contracts by $r = 0.207$.
* thus, the complex part dies off pretty quickly ($r = 0.207$), and it converges to the dominant real eigenvector, which is also dying off but more slowly ($r = 0.983$)

We can see this in the traectory plot below. The oscillating part at the beginning comes from the complex eigenvalues as they quickly die off and the system converges to the dominant real eigenvector.

```{r,echo=FALSE,fig.width=7, fig.height=7}
A = cbind(c(0,0.18,0),c(0,0,.71),c(0.33,0,0.94)) # the population dynamics matrix
x0 = c(70,600,8) # the inital value
N = 15  # iterate N=100 times
X = matrix(0,nrow=nrow(A),ncol=N+1) # initialize an all 0 matrix to store values in
X[,1] = x0 # the first column is the initial population
for (i in 2:(N+1)) {  # loopn from 2 to N+1
  X[,i] = A %*% X[,i-1] # Apply A to column i-1 and put the value in column i
}
tot = X[1,] + X[2,] + X[3,]
t = seq(1,N+1)
plot(t,X[1,],type='l',col='blue',ylim=c(0,800),ylab="population",xlab="year",main="Spotted Owl Population")
points(t,X[1,],col='blue',pch=20,cex=.8)
lines(t,X[2,],col='orange')
points(t,X[2,],col='orange',pch=20,cex=.8)
lines(t,X[3,],col='red')
points(t,X[3,],col='red',pch=20,cex=.8)
points(t,tot,col='black',pch=20,cex=.8)
lines(t,tot,col='black')
legend(8,750, legend=c("juvenile", "subadults", "adults","total"), col=c('blue','orange','red','black'), lty=1)
```

<!--chapter:end:08-complex-eigenvalues.Rmd-->

# Voting Patterns in the US Senate

[Download this Rmd file](https://raw.github.com/mathbeveridge/math236_f20//main/08-voting-patterns.Rmd)



In 2014, the [Pew Research Center published a report](https://www.pewresearch.org/politics/2014/06/12/political-polarization-in-the-american-public/) about the increasing polarization of US politics. They wrote:


> Republicans and Democrats are more divided along ideological lines – and partisan antipathy is deeper and more extensive – than at any point in the last two decades. These trends manifest themselves in myriad ways, both in politics and in everyday life.

Is this really true, or is this just hype? Let's see what linear algebra can tell us about the evolution of voting patterns in the US Senate from 1964 to 2012. 

## The Data

We will analyze datasets corresponding to US Senate votes during a 2-year Congressional Session. 
Our data sets are 12 years apart, and were chosen to coincide with US election years. 

```{r}

tabledata = rbind(c(84, '1963-1965', 'Lyndon Johnson vs Richard Nixon', 534),
                  c(94, '1975-1977',  'Jimmy Carter vs Gerald Ford', 1311),
                  c(100, '1987-1989', 'George H. W. Bush vs Michael Dukakis', 799),
                  c(106, '1999-2001', 'George W. Bush vs Al Gore', 672),
                  c(112, '2011-2013', 'Barack Obama vs John McCain', 486)
)

tableframe = data.frame(tabledata)
names(tableframe) = c('Session', 'Years', 'US Election', '# Floor Votes')

knitr::kable(
  tableframe, booktabs = TRUE,
  caption = 'Congressional Sessions of the US Senate'
)
```


Here is a list our our data files, which we will load from Github. The original data can be found at [voteview.com](https://voteview.com/). There are two files for each year.

* A csv file containing a matrix whose $(i,j)$ entry counts the number of times Senator $i$ and Senator $j$ cast the same vote on an issue. There are actually [nine different possibilities](https://voteview.com/articles/data_help_votes), including Yea, Nay, Present, Abstention and "Not a member of the chamber when the vote was taken."
* A csv file containing senator information: name, state and party affiliation. 

```{r, echo=TRUE}

senate.1964.files = c('https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate088matrix.csv', 'https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate088senators.csv')

senate.1976.files = c('https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate094matrix.csv', 'https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate094senators.csv')

senate.1988.files = c('https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate100matrix.csv', 'https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate100senators.csv')

senate.2000.files = c('https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate106matrix.csv', 'https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate106senators.csv')

senate.2012.files = c('https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate112matrix.csv', 'https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate112senators.csv')

```


## The 88th Congressional Session (1964)

Let's load in our data.


```{r, echo=TRUE}

# pick the data set that we want to look at
senate.files = senate.1964.files


# First we load in the information about the senators.
# We will use these names as our labels.
# We also set up the colors we will use for our data points.
senators <- read.csv(senate.files[2], header=FALSE)

sen.name = senators[,2]
sen.party = senators[,4]

sen.color=rep("goldenrod", length(senators))
sen.color[sen.party=='D']="cornflowerblue"
sen.color[sen.party=='R']="firebrick"


# Next we load in the square matrix that measures how often senators voted together. 
# We add names for the columns and rows.
votes <- read.csv(senate.files[1], header=FALSE)
names(votes) <- sen.name
row.names(votes) <- sen.name


knitr::kable(
  head(votes), booktabs = TRUE,
  caption = 'Congressional Sessions of the US Senate'
)



```


Looking at this table, we see that 

* Aiken voted with Allott 321 times. 
* Aiken voted with Anderson 252 times.
* Allot voted with Anderson 221 times.
* And so on.

We also should note that ** each "coordinate" represents a senator. Coordinate 1 is "Aiken." Coordinate 2 is "Allot," etc. This interpretation will be important later!


## Using the Eigenbasis

As the table above notes, there are 102 Senators in our data set. So each Senator is represented by a vector in $\mathbb{R}^{102}$. This vector represents how similar the Senator is to each colleague. **But just because our data lives in $\mathbb{R}^{102}$ doesn't mean that it is inherently 102-dimensional!** We can draw a line in $\mathbb{R}^3$. That line is 1-dimensional, even though it lives in a higher dimensional space.

**Maybe if we find a good basis for** $\mathbb{R}^{102}$ then we can detect some interesting features of the data set. So what happens if we use an eigenbasis? We know that this basis is custom-made for our matrix. Here is what we will observe:

* The basis of eigenvector of our matrix will pick up some patterns in our data.
* The eigenvectors corresponding to the largest eigenvalues are the most important ones when modeling our column space. This is where the patterns are!
* The eigenvectors for the small eigenvalues don't really matter. They just pick up "noise" in the data.

In other words:

*  Our data set will be "essentially low dimensional" (maybe only 3D or 4D) even though it lives in a high dimensional space.
* Moreover, the structure of the eigenvectors (for the large eigenvalues) will reflect the prevalent patterns in the data. 

### Using the Dominant Eigenvectors.

First we will find the eigenvalues and eigenvectors. 


```{r, echo=TRUE}

mat = data.matrix(votes)

eigsys = eigen(mat)

eigsys$values

```

What do we see?

* The largest eigenvalue $\approx 24200$ is huge compared to the others! There is a **huge gap** after that. So this eigenspace is the most important (by far)!

* The second and third eigenvectors are still pretty big $\approx 4400$ and $\approx 2500$. But then we have another **big gap**: the rest have magnitudes below 500. 

So it seems like this data set is "roughly" 3-dimensional. Of course, this isn't technically correct because our matrix is invertible (all the eigenvalues are nonzero). But you can think of this data set as a "cloud of points" around a 3D subspace of $\mathbb{R}^{102}$.

### Patterns in the First Two Eigenvectors

Let's create a plot of the first two eigenvectors. Each point represents a Senator. 

```{r, senate1964, fig.height=5, fig.width=5, echo=TRUE}

# let's give simple names to our top eigenvectors
v1 = eigsys$vectors[,1]
v2 = eigsys$vectors[,2]
v3 = eigsys$vectors[,3]
v4 = eigsys$vectors[,4]
v5 = eigsys$vectors[,5]


# Plot use v1 for the x-axis and v2 for the y-axis
# We color the points by the party of the Senator
xdir = v1
ydir = v2

plot(xdir, ydir, pch=20,cex=1, col=sen.color)



```


During class, we will talk about how to think about this picture. We will also compare this plot to plots made using the other eigenvectors. 


### Creating a Table Sorted by an Eigenvector

When we get to more contemporary data, it will be fun later on to look at the names of the Senators who get large (positive or negative) eigenvector scores. Here is some code to help with that.

```{r, echo=TRUE}

myorder= order(v1)

eigendata = data.frame(cbind(v1,v2, sen.party))
rownames(eigendata)=sen.name


myorder= order(v1)

knitr::kable(
  head(eigendata[myorder,]), booktabs = TRUE,
  caption = 'One Extreme'
)

myrevorder = order(-v1)

knitr::kable(
  head(eigendata[myrevorder,]), booktabs = TRUE,
  caption = 'The Other Extreme'
)
```

## Your Turn

The data for the 88th Congressional Session (1964) does not seem very partisan. Democrats and Republicans are  finding plenty of common ground. When we look at the "extremes" we find some recognizable names, including Hubert Humphrey (D) and Barry Goldwater (R). But there is no clear "left" or "right" pattern to the party affiliation.

What about the remaining 5 data sets? It's your turn to explore and discuss.

The code above has been written for ease of reuse. 

* Go back to the top and change the following line:

```
# pick the data set that we want to look at
senate.files = senate.1964.files
```

to one of the other options: `senate.1976.files, senate.1988.files, senate.2000.files, senate.2012.files`

* Do a similar analysis as described above, as well as the things we tried in class together.

    + Find the eigenvalues. Where are the gaps? What is the rough "dimension" of each data set?
    + Plotting the two dominant eigenvectors. Do you see evidence of political divission?
    + Then try plotting other pairs of eigenvectors. Which ones are just noise?
    + Create tables of the two extreme for the dominant eigenvectors. Do you see any names that you recognize?
      
* Finally, you have some evidence to help you to answer the orignal question: "Is US Politics more polarized than ever before?"

<!--chapter:end:08-voting-patterns.Rmd-->


# Modeling of Ecological Systems

[Download this Rmd file](https://github.com/mathbeveridge/math236_f20/blob/main/09-ecological-models.Rmd)


In this exploration, we will explore the dynamics of a female animal population modeled by a Leslie matrix $L$ which has the form
$$L = 
\begin{bmatrix}
F_1 & F_2 & F_3 & \cdots & F_{n-1} & F_n \\
S_1 & 0 & 0 & \cdots & 0 & 0  \\
0& S_2 & 0 & \cdots & 0 & 0  \\
\vdots & \ddots & \ddots & \vdots & \vdots \\
0& 0 & 0 &  S_{n-2} & 0 & 0  \\
0& 0 & 0 &  0 & S_{n-1} & S_n  \\
\end{bmatrix}
$$


The female population is grouped in $n$ age classes. 

* The survival rate $S_i$ is probability that an animal in age class $i$ survives and enters age class $i+1$.
* The fecundity rate $F_i$ is the reproduction rate of animals in age class $i$.

Given an initial population $\mathsf{x}_0 \in \mathbb{R}^n$, the population $\mathsf{x}_{t+1}$ at time $t+1$ is determined by 
$$
\mathsf{x}_{t+1} = A \mathsf{x}_t.
$$



## Helper Functions to Plot Dynamical Systems

I have written some code that generates a trajectory  for a dynamical system and then plots it. You should run this code chunk before the others.

```{r, echo=TRUE}

# run this command to load some practical math functions
require(pracma)

# Creates a trajectory for the dynamical system
# A = the matrix
# start = the initial vector
# N = number of iterations
get_trajectory <- function(A, start, N) {
    ### this code follows the populations for N steps
  m = dim(A)[1]  # m is the number of rows of L
  X = matrix(0, nrow=m, ncol=N)  #  Store the results in a (m x N) matrix called X
  X[,1] = start  # put start in the first column of X
  
  # loop N times and put your results in X
  for (i in 2:N) {
    X[,i] = A %*% X[,i-1] 
  }
  
  return(X)
}


# Plots a trajectory along a time axis
# X  = the trajectory
# title = the title for the plot
# types = a vector of the names for each of the entries of the vector.
plot_trajectory <- function(X, title, types) {

  m = dim(X)[1]
  N = dim(X)[2]

  t = seq(1,N)  # time

  print(dim(X))
  print(dim(t))
    
  # Expand right side of clipping rect to make room for the legend
  par(xpd=T, mar=par()$mar+c(0,0,0,10))
  
  ymin = min(0, 1.1 * min(X))
  ymax = max(0, 1.1 * max(X))
  
  # Plot graph 
  plot(t, X[1,], type='l', col=1, ylim=c(ymin,ymax), ylab="amount", xlab="time",  main=title)
  
  for (i in 1:m) {
    lines(t, X[i,], col=i)
    points(t,X[i,], col=i, pch=20, cex=.8)
  }
  
  # Plot legend where you want
  legend(N *1.1, ymax * .85, types, col=1:m, lty = 1)
  
  # Restore default clipping rect
  par(mar=c(5, 4, 4, 2) + 0.1)
}

```

## Northern Spotted Owl Population

We model the population dynamics of the Northern Spotted Owls using a Leslie Matrix. The age classes for the spotted owls are:

- juvenile (less than 1 year old)
- subadult (1 to 2 years old)
- adult (over 2 years old)

and the Leslie Matrix is:
$$L=\begin{bmatrix} 0&0&0.33 \\ 0.18 & 0 & 0 \\ 0 & 0.71 & 0.94 \end{bmatrix}.$$



### Population forecast

Suppose that we started out with 200 juveniles, 100 subadults and 500 adults. What happens to this population over time?
 
```{r owls, fig.height=5, fig.width=7, echo=TRUE}
L = cbind(c(0,.18,0),c(0,0,.71),c(.33,0,.94)) # the Leslie Matrix
L
start = c(200,100,500)  # the starting distribution
N = 50  # N is the number of iterations

X = get_trajectory(L, start, N)

plot_trajectory(X, "Owls in Age Group", c("(0-1)", "(1-2)", "(over 2)"))

```

 
### The population at $t=200$

Change the code above to run for $N=200$. Can you now make a stronger statement about the long-term population?



```{r owls2, fig.height=5, fig.width=7, , echo=TRUE}

N = 200  # N is the number of iterations

X = get_trajectory(L, start, N)

plot_trajectory(X, "Owls in Age Group",  c("(0-1)", "(1-2)", "(over 2)"))
```

Yes, we can now see that populations are dying out.

### Eigenvectors and eigenvalues of $L$

Use eigenvalues and eigenvectors to explain the asymptotic behavior of the system. You can use the function `eigen(A)` to compute the eigenvalues and eigenvectors of a matrix.


```{r, echo=TRUE}
# your code goes here
eigen(L)
```

The largest eigenvalue of less than one. This explains why the owls are dying: when the dominant eigenvalue is less than one, we tend to the zero vector as time increases.


## Northern Spotted Owls Revisited

More recent spotted owl data give the following entries for the Leslie Matrix of the spotted owl population:

- Juvenile Survival 0.22
- Subadult Survival 0.85
- Adult Survival 0.94
- Subadult Fecundity 0.00
- Adult Fecundity 0.33


### Find the long-range populations

Using this new Leslie matrix $L$ and the same initial populations $\mathsf{x}_0 = [200,100, 500]$ as above, create a plot of the owl populations for $N=100$. 

```{r owls3, fig.height=5, fig.width=7, , echo=TRUE}
# Uncomment the next line and create this matrix 
# L = 

L = cbind(c(0,.22,0),c(.15,0,.85),c(.33,0,.94)) # the Leslie Matrix
L
start = c(200,100,500)
N = 100 

X = get_trajectory(L, start, N)

plot_trajectory(X, "Owls in Age Group", c("(0-1)", "(1-2)", "(over 2)"))

```




### The populations at $t=100$ versus the dominant eigenvector

 The following code compares your $t=100$ population ratios to the dominant eigenvector ratios. What do you notice? Why does this make sense?

```{r, echo=TRUE}

finalpop = X[,N]
finalpop = finalpop/sum(finalpop)

vec = as.numeric(eigen(L)$vectors[,1])
vec = vec/sum(vec)



tabledata = cbind(finalpop,vec)

tableframe = data.frame(tabledata)
names(tableframe) = c('population', 'eigenvector')
rownames(tableframe) = c('juveniles','subadults','adults')

knitr::kable(
  tableframe, booktabs = TRUE,
  caption = 'Ratios of vector entries'
)
```

These ratios are the same. This is as expected, since in the limit, the vectors should coverge to the direction of the dominant eigenvalue.


### The long-range prospects

What are the long-range prospects for the Northern Spotted Owl? Is it better or worse than the original data given above? 

They are much better. The population is increasing because the dominant eigenvalue 1.003 is larger than 1.

```{r}
eigen(L)$values[1]
```



## Brook Trout in Hunt Creek, MI

Here is the Leslie Matrix of a population of brook trout in Hunt Creek in Michigan. The population is categorized into 5 age categories: fingerlings (0,1), yearlings (1-2), young adults (2-3), adults (3-4), and adults (4-5). Right now the population is seen to be dying off. 

### The Leslie Matrix

Explain, in words, the meaning of each of the non-zero entries of the matrix $L$. 

```{r, echo=TRUE}
L = cbind(c(0,.05,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00))
L
```


The survival rates are:

* fingerlings to yearlings: 0.05
* yearlings to young adults: 0.28
* young adults to adults (3-4): 0.16
* adults (3-4) to adults (4-5): 0.08
* adults (4-5) to adults (4-5): 0

The nonzero fecundity rates are:

* young adults: 37
* adults (3-4): 64
* adults (4-5): 82

### Find the long-range populations

Here are the population dynamics. Calculate the eigenvalues for $L$ and match what you see to the plot of the populations for $t=50$. What do you conclude about the long-range population?

```{r, echo=TRUE}
# your code goes here

eigen(L)

```

The largest eigenvalue 0.887 is less than 1. So the population is going to die out.

```{r trout, fig.height=5, fig.width=7, echo=TRUE}

start = c(100,100,100,100,100)  # the starting distribution
N = 50              # N is the number of iterations

X = get_trajectory(L, start, N)

plot_trajectory(X, "Trout in Age Group", c("Fingerlings (0-1)", "Yearlings  (1-2)", "Young Adults (2-3)","Adults (3-4)","Adults (4-5)"))


```
  


### River Clean Up

A river clean up effort is being conducted with the hope of *increasing the rate of survival from fingerlings to yearlings*. 

- To what level does that survival rate need to be increased to in order for the population to reach a steady state? Find your answer (accurate up to 3 decimal places) by trial and error. (You only need to change a single entry of $L$. Which one?)
- What are the population percentages in the stable population?

```{r, echo=TRUE}

# update the fingerling to yearling survival rate
L = cbind(c(0,.074,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00))
L

## do your eigenvalue/eigenvector analysis here
eigensys = eigen(L)
val = eigensys$values[1]
vec = eigensys$vectors[,1]

val
vec/sum(vec)

```

Changing the fingerling survival rate to 0.074 results in a dominant eigenvector of 1 (approximately). The long-range population percentages will match the dominant eigenvector calculated above:
$(0.9105, 0.0674, 0.0189, 0.0030, 0.0002)$.


This code shows the population dynamics. You shouldn't need to edit this. 


```{r trout2, fig.height=5, fig.width=7, echo=TRUE}

start = c(100,100,100,100,100)  # the starting distribution
N = 50              # N is the number of iterations

X = get_trajectory(L, start, N)

plot_trajectory(X, "Trout in Age Group", c("Fingerlings (0-1)", "Yearlings  (1-2)", "Young Adults (2-3)","Adults (3-4)","Adults (4-5)"))

```


<!--chapter:end:09-ecological-models.Rmd-->


#  Network Centralities


[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/NetworkCentrality.Rmd)


In this example, we will use a package called `igraph`. To install it, you need to go to the packages window (bottom right), choose `install`, and search for and install `igraph` from the packages window.

```{r,message=FALSE, echo=TRUE}
library(igraph)
```
The igraph R package isn't all that well documented. Here are some places to look for documentation if you want to learn about other features. Let me know if you find any other good references:

* http://kateto.net/netscix2016
* http://igraph.org/r/doc/aaa-igraph-package.html

## Graphs and Networks

Graphs consists of **vertices** and the **edges** between them. These edges are used to model connections in a wide array of applications, including but not limited to, physical, biological, social, and information networks. To emphasize the application to real-world systems, the term *Network Science* is sometimes used. So we will use the terms **graph** and **network** interchangeably.  In this application, we will see that linear algebra is an important tool in the study of graphs. 

### Adjacency Matrices

Matrices are used to represent graphs and networks in a very direct way: we place a 1 in position $(i,j)$ of the **adjacency matrix** $A$ of the graph $G$, if there is an edge from vertex $i$ to vertex $j$ in $G$. Here is the adjacency matrix we will use today. 
```{r, echo=TRUE}
A = rbind(
  c(0,1,0,1,0,0,0,0,1,0,0,0), c(1,0,1,1,1,0,1,0,0,0,0,0),
  c(0,1,0,0,1,0,0,0,0,0,0,0), c(1,1,0,0,0,1,0,1,0,0,0,0),
  c(0,1,1,0,0,0,1,1,0,0,0,1), c(0,0,0,1,0,0,1,0,0,0,0,0),
  c(0,1,0,0,1,1,0,1,0,0,0,0), c(0,0,0,1,1,0,1,0,0,1,1,0),
  c(1,0,0,0,0,0,0,0,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0),
  c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,1,0,0,0,0,0,0,0))
A
```


We make a graph from the adjacency matrix as follows:
```{r, network1, fig.height=6, fig.width=6, echo=TRUE}
g=graph_from_adjacency_matrix(A,mode='undirected')
plot(g, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

Observe that there is an edge from vertex $i$ to vertex $j$ if and only if there is a 1 in position $(i,j)$ in the matrix. 

This network is the route map of a small airline. We will add vertex labels and change the vertex size:

```{r, network2, fig.height=6, fig.width=6, echo=TRUE}
airports = 
  c("ATL","LAX","ORD","MSP","DEN","JFK","SFO","SEA","PHL","PDX","MDW","LGA")
V(g)$label = airports
plot(g,vertex.size=30, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

### Graph Layouts

There are a variety of graph layout algorithms which place the vertices in the plane. You can find many algorithms [in the igraph documentation](https://igraph.org/c/doc/igraph-Layout.html). For example, here is a layout on a circle

```{r,  network3, fig.height=6, fig.width=6, echo=TRUE}
coords = layout_in_circle(g)
plot(g, layout=coords, vertex.size = 30,vertex.label.cex=0.85, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

The **Fruchterman-Reingold algorithm** is one of the most popular graph vertex layout algorithms. It is a force-directed layout that tries to get a nice-looking graph where edges are similar in length and cross each other as little as possible. The algorithm simulates the graph as a physical system. Vertices are electrically charged particles that repulse each other when they get too close. The edges act as springs that attract connected vertices closer together. As a result, vertices are evenly distributed through the chart area. The resulting layout is intuitive: vertices which share more connections are closer to each other. 

```{r, network4, fig.height=6, fig.width=6, echo=TRUE}
coords = layout_with_fr(g)
plot(g, layout=coords, vertex.size = 30, vertex.label.cex=0.85, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

We can also choose to layout vertices by hand:

```{r, network5, fig.height=6, fig.width=6, echo=TRUE}
locations = rbind(
  c(20,0),c(-10,0),c(11,7),c(10,15),c(3,12),c(25,10),
  c(-10,10),c(-12,15),c(20,6),c(-15,12),c(12,4),c(25,13)
)
plot(g,vertex.size=20, layout=locations, vertex.label.cex=0.85, vertex.color='tan1', vertex.frame.color="dodgerblue")
```


## Degree Centrality


If we are considering placing an office in one of our airport locations, we may want to chose the most **central** hub for that office. It turns out that there are many interesting **centrality measures** for networks. We will talk about two of them today.

The simplest measure centrality is the **degree** of the vertex, or the number of edges connected to that vertex. We calculate the degree centralities from the adjacency matrix as follows:

1. First make a vector $\mathsf{v}$ of all 1's; then 
1. multiply $\mathsf{p} = A\mathsf{v}$ to get the degree proportions; and 
1. divide the vector $\mathsf{p}$ by the sum of its entries. 

The result is a **normalized vector** $\mathsf{p}$ whose entries sum to 1. Each entry of vector $\mathsf{p}$ represents to *proportion* of edges incident with the corresponding vertex.

```{r, echo=TRUE}
v=rep(1,nrow(A)) # all 1s vector
d = A %*% v  # degrees
p=d/sum(d)   # proportion of degrees
dp = cbind(d,p) # show d and p together side-by-side in a matrix
rownames(dp) = airports
colnames(dp) = c("degree","proportion")
dp
```

We can also sort the vertices by degree. 

```{r,echo=TRUE}
ii=order(d,decreasing=TRUE)  # find the decreasing order of d
dp2 = dp[ii,]    # sort the data frame in that order
dp2
```

Now let's create a **data visualization.** We plot the network and size each vertex according to the vector $p$. The larger vertices have more edges connected to them. This conveys information to the viewer about the relative importance of the vertices. 

```{r, network7, fig.height=6, fig.width=6, echo=TRUE}
plot(g, layout=locations, vertex.size=250*p,vertex.label.cex=0.65, vertex.color='tan1', vertex.frame.color="dodgerblue")
```


## Gould's Index

Gould's Index is a measure of centrality that uses the dominant eigenvector of a matrix. It was introduced by geographer P. R. Gould in 1967 to analyze the geographical features on maps. We will build up Gould's Index step-by-step so that we can understand what it measures. 

### Step 1: Add Layovers

The first step is typically to add the identity matrix to the adjancency matrix $A$ to get a new matrix 
$$
B = A + I.
$$ 
The $n \times n$ identity matrix in `R` is obtained by using `diag(n)`. Adding the identity gives a connection from a vertex to itself. This **loop edge** corresponds to staying at the current city during a layover. 

```{r, echo=TRUE}
(B = A + diag(nrow(A)))
```

Here is what the corresponding network (with layovers) looks like. You can see why we refer to these additional edges as "loops." However, we usually do not draw the network with these added loops to keep the figure less cluttered.


```{r, networkB, fig.height=6, fig.width=6, echo=TRUE}
g2=graph_from_adjacency_matrix(B,mode='undirected')
airports = 
  c("ATL","LAX","ORD","MSP","DEN","JFK","SFO","SEA","PHL","PDX","MDW","LGA")
V(g2)$label = airports
coords = layout_with_fr(g2)
plot(g2, layout=coords, vertex.color='tan1', vertex.frame.color="dodgerblue")
```


### Step 2: Dynamical System

Starting with the all 1's vector $\mathsf{v}_0 = [1, 1, \ldots ,1 ]^{\top}$ create the dynamical system
$$
\mathsf{v}_0, \quad  
\mathsf{v}_1 = B \mathsf{v}_0,  \quad 
\mathsf{v}_2 = B \mathsf{v}_1,  \quad
\mathsf{v}_3 = B \mathsf{v}_2,  \quad \cdots , \quad
\mathsf{v}_n  = B \mathsf{v}_{n-1}.
$$
Here we calculate $\mathsf{v}_1, \ldots, \mathsf{v}_{10}$ using a loop:
```{r, echo=TRUE}
N = 10 
X = matrix(0,nrow=nrow(B),ncol=N+1) # make a a table of 0s
X[,1] = rep(1,nrow(B))              # put v0 in first column
for (i in 1:N) {                  # loop N times
   X[,i+1] = B %*%  X[,i]   # apply B to the ith column and make it the (i+1)st column      
}
rownames(X) = airports
colnames(X) = 0:10
X
```

**Discuss with your group:** Each of the entries of the vector $\mathsf{v}_{t}$ in the columns of the table above corresponds to "a trip of length $t$." What kinds of trips does the $i$th entry of $\mathsf{v}_{t}$ count? Here is how you can figure this out:

1. Compare the table of vectors with the picture of the network with layovers.
1. Start by looking at the $t=1$ column. The $i$th entry has something to do with the $i$th city. 
1. Next, look at the $t=2$ column. And so on.
1. Once you have noticed the connection between the network and data, *explain why the rule $\mathsf{v}_t = B \mathsf{v}_{t-1}$ leads to this result*.
1. These numbers get big fast! Let's normalize by dividing by the sum each time. The vectors will always be proportions then, which sum to 1. See the table below. What do the entries in this table tell us?

```{r, echo=TRUE}
N = 10 
X = matrix(0,nrow=nrow(B),ncol=N+1)
X[,1] = rep(1,nrow(B))
for (i in 2:(N+1)) {  
   X[,i] = B %*%  X[,i-1]
   X[,i] = X[,i]/sum(X[,i])
}
rownames(X) = airports
colnames(X) = 0:10
X
```

### Step 4: Eigen-analysis 

We see that the vectors are converging to a common direction, and 
we know that dynamical systems converge to the **dominant eigenvector** (if there is one). We can see below that there is a dominant eigenvector in this case. 
```{r, echo=TRUE}
eigen(B)
```

For an adjacency matrix $A$, the dominant eigenvector of $B + I$, scaled to sum to 1, is called **Gould's Index** of network centrality. Here we extract it, scale it to sum to 1, and we show that the dynamical system is converging to it.

```{r, echo=TRUE}
# Get the dominant eigenvector
vecs = eigen(B)$vectors
gould = vecs[,1]
gould = gould/sum(gould)

# Compute the dynamical system
N = 30
X = matrix(0,nrow=nrow(B),ncol=N+1)
X[,1] = rep(1,nrow(B))/nrow(B)
for (i in 1:N) {  
   X[,i+1] = B %*%  X[,i]
   X[,i+1] = X[,i+1]/sum(X[,i+1])
}

# Display the data
Y = cbind(X[,1],X[,2],X[,3],X[,11],X[,21],X[,31],gould)
rownames(Y) = airports
colnames(Y) = cbind("n=0","n=1","n=2","n=10","n=20","n=30","Gould")
Y
```


### Step 5

Now let's plot the network with:

* the vertices sized by Gould's Index
* the labels sized by degree centrality

```{r, network6, fig.height=6, fig.width=6, echo=TRUE}
plot(g, layout=locations, vertex.size=250*gould,vertex.label.cex=8*p, vertex.color='tan1', vertex.frame.color="dodgerblue" )
```

And we show the data containing Gould's Index and the Degree Centrality. We order the data using the Gould Index and then compare the two. Observe that degree centrality and Gould's Index do not always agree.
```{r, echo=TRUE}
Z = cbind(gould,p) 
rownames(Z)=airports
colnames(Z)=c('Gould', 'Degree')
ii=order(gould,decreasing=TRUE)
Z = Z[ii,]
Z
```

**Discuss with your group:** Degree centrality and Gould's Index give different rankings. Look at the table and observe that:

* LAX, DEN and SEA have the same degree centrality. However LAX and DEN have higher Gould Index than SEA.
* SFO has lower degree centrality than SEA, but higher Gould centrality! So these two centralities give different rankings.
* Why does the Gould Index value SFO more than SEA?
* Find another pair of cities where the rankings of degree centrality and Gould's Index differ. Look at the plot of the network and explain why this is the case.


### Gould Index Summary

Now that we understand what Gould's Index means, let's summarize how to find the Gould Index values for an adjacency matrix $A$.

1. Create the matrix $B = A+I$.
2. Find the dominant eigenvector $\mathbf{v}$ of $B$.
3. Normalize the values of $\mathbf{v}$ so that the entries sum to 1.



## Your Turn: The Rise of Moscow

Russian historians often attribute the dominance and rise to power of Moscow to its strategic position on medieval trade routes (see Figure 1). Others argue that sociological and political factors aided Moscow’s rise to power, and thus Moscow did not rise to power strictly because of its strategic location on the trade routes. The figure below shows the major cities and trade routes of medieval Russia.

![](https://raw.githubusercontent.com/Tom-Halverson/math236_s21/main/images/MedievalRussia.png){width=100%}

Use Gould’s Index to form a geographer's opinion about this debate. Either:

* Moscow’s location was the primary reason for its rise to power, or 
* Other forces must have come into play.

Here is the adjacency matrix for this transportation network into an adjacency matrix and a plot of the network.
```{r, networkRussia, fig.height=8, fig.width=8, echo=TRUE}
RusCity = c("Novgorod", "Vitebsk", "Smolensk", "Kiev", "Chernikov",
"Novgorod Severskij", "Kursk", "Bryansk", "Karachev", "Kozelsk",
"Dorogobusch", "Vyazma", "A", "Tver", "Vishnij Totochek", "Ksyatyn",
"Uglich", "Yaroslavl", "Rostov", "B", "C", "Suzdal", "Vladimir",
"Nizhnij Novgorod", "Bolgar", "Isad'-Ryazan", "Pronsk", "Dubok",
"Elets", "Mtsensk", "Tula", "Dedoslavl", "Pereslavl", "Kolomna",
"Moscow", "Mozhaysk", "Dmitrov", "Volok Lamskij", "Murom")
A = rbind(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(1, 0, 
    1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 1, 0, 1, 0, 0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 
    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
    0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 
    0, 1, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(1, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 1, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
    1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 
    0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
    0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
    1, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))
g=graph_from_adjacency_matrix(A,mode='undirected')
V(g)$label = RusCity
# Plot network
plot(g)
```

a. Create a vector containing the normalized Degree Centralities. See Section \@ref(degree-centrality) for help.

a. Create a vector containing the Gould Index values. See Section \@ref(gould-index-summary) for help.

a. Plot the network where the size of the vertices is determined by Gould's Index and the size of the label is determined by degree centrality.

a. Create a data frame that contains Gould's Index and Degree Centralities. The rows should be labeled with the city names and the columns should be named by the centrality measures.  Sort according to Gould's Index.

a. Use Gould's Index to decide whether Moscow's dominance was solely due to its geographic location.

a. Compare the Gould's Index and Degree Centrality rankings and note any interesting findings. See Section \@ref(step-5) for help.

<!--chapter:end:09-network-analysis.Rmd-->



# Digit Recognition

[Download this Rmd file](https://github.com/mathbeveridge/math236_f20/blob/main/10-digit-recognition.Rmd)

**Machine Learning** has become a big part of our daily experience. When my phone suggests that it's time for bed, or volunteers how long my today's commute will be, I am benefiting from machine learning. My phone has learned my patterns and is predicting what might be useful for me today based on past data.

Rather than being "told" the answer up front, a machine learning algorithm **learns what to do by analyzing training data**. The algorithm  then uses patterns to decide what to do with new data.  In other words, these algorithms learn from experience, just like humans do. That's why machine learning falls under the umbrella of artificial intelligence.

And guess what sits at the heart of machine learning? That's right: **linear algebra.** Today we will create a rudimentary machine learning algorithm using the dot product in a high-dimensional space.

## US Post Office Zip Code Data

We will look at a classic machine learning problem: identifying hand-written digits. Our data comes from the U.S. Postal Service: each digit comes zip codes automatically scanned from envelopes. I found this data at [this Machine Learning course page at Stanford](https://web.stanford.edu/~hastie/StatLearnSparsity_files/DATA/zipcode.html), but it originates from the neural network group at AT&T research labs. I've made a copy of the data, and pre-processed the training data (to speed things up for us).

Let's load the data and print out the first 10 rows and columns of the test data.


```{r, echo=TRUE, message=FALSE}
library(readr)
require(pracma)

digit.test.file = 'https://raw.github.com/mathbeveridge/math236_f20//main/data/digit-test.csv'
digit.centroids.file = 'https://raw.github.com/mathbeveridge/math236_f20//main/data/digit-centroid.csv'

test <- data.matrix(read_csv(digit.test.file, col_names = FALSE))
centroids <- data.matrix(read_csv(digit.centroids.file, col_names = FALSE))

# take the transpose so that we can deal with column vectors (which is what we are used to)
test=t(test)
centroids=t(centroids)

dim(test)
test[1:10,1:10]

```


The data set has 257 rows and 2007 columns. Each column of the data corresponds to a hand-written digit. So the test data has 2007 hand-written digits. 


### Visualizing the Data

Here is how the data is formatted.

* The first value in each column tells you the correct digit. Looking at the data above, we see that the first ten hand-written digits are $9,6,3,6,6,0,0,0,6,9$. 
* The remaining 256 values in each column correspond to a grayscale image of the digit. 

We must reshape this $256 \times 1$ column vector into a $16 \times 16$ matrix in order to display it. The following code will display the handwritten image for a given column. (Note: You don't have to understand this code.)



```{r, echo=TRUE}

# Takes in a vector of length 257 and creates an image form entries 2:257
# digitcol: a 257 x 1 vector. The first entry is the digit classification. The remaining 256 entries are the 16 x 16 image.
plot_digit <- function(digitcol) {
  img = digitcol[2:257]
  dim(img) <- c(16, 16)
  img = img[,ncol(img):1 ]
  
  image(img, axes = FALSE, col = grey(seq(0, 1, length = 256)))
}

```


Now let's display a few random digits from the test dataset. Feel free to re-run this block a few times. You'll see different examples each time!

```
'```{r digitsamples, out.width = "150px", out.height = "150px", echo=TRUE}


samp = sample(1:dim(test)[2], 6, replace=FALSE)

for (i in 1:6) {
  print(test[1,samp[i]])
  plot_digit(test[,samp[i]])
}

```

![](images/digitsamples-1.png){width=150px, height=150px}
![](images/digitsamples-2.png){width=150px, height=150px}
![](images/digitsamples-3.png){width=150px, height=150px}

![](images/digitsamples-4.png){width=150px, height=150px}
![](images/digitsamples-5.png){width=150px, height=150px}
![](images/digitsamples-6.png){width=150px, height=150px}

Next, let's talk about our training data. The original training data has 7291 different digits. Different machine learning techniques will use this data in different ways. But each of them follows the same idea: 

* Define a measure of "closenesss" to the training data of known digits. 
* Then compare the new (unknown) digits and categorize it according to the best fit to the training data.

Our goal today is to **highlight the role of linear algebra**, not come up with a high-performance algorithm. So we will simplify our training set. I have pre-processed the 7291 training digits and found the "average" vector for each digit. 
We will refer to these 10 training vectors as **centroids** since we are finding the vector in the center of all the training digits of each type. Since these are averages, when we plot them, they will look like blurry versions of the actual digit.  Let's look. 


```
'```{r digitcentroid, out.width = "150px", out.height = "150px", echo=TRUE}
library(grid)
images = vector()

for (i in 0:9) {
    dimg = centroids[,i+1]
    plot_digit(dimg)
  }
```

![](images/digitcentroid-1.png){width=150px, height=150px}
![](images/digitcentroid-2.png){width=150px, height=150px}
![](images/digitcentroid-3.png){width=150px, height=150px}

![](images/digitcentroid-4.png){width=150px, height=150px}
![](images/digitcentroid-5.png){width=150px, height=150px}
![](images/digitcentroid-6.png){width=150px, height=150px}

![](images/digitcentroid-7.png){width=150px, height=150px}
![](images/digitcentroid-8.png){width=150px, height=150px}
![](images/digitcentroid-9.png){width=150px, height=150px}

![](images/digitcentroid-10.png){width=150px, height=150px}




### Algorithm Overview

Here is our simple machine learning algorithm. For each test digit:

* Find the centroid that is closest to that digit.
* Classify the test digit as the same type as that centroid.


We will consider two definitions of "closeness." Both are calculated using the dot product!

1. The distance between the vectors in $\mathbb{R}^{256}$. This **Euclidean distance** or **Pythagorean distance** is probably what you were thinking of when I started talking about distance in high dimensions.
2. The angle between the vectors in $\mathbb{R}^{256}$. Interesting! Does this surprise you?  Why is this a reasonable way to measure distance? Well, span is one of the two fundamental characteristics of linear algebra (the other is linear independence). By measuring the angle between the vectors, we have a notion of distance between the span of each vector. In other words, **the angle measures the distance between a pair of one-dimensional subspaces**.  This measure is known as **cosine similarity**.



So let's get started.

* **Here is the good news**: I've written the code to process all 2007 of the digits and then compare the classification to the truth. The results are then presented a matrix whose $(i,j)$-th entry counts the number of time digit $i$ was classified as digit $j$. (So we hope that the diagonal entries will be big!)

* **Here's the even better news**: I've left three functions for you to implement using the dot product. 



## Magnitude, Distance and Angle


Here are the three methods that you must implement:

* `get_magnitude(colvec)`
* `get_distance(colvec1, colvec2)`
* `get_angle(colvec1, colvec2)`

Implement these methods according to the definitions:
$$
\| \mathsf{v} \| = \sqrt{ \mathsf{v} \cdot \mathsf{v}}, \qquad 
d(\mathsf{v}, \mathsf{w}) = \| \mathsf{v} - \mathsf{w} \|, \qquad
\theta = \cos^{-1} \left( \frac{\mathsf{v} \cdot \mathsf{w}}{\| \mathsf{v} \| \, \| \mathsf{w} \|} \right).
$$
**Note:** The R command `length(vec)` returns the number of entries in the vector `vec`. To avoid confusion, we use "magnitude" to refer to the geometric length of a vector.


Here are some R functions that you will need.

* The dot product of two column vectors `colvec1` and `colvec2`  is calculated by `t(colvec1) %*% colvec2`. Here we are tranposing `colvec1` to turn it into a row vector.
* The square root of `y` is calculated by `sqrt(y)`.
* The inverse cosine of `x` is calculated by `acos(x)`. 


```{r, echo=TRUE}

get_magnitude <- function(colvec) {
  # replace with your implementation
  len = sqrt(t(colvec) %*% colvec)
  
  return (len)
}


get_distance <- function(colvec1, colvec2) {
  v = colvec1 - colvec2
  dist = get_magnitude(v)
  
  return (dist)
} 



get_angle <- function(colvec1, colvec2) {
  dotprod = t(colvec1) %*% colvec2
  len1 = get_magnitude(colvec1)
  len2 = get_magnitude(colvec2)

  val = dotprod / (len1 * len2)
  angle = acos(val)
  
  return (angle)
  
} 


```





Here is some test code you can use to confirm that your implementations are correct.

```{r, echo=TRUE}


check_equal <- function(val1, val2) {
  diff = zapsmall(val1 - val2)
  if (diff^1 > 0.001) {
    sprintf('fail %f %f', val1, val2)
  } else {
    print('pass')
  }
}

print('get_magnitude')
check_equal(get_magnitude(c(3,4)),5)
check_equal(get_magnitude(c(1,2,-2,1)),sqrt(10))
check_equal(get_magnitude(c(0,0,0)),0)

print('get_dist')
check_equal(get_distance(c(3,4,5),c(2,-1,-3)), sqrt(90))
check_equal(get_distance(c(1,2,-2,1),c(1,2,-2,1)), 0)
check_equal(get_distance(c(2,1.5),c(-2,-1.5)), 5)

print('get_angle')
check_equal(get_angle(c(1,2,3,1),c(-3,3,-1,0)), pi/2)
check_equal(get_angle(c(1,0),c(1,1)), pi/4)
check_equal(get_angle(c(3,0),c(1,0)), 0)

```


## Machine Learning Algorithms

Once your magnitude, distance and angle code is working, you are ready to try out our two rudimentary machine learning algorithms! You will find that each of them does okay (but not great) for this data set. Both get around $80\%$ of the digits correct. (A good machine learning algorithm can get $98\%$ correct.)

Run the code below and look at the output. Discuss the following questions

1. Which digits are most commonly mistaken for one another? 
1. Which algorithm does better: closest by distance or closest by angle? 
1. We turned the training data into a single centroid and then used that to measure distances. A better algorithm would make use of all of the training points during the classification. Discuss what improvements you would try instead of using the centroids.

### Get Closest by Distance

```{r, echo=TRUE}

# Finds closest centroid to the given data
# data - a column vector of length 257. the first entry is the actual digit. the remaining 256 are the data
# centroids - a 257 x 10 matrix. Column j is the centroid of digit j-1. 
get_closest_by_distance <- function(data, centroids) {
  idx = 0
  min_dist = 1024
  
  for (i in 1:10) {
    colvec1 = centroids[2:257,i]
    colvec2 = data[2:257]
    
    dist = get_distance(colvec1, colvec2)
    
    if (dist < min_dist) {
      idx = i
      min_dist = dist
    }
  }
  
  return (idx)
  
}



results = matrix(0, nrow=10, ncol=10)

# try to match each column of the test data
for (i in 1:dim(test)[2]) {
  true_digit = data.matrix(test[1, i]) + 1
  digit_data = data.matrix(test[,i])
  matched_digit = get_closest_by_distance(digit_data, centroids)
  results[true_digit,matched_digit] = results[true_digit,matched_digit]+1
}

labels=c(0:9)
colnames(results) <- labels
rownames(results) <- labels

results

sprintf('matched: %f percent' , sum(diag(results))/sum(results) * 100) 

```




### Get Closest by Angle

```{r, echo=TRUE}

get_closest_angle <- function(data, centroids) {
  idx = 0
  minangle = pi

  for (i in 1:10) {
   angle = get_angle(data[2:257,1], centroids[2:257,i])
    
    if (angle < minangle) {
      idx = i
      minangle = angle
    }
  }
  
  return (idx)
}



results = matrix(0, nrow=10, ncol=10)

for (i in 1:dim(test)[2]) {
  true_digit = data.matrix(test[1, i]) + 1
  digit_data = data.matrix(test[, i])
  matched_digit = get_closest_angle(digit_data, centroids)
  results[true_digit,matched_digit] = results[true_digit,matched_digit]+1
}
  
labels=c(0:9)
colnames(results) <- labels
rownames(results) <- labels

results

sprintf('matched: %f percent' , sum(diag(results))/sum(results) * 100) 

```







<!--chapter:end:10-digit-recognition.Rmd-->



# Geometry in $\mathbb{R}^n$

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/10-geometry.Rmd)

```{r,echo=FALSE}
require('pracma')
```


## Dot Product

We will look at the gometry of $n$-dimensional vectors. For example, here are three vectors in $\mathbb{R}^6$.
```{r}
u = cbind(c(1,2,3,4,5,6))
v = cbind(c(1,1,3,3,5,5))
w = cbind(c(0,-1,-1,1,-1,1))
```

We can compute the **dot product** two different ways. If you have included lit `pracma` library, you can use the `dot` command. 
```{r}
dot(u,v)
dot(v,w)
dot(u,w)
```

We can also compute the dot product in native `R` by multiplying $u^T v$. It is important to remember that this works.

```{r}
t(u) %*% v
t(v) %*% w
t(u) %*% w
```
## Length, Distance, Angle

The **length** of a vector can be computed using $\sqrt{u\cdot u}$ or as the built in 2-norm of a vector. The reason it is the 2-norm is because we are squaring (second power) and taking the square root.
```{r}
sqrt(t(u) %*% u)
norm(u,'2')
```

The **distance** between two vectors, is the length of the difference between them
```{r}
sqrt(t(u-v) %*% (u-v))
norm(u-v,'2')
```
The **angle** between two vectors is given by the formula
$$
\theta  = \arccos\left(\frac{ v \cdot w } {||v|| ||w||} \right)
$$
which can be comuted using arccosine function `acos`.
```{r}
acos( t(u) %*% v / (sqrt(t(u) %*% u)*sqrt(t(v) %*% v)))
```
Sometimes we use the cosine of the angle between the two vectors (we will see an example of this in the homework)
$$
\cos(\theta)  = \frac{ v \cdot w } {||v|| ||w||}
$$
which is computed as
```{r}
 t(u) %*% v / (sqrt(t(u) %*% u)*sqrt(t(v) %*% v))
```
This is a number between -1 and 1, with numbers close to 1 meaning that they are closely aligned, numbers close to 0 meaning that they are close to orthogonal, and numbers close to -1 meaning that they are close to opposite.
```{r}
 t(u) %*% w / (sqrt(t(u) %*% u)*sqrt(t(w) %*% w))
 t(v) %*% w / (sqrt(t(v) %*% v)*sqrt(t(w) %*% w))
```

## Orthogonal Complement

The **orthogonal complement** of a vector space $W$ is 
$$
W^\perp = \left\{ v \in \mathbb{R}^n \mid v \cdot w = 0 \text{ for every } w \in W \right\}.
$$
The orthogonal complement is a subspace. Furthermore, it is enough to check that $w$ is orthogonal to a basis of $W$. Tnat is, you don't have to check every vector in $W$; if you are orthogonal to the basis then you are orthogonal to $W$.

For example, if 
$$
W = \mathsf{span} \left\{ 
\begin{bmatrix} 1\\2\\3\\4\\5 \end{bmatrix},
\begin{bmatrix} 1\\1\\1\\1\\1 \end{bmatrix},
\begin{bmatrix} 1\\2\\2\\2\\1 \end{bmatrix},
\begin{bmatrix} 3\\5\\6\\7\\7 \end{bmatrix},
\begin{bmatrix} 0\\2\\1\\0\\-4\end{bmatrix}
\right\},
$$
then we can put the vectors of $W$ into the rows of a matrix. So in this case, we make the matrix
```{r}
A = rbind(c(1, 2, 3, 4, 5), c(1, 1, 1, 1, 1), c(1, 2, 2, 2, 1), c(3, 5, 6, 7, 7), c(0, 2, 1, 0, -4))
A
```
Now, $W$ is the *row space* of $A$. That is, $W = \mathsf{Row}(A)$. And the row space is orthogonal to the null space. Therefore $W^\perp = \mathsf{Nul}(A)$, so we row reduce,
```{r}
rref(A)
```
There are 2 free variables, so the null space and, thus, $W^\perp$ are 2 dimensional. We describe a basis of the null space
$$
W^\perp = \mathsf{Nul}(A) = \mathsf{span} \left\{
\begin{bmatrix} 0 \\ 1 \\ -2 \\ 1 \\ 0 \end{bmatrix},
\begin{bmatrix} -1 \\ 4 \\ -4 \\ 0 \\ 1 \end{bmatrix}
\right\}.
$$
We can check that these vectors are orthogonal to $W$ by multiplying
```{r}
A %*% cbind(c(0,1,-2,1,0),c(-1,4,-4,0,1))
```



<!--chapter:end:10-geometry.Rmd-->

# Least Squares Approximation

[Download this Rmd file](https://github.com/Tom-Halverson/math236_s21/blob/main/11-least-squares.Rmd)

```{r,warning=FALSE}
require(pracma)
```

## Introduction

Let's start with a summary of Least Squares Approximation.

**The Why**: Given a matrix $A$ and a vector $\mathsf{b}$ that is not in $W = \mathrm{Col}(A)$, we want to find the "best approximate solution" $\hat{\mathsf{b}} \in W$. In other words, we want to pick the best possible $\hat{\mathsf{b}} \approx \mathsf{b}$ that lies in the column space of $A$.

**The What**: The answer is to use projections.

* This "best approximation" is the projection $\hat{\mathsf{b}} = \mbox{proj}_W \mathsf{b}$. 
* The residual vector vector $\mathsf{r} = \mathsf{b} - \hat{\mathbf{b}}$  is in $W^{\perp}$.
* The length $\| \mathsf{r} \|$ of the residual vector measures the closeness the approximation.
* The approximate solution to our original problem is the vector $\hat{\mathsf{x}}$ such that 
$A \hat{\mathsf{x}} = \hat{\mathsf{b}}$.

![](images/least-squares-pic.png){width=70%}

**The How**: A clever way to solve this is to use the *normal equations*. The best choice for $\hat{\mathsf{x}}$ satisfies
$$
A^{\top} A \hat{\mathsf{x}} = A^{\top} \mathsf{b}. 
$$
\newcommand{\A}{{\mathsf{A}}}
\newcommand{\b}{{\mathsf{b}}}

## Example 1

Here is an example that we did by hand in class. We now see how to do it in R. 

Find the least-squares solution to $\A x = \b$ if
\begin{equation}
\A = 
\begin{bmatrix}
1 & 1 \\
1 & 2 \\
0 & -1 \\
\end{bmatrix}
\quad \mbox{and} \quad
\b = 
\begin{bmatrix} 1 \\ 1 \\ 3 \end{bmatrix}.
(\#eq:inconsistent)
\end{equation}
First, for good measure, let's see if the system is inconsistent

```{r,echo=TRUE}
A = cbind(c(1,1,0),c(1,2,-1))
b = c(1,1,3)
Ab = cbind(A,b)
Ab
rref(Ab)
```

True indeed: $\b \not \in Col(\A)$.

Now we compute the normal equations to see what they look like:
```{r,echo=TRUE}
t(A) %*% A
t(A) %*% b
```
So we are going to instead solve the following **normal equations** instead of 
\@ref(eq:inconsistent):
\begin{equation}
\begin{bmatrix}
2 & 3 \\
3 & 6 \\
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
= 
\begin{bmatrix} 2 \\ 0 \end{bmatrix}.
(\#eq:normal)
\end{equation}

We can do this in the following nice, single R command
```{r,echo=TRUE}
(xhat = solve(t(A) %*% A, t(A) %*% b))
```
To compute $\hat \b$ we use
```{r,echo=TRUE}
(bhat = A %*% xhat)
```
And to get the residual, we use
```{r,echo=TRUE}
(r = b - bhat)
sqrt(t(r) %*% r)
```
We can also check that the residual is orthogonal to $Col(\A)$:
```{r,echo=TRUE}
t(A) %*% r
```


## Template

The following R code does it all. You can use this as a **template** for future problems. Just enter the matrix A and the vector b.

```{r, echo=TRUE,collapse=TRUE}
# Given: the matrix A
A = cbind(c(1,1,0),c(1,2,-1))
# Given: the target vector b
b = c(1,1,3)

#solve the normal equation
(xhat = solve(t(A) %*% A, t(A) %*% b))

# find the projection
(bhat = A %*% xhat)

# find the residual vector
(r = b - bhat)

# check that z is orthogonal to Col(A)
t(A) %*% r

# measure the distance between bhat and b
sqrt( t(r) %*% r)
```


## Fitting for a Linear Function

Here are some points that we'd like to fit to a linear function $y = a_0 + a_1 x$. 

**Note:** Here we use `y` instead of `b` because we like to write linear equations as "$y = cx + d$." So the expression "$b = a_0 + a_1 x$" looks funny to us. So we will talk about `y` and `yhat` instead of `b` and `bhat`.

```{r quadplot1,fig.width=4.5, fig.height=4.5, echo=TRUE}
x = c(1,2,3,4,5,6)
y = c(7,2,1,3,7,7)
plot(x,y,pch=19,ylim=c(0,10))
grid()
```

The linear equations that we want to fit are as follows. 
$$
\begin{bmatrix} 
1 & 1 \\  1 & 2  \\ 1 & 3  \\ 1 & 4  \\ 1 & 5  \\ 1 & 6  \\
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1  \end{bmatrix}
= 
\begin{bmatrix} 7 \\ 2 \\ 1 \\ 3 \\ 7 \\ 7 \end{bmatrix}.
$$

These equations are inconsistent, so we solve the normal equations $A^T A x = A^T y$ and find an approximate solution instead. **Pro Tip:** a clever way to create the desired matrix $A$ is to use the fact that $x^0=1$ for any number $x$.

```{r, echo=TRUE}
(A = cbind(x^0,x))
```
Let's take a look at the normal equations:
```{r, echo=TRUE}
t(A) %*% A
t(A) %*% y
```
So the normal equations to solve are below. It's surprising how, even though there are 6 variables, we only have to solve a 2x2 equation, since there are 2 unknowns.
$$
\begin{bmatrix} 
6 & 21 \\  21 & 91  \\
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1  \end{bmatrix}
= 
\begin{bmatrix} 27 \\ 103 \end{bmatrix}.
$$
```{r,echo=TRUE}
(xhat = solve(t(A) %*% A, t(A) %*% y))
```
This tells us that the desired intercept is $a_0 = 2.8$, the desired slope is $a_1 = 0.4856$, and the linear model is $y = 2.8 + 0.4856x$.

We can plot the points together with the solution using:
```{r quadplot2, fig.width=4.5, fig.height=4.5, echo=TRUE}

#plot the original set of points
plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main='the best-fit linear function')

# generate points for the fitted line and plot it
tt = seq(1,6,len=100)  
lines(tt,xhat[1]+xhat[2]*tt,col='blue')

# get yhat
yhat = A %*% xhat

# add the residuals to the plot
for (i in 1:length(x)) {
  lines(c(x[i],x[i]),c(y[i],yhat[i]), col='red')
}

#add yhat to the plot
points(x,yhat,pch=19,col='orange')

#put the original points back on the plot last so we can see them 
points(x,y,pch=19,col="black")
grid()
```

In this visualization we see the following:

* The black points: the original data points  `cbind(x,y)`. This represents the entries of the desired target vector `y`. 

* The blue curve: the fitted curve, created from the approximate solution `xhat`.  
* The orange points: the approximations `cbind(x,yhat)` of the data points `cbind(x,y)`. This represents entries of the projection `yhat`.

* The red line segments: the distances between the original data points (block dots) and their approximations (orange dots). The lengths of these red segments are the entries of the residual vector `r`.

Let's look at the residual and see that it is indeed orthogonal to the columns of $A$.

```{r,echo=TRUE}
yhat = A %*% xhat
r = y - yhat
res=cbind(y,yhat,r)
colnames(res) = c("y","yhat","r")
res
t(A) %*% r
sqrt(t(r) %*% r)
```


## Fitting for a Quadratic Function

The data we have been working with has a quadratic look to it, so let's try adding an $x^2$ term. That is, we will fit the model $y = a_0 + a_1 x + a_2 x^2$. The equations we want to solve are
In this case, the linear model that we'd like to solve is:
$$
\begin{bmatrix} 
1 & 1 & 1\\  1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \\ 1 & 5 & 25 \\ 1 & 6 & 36 \\
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix}
= 
\begin{bmatrix} 7 \\ 2 \\ 1 \\ 3 \\ 7 \\ 7 \end{bmatrix}.
$$


It is easy enough to add this to our matrix $A$.

```{r, echo=TRUE}
(A = cbind(x^0,x,x^2))
```
In this case our normal equations are 3x3

```{r, echo=TRUE}
t(A) %*% A
t(A) %*% y
```

$$
\begin{bmatrix} 
6 & 21 & 91 \\  21 & 91 & 441 \\ 91 & 441 & 2275 
\end{bmatrix}
\begin{bmatrix} a_0 \\ a_1 \\ a_2 \end{bmatrix}
= 
\begin{bmatrix} 27 \\ 103 \\ 499 \end{bmatrix}.
$$
Whose solution is computed by
```{r,echo=TRUE}
(xhat = solve(t(A) %*% A, t(A) %*% y))
```
Notice that our solution is now $y = 10.3 - 5.1393 x + 0.8036 x^2$. The linear term is now negative, but there is a positive quadratic term. Let's look at the same plo but with the addex $x^2$ term. We see that the residuals are smaller **and**, importantly, the model appears to better fit the data.

```{r quadplot3, fig.width=4.5, fig.height=4.5, echo=TRUE}

#plot the original set of points
plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main='the best-fit quadratic function')

# generate points for the fitted line and plot it
tt = seq(0,7,len=100)  
lines(tt,xhat[1]+xhat[2]*tt+xhat[3]*tt^2,col='blue')

# get yhat
yhat = A %*% xhat

# add the residuals to the plot
for (i in 1:length(x)) {
  lines(c(x[i],x[i]),c(y[i],yhat[i]), col='red')
}

#add yhat to the plot
points(x,yhat,pch=19,col='orange')

#put the original points back on the plot last so we can see them 
points(x,y,pch=19,col="black")
grid()
```

Let's look again at the residual and see that it is indeed orthogonal to the columns of $A$ and see that the residual got shorter.

```{r,echo=TRUE}
yhat = A %*% xhat
r = y - yhat
res=cbind(y,yhat,r)
colnames(res) = c("y","yhat","r")
res
t(A) %*% r
sqrt(t(r) %*% r)
```


<!--chapter:end:11-least-squares.Rmd-->


# SVD and Image Compression

[Download this Rmd file](https://github.com/mathbeveridge/math236_f20/blob/main/12-image-compression.Rmd)

Today we will use the `svd` command to find the Singular Value Decomposition of an $m \times n$ matrix.
We will practice reading the output of this command and finding bases for the four fundamental subspaces of a matrix.
Then we will look at a cool application of SVD: image compression!

<center>
![](https://raw.githubusercontent.com/mathbeveridge/math236_f20/main/images/svd-summary.png){width=100%}
</center>


## Singular Value Decomposition

Let's start by looking at the SVD for a couple of matrices. For each one: 

a. Determine the rank of $A$
b. Identify the basis for each of the four fundamental subspaces
$$
\mbox{Nul}(A), \qquad \mbox{Col}(A), \qquad \mbox{Row}(A), \qquad \mbox{Nul}(A^{\top}).
$$
c. Pick a vector $\mathsf{x}$ from the basis for $\mbox{Nul}(A)$ and confirm that $A \mathsf{x} = \mathbf{0}$.
d. Pick a vector $\mathsf{y}$ from the basis for $\mbox{Col}(A)$ and confirm that $\mathsf{y}$ can be written as the a linear combination of the columns of $A$.
e. Find the spectral decomposition of $A$
f. Use the spectral decomposition to create approximations of matrix $A$ and then compare the quality of the approximation to the size of the gaps of the singular values.



### SVD of a Wide Matrix

Here is a $4 \times 5$ matrix $A$.

```{r, echo=TRUE}
(A = cbind(c(1,-1,1,0),c(-2,3,0,2),c(1,-1,1,0),c(0,1,4,0),c(1,-1,5,-4)))

```

Let's call `svd(A)` and see what we get.

```{r, echo=TRUE}
(decomp = svd(A))

```

The return value has three attributes: 

* `decomp$d`: these are the singular values $\sigma_1, \sigma_2, \sigma_3, \sigma_4$. Note that $\sigma_4=0$.
* `decomp$u`: the $4 \times 4$ matrix whose columns are the left singular vectors $\mathsf{u}_1, \mathsf{u}_1, \mathsf{u}_1, \mathsf{u}_1$ corresponding to the singular values. 
* `decomp$v`: the $5 \times 4$ matrix whose columns are right singular vectors the right singular vectors $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4$ corresponding to the singular values. The final vector $\mathsf{v}_5$ is missing!

**Note:** The columns of the matrix `decomp$v$` are singular vectors (not the rows).

**Warning!** We were expecting `decomp$v` to be a $5 \times 5$ matrix, and R only returned a $5 \times 4$ matrix. What is wrong!? The missing column is the final orthonormal vector $\mathsf{v}_5$ from $\mbox{Nul}(A)$.  

#### Why has R Omitted this Vector? 

The answer is that **RStudio is being efficient.** R has omitted vector $\mathsf{v}_5$ because we do not need this vector to create the spectral decomposition of $A$. That spectral decomposition is
$$
A = \sigma_1 \mathsf{u}_1 \mathsf{v}_1^{\top} + \sigma_2 \mathsf{u}_2 \mathsf{v}_2^{\top} +  \sigma_3 \mathsf{u}_3 \mathsf{v}_3^{\top} +  \sigma_4 \mathsf{u}_4 \mathsf{v}_4^{\top}.
$$
Note that we do not use the vector $\mathsf{v}_5$. So R didn't calculate it for us (to save computation time). If $A$ had been a $10 \times 4$ matrix, this would be a pretty good idea: why find 6 vectors that we don't need?

And don't worry: we can easily extend the columns of `decomp$u` into an orthonormal basis for $\mathbb{R}^5$. One simple way is to find a basis for the orthogonal complement of $\mbox{span}(\mathsf{v}_1\mathsf{v}_2,\mathsf{v}_3,\mathsf{v}_4)$. We could do this by manually by finding an orthogonal basis for the nullspace of `t(decomp$v)`. 

If you take MATH 365 Computational Linear Algebra, you will learn more about the $QR$-decomposition. That gives a fast way to fill in the missing vectors if you need them. 


**Pro Tip:** When you use `svd` on a rectangular matrix, just remember that some of the singular vectors are missing. 

* If $m > n$ then there are $m-n$ missing columns of $U$, all lying in $\mbox{Nul}(A^{\top})$. 
* If $n > m$ then there are $n-m$ missing columns of $V$, all lying in $\mbox{Nul(A)}$. 



#### The Four Fundamental Subspaces of $A$

Keeping this is mind, let's characterize the four fundamental subspaces of our example $5 \times 4$ matrix $A.$

* The matrix $A$ has rank $3$ because there are three nonzero signular values.
*  $\mbox{Row}(A)$ is 3-dimensional with basis `decomp$v[,1:3]`
* $\mbox{Nul}(A)$ is 2-dimensional because $\sigma_4=0$ and there is $5-4=1$ missing left right singular vector. We can use `decomp$v[,4]` as one basis vector. We can find the remaining $5-4=1$ vector by finding the nullspace of `t(decomp$v)`.
* $\mbox{Col}(A)$ is 3-dimensional with basis `decomp$u[,1:3]`
* $\mbox{Nul}(A^{\top})$ is 1-dimensional. The vector `decomp$u[,4]` is a basis. 

### Your Turn: SVD of a Square Matrix

Characterize the four subspaces of this $5x5$ matrix. Note that since $A$ is square, the matrices $U$ and $V$ will both contain a full basis for $\mathbb{R}^5$.

```{r, echo=TRUE}
(A = cbind(c(-3,5,-3,6,-1),c(-1,-2,-1,2,-4),c(9,-4,9,-18,14),c(3,-4,3,-6,3),c(-2,11,-2,4,11)))

```


### Your Turn: SVD of a Tall Matrix

Characterize the four subspaces of this $6x4$ matrix. This time around, we will be missing $6-4=2$ of the left singular vectors that we need to extend `svd(A)$u$ into a basis of $\mathbb{R}^6$. These two singular vectors complete the basis for $\mbox{Nul(A^{\top})}$.

```{r, echo=TRUE}
x = c(1,2,3,4,5,6)
(A = cbind(x^0, x^1, x^2, x^3))

```


## Image Compression



### Converting a JPEG image into a Matrix

You may need to install the `jpeg` and `raster` packages. Let's find out.

* Click on the 'Packages' tab in the lower right window
* Either search or scroll to see if the the `jpeg` and `raster` are there. 
* If so, then click the checkbox to load the package into memory.
If the package is missing, then uncomment and run  the folllowing code chunk
```{r, echo=TRUE}
#install.packages('jpeg')
#install.packages('raster')
```

You've probably heard of JPEG image files. The `jpeg` package will allow us to import those images into R. The JPEG format uses image compression. So we need to turn them into a **raster (bitmap) image** which is a rectangular grid of pixels or dots. In other words a raster image is a matrix. The package `raster` will do this conversion for us.

```{r,message=FALSE, echo=TRUE}
library(jpeg)
library(raster)
```





The following defines two helper functions

* `get_image(filename)` reads in a JPEG file and convert it into a raster image.
* `plot_image(img)` creates a plot of the raster image

```{r, echo=TRUE}


# converts a JPEG file into a raster image (a numerical matrix)
# if the JPEG is a color image, it is converted to black and white.
get_image <- function(filename) {
  # read the jpeg file
  img = readJPEG(readBin(filename,"raw",1e6))
  img.dim = dim(img)
  
  # if this is a color image, we need to turn it into a grayscale image
  img = img[,,1]+img[,,2]+img[,,3]
  img <- img/max(img)
  img.dim = dim(img)
  
  return (img)
}


plot_image = function(img,...) {
  plot(2:1, type='n',xlab=" ",ylab= " ",...)
  rasterImage(img, 1.0, 1.0, 2.0, 2.0)
}




```



Let start by reading in a picture of a tartan.

```{r, fig.width=6, fig.height=6, echo=TRUE}
where.tartan = "https://upload.wikimedia.org/wikipedia/commons/e/ec/Burberry.jpg"
img = get_image(where.tartan)
#plot_image(img,main="Image")

dim(img)
prod(dim(img)) # prod(vec) = product of the entries of vec


```


Each entry of `img` is a value in $[0,1]$. This is the  **grayscale value** of a single pixel: value 0 corresponds to white and value 1 corresponds to black.  The matrix `img` is a $335 \times 333$ matrix. So to store the image, we need to store $111,555$  floating point numbers (!). You can see compression methods are essential in practice.



### SVD of a Raster Image


The `img` variable is just a matrix. So we can find its singular value decomposition. We find that there are some large gaps in the singular values.
```{r, echo=TRUE}
decomp =svd(img)$d
#plot(decomp,pch=19,cex=.5,col='blue')
#plot(decomp,pch=19,cex=.5,col='blue',ylim=c(0,5))
decomp[1:10]
```

### SVD Approximation of the Image


Now let's create an SVD approximation of the image. Here is some helper code for us to use. The functions that you will call are

* `plot_svd_approx(img, k)`: create the spectral decomposition corresponding to the $k$ largest singular values in the spectral decomposition.
* `get_svd_approx_error(img, k)`: reports the average pixel error of the approximation.
```{r,echo=TRUE}

# returns the spectral decomposition matrix for the first k singular values
svd_approx = function(A,k = floor(1/2*min(nrow(A),ncol(A)))) {
  decomp = svd(A)
  sings = decomp$d
  U = decomp$u
  V = decomp$v
  if(k==1)
    D=matrix(sings[1],nrow=1,ncol=1)
  else
    D=diag(sings[1:k])
  M=U[,1:k]%*%D%*%t(V[,1:k])
  return(M)
}


# gets the svd approximation of the image
get_svd_approx_img <- function(img,k) {
  approxIm = svd_approx(img,k)
  approxIm[approxIm<0] = 0
  approxIm[approxIm>1] = 1
  
  return (approxIm)
}

# returns the average pixel error for the svd approximation of the image
get_svd_approx_error <-  function(img, k) {
  approxImg = get_svd_approx_img(img,k)
  return  = mean(abs(img - approxImg))
}

# plots the SVD approximation of the image
plot_svd_approx=function(img,k){
  approxIm = get_svd_approx_img(img,k)
  plot(1:2, type='n')
  rasterImage(approxIm, 1.0, 1.0, 2.0, 2.0)

}




```

And here we show the singular value approximation with increasing numbers of singular values:
```{r, fig.width=6, fig.height=6, echo=TRUE}
#plot_svd_approx(img,1)
get_svd_approx_error(img,1)
```


### SVD of a Lighthouse

Now let's see how SVD does with this picture of a lighthouse. Use the appropriate code above to explore the SVD approximations of this lighthouse. How big must $k$ be in order to get an okay approximation? to get an approximation that you can't distinguish from the original?


```{r, echo=TRUE}
where.lighthouse  = "https://images.unsplash.com/6/lighthouse.jpg"
```


### Your Turn

Here are some JPEG files available on the web for you to try. Or you can use an image of your own choice! Have fun!
```{r, echo=TRUE}

# some jpeg images available on the web
where.cameraman = "https://www.macalester.edu/~dshuman1/data/cameraman_small.jpg"
where.tiger = "https://i.pinimg.com/originals/f2/b5/0b/f2b50b1cbdb7cd16fef50f5641d41e77.jpg"
where.flower= "https://www.amylamb.com/wp-content/uploads/2013/04/Gerbera-320x320.jpg"
where.pattern  = "https://previews.123rf.com/images/noppanun/noppanun1411/noppanun141100046/33287656-black-and-white-geometric-seamless-pattern-with-triangle-and-trapezoid-abstract-background-vector-ep.jpg"

```


<!--chapter:end:12-image-compression.Rmd-->

---
title: "day25.Rmd"
author: "Math 236"
date: "2/25/2021"
output: html_document
---

```{r}
A = cbind(c(-1,1),c(-4,-1))
eigen(A)
Arg(eigen(A)$values)/(2 * pi)*360
```

```{r,fig.height=4, fig.width=6}
# plot
N = 21
start = c(1,0)
X = matrix(0,nrow=2,ncol=N)
X[,1] = start
for (i in 2:N) {X[,i] = A %*% X[,i-1]}

plot(X[1,],X[2,],col='blue',xlim=c(-100000,100000),ylim=c(-100000,100000),xlab='x',ylab='y',pch=20,cex=1,type="l")
points(start[1],start[2],col="red")
grid(nx=10,ny=10)
```


<!--chapter:end:day25.Rmd-->

---
title: "Exploring Linear Independence and Dependence"
author: "Your Name Here"
date: "1/28/2021"
output: html_document
---

In this R exploration, we will work on solving nonhomogeneous $A x = b$ and homogeneous equations $A x = 0$, and we will look at the ideas of linear dependence and linear dependence. You will also get a little exposure to typesetting using the LaTeX features that are build into R markdown.

Remember that we will use the `pracma` package to get the `rref` function, so we first load it in:
```{r}
require("pracma")
```

## Example 1: a 7x9 integer matrix

Here is a 7 x 9 coeefficient matrix that we will use. These commands define it and echo it back.
```{r}
A = cbind(
  c(3, 0, 0, 1, -2, -4, 1), 
  c(5, -5, 0, 3, 3, 1, 4), 
  c(3, 5, -1,  1, -3, -3, 5), 
  c(4, -1, -2, 0, -1, 2, -3), 
  c(0, 17, 3, 0, -17, -29,  8), 
  c(-4, -1, -5, -2, -1, -4, 3), 
  c(5, 3, -4, -5, -2, -3, -1), 
  c(0, 5, -3, -2, -1, -5, 0),
  c(37, -10, -27, -29, 4, 7, -24))
A
```

And here is a vector b that we hope to use in solving A x = b.

```{r}
b = c(382, 51, -321, -314, -86, -170, -153)
b
```

You can augment A with b, and call it Ab, using `cbind`:

```{r}
Ab = cbind(A,b)
Ab
```

And row reduce using `rref`
```{r}
rref(Ab)
```

### Solution to the nonhomogeneous equations Ax = b

Write out the solution to Ax=b in *parametric* form using the following formatting. You just need to fill in the correct values of the vectors:
$$
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8 \\ x_9 \end{bmatrix}
=
\begin{bmatrix} p1 \\ p2 \\ p3 \\ p4 \\ p5 \\ p6 \\ p7 \\ p8 \\ p9 \end{bmatrix}
+ s
\begin{bmatrix} 
u1 \\ u2 \\ u3 \\ u4 \\ u5 \\ u6 \\ u7 \\ u8 \\ u9 
\end{bmatrix}
+ t
\begin{bmatrix} 
v1 \\ v2 \\ v3 \\ v4 \\ v5 \\ v6 \\ v7 \\ v8 \\ v9 
\end{bmatrix}
$$

Describe this solution space (by fixing up this sentence, which is incorrect right now): 

<center/>
the set of solutions to A x= b  is a *line* in $\mathbb{R}^3$.
<center>

### Solution to the nonhomogeneous equations Ax = 0

Now, describe the set of solutions to the homogeneous equations A x = 0. Again, you can just edit this:
$$
\begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\ x_7 \\ x_8 \\ x_9 \end{bmatrix}
=
\begin{bmatrix} p1 \\ p2 \\ p3 \\ p4 \\ p5 \\ p6 \\ p7 \\ p8 \\ p9 \end{bmatrix}
+ s
\begin{bmatrix} 
u1 \\ u2 \\ u3 \\ u4 \\ u5 \\ u6 \\ u7 \\ u8 \\ u9 
\end{bmatrix}
+ t
\begin{bmatrix} 
v1 \\ v2 \\ v3 \\ v4 \\ v5 \\ v6 \\ v7 \\ v8 \\ v9 
\end{bmatrix}
$$

And describe, in words, the geometric relationship between the solutions to Ax=b and Ax=0.

your answer here

### Linearly dependent columns

The columns of the matrix A are *linearly dependent.* You can see that in rref(A). 

```{r}
rref(A)
```


Discuss in your group how you see it. Then write out a dependence relation among the columns by filling in numbers for the weights in this equation

$$
0 = 
c_1 \vec{a}_1 + 
c_2 \vec{a}_2 +
c_3 \vec{a}_3 +
c_4 \vec{a}_4 +
c_5 \vec{a}_5 +
c_6 \vec{a}_6 +
c_7 \vec{a}_7 +
c_8 \vec{a}_8 +
c_9 \vec{a}_9.
$$
**Challenge**: give a dependency relation that none of the other groups in the class have.

This is telling us that there is some redundancy in the matrix A. Remove columns from A to get a new matrix M whose columns are *linearly independent.* You can do this by removing the appropriate columns from the code below:

```{r}
M = cbind(    # you need to edit this matrix
  c(3, 0, 0, 1, -2, -4, 1), 
  c(5, -5, 0, 3, 3, 1, 4), 
  c(3, 5, -1,  1, -3, -3, 5), 
  c(4, -1, -2, 0, -1, 2, -3), 
  c(0, 17, 3, 0, -17, -29,  8), 
  c(-4, -1, -5, -2, -1, -4, 3), 
  c(5, 3, -4, -5, -2, -3, -1), 
  c(0, 5, -3, -2, -1, -5, 0), 
  c(37, -10, -27, -29, 4, 7, -24)
  )
M
rref(M)
```

Your matrix should now be square (7x7) with linearly independent columns. R has a build in solve command, `solve`, that works for matrices of this form (i.e., square with linearly independent columns). You can try it here. First you need to un-comment-out the solve command. I have it commented out right now, because it does not work with the matrix M (above) until you remove its redundancies. 

```{r}
# solve(M,b)
```

Now, you should get a *unique* solution to the equation M x = b, since M has no free variables, and it should be one of the solutions to the original question A x = b. Which solution is it? That is, which of the many solutions to A x = b are you getting here (forw which values of the paramters?).

Compare this with trying to use solve on the original equation A x = b with linearly dependent columns. The solve command in the next bit of code is commented out. Delete the comment command and try executing it.

```{r}
# solve(A,b)
```

## A 5 x 6 Numerical Matrix

So far, all of the matrices we've worked with in this class have integer values. This is only so that the calulations are nice to do by hand. All of our theory works over the real numbers. Here we will look at a real matrix with numerical values, something you might  find when dealing with real-world data. 

```{r}
B = cbind(
  c(0.717, -0.274, 0.365, 0.482, -0.362), 
  c(0.587, -0.545, 0.5, -0.407, -0.597), 
  c(-0.441, 0.886, 0.784, -0.831, -0.594), 
  c(0.923, -0.466, 0.222, 0.867, 0.493), 
  c(-0.42, -0.745, -0.02, -0.44, 0.209), 
  c(0.621, 0.049, -0.134, -0.844, -0.31)
  )
B
```
and here is a vector d in $\mathbb{R}^5$.

```{r}
d = c(5.886, -4.001, 3.701, -6.621, -2.199)
d
```

Try answering some of these questions:

a. Are the columns of B linearly independent?
b. Do the columns of B span $\mathbb{R}^5$?
c. Give the parametric solution to B x = d.
d. What is the geometric form of this solution (e.g., a plane in $\mathbb{R}^4$)?
e. Remove redundancies from the columns of B to get a new matrix B2 and use solve to solve the equation B2 x = d. Which of the parametric solutions to you get.


## Random Matrices

The following code generates a random 5 x 5 matrix. Every time you enter it, it will give you a new matrix. Use this to try to figure out how likely it is that a random square matrix has linearly dependent columns. 

```{r}
R1 = matrix(runif(5*5), nrow = 5, ncol = 5)
R1
```

Try the same using the following code that generates a random 5 x 6 matrix. 

```{r}
R2 = matrix(runif(5*6), nrow = 5, ncol = 6)
R2
```

Try the same using the following code that generates a random 5 x 4 matrix. 

```{r}
R3 = matrix(runif(5*4), nrow = 5, ncol = 4)
R3
rref(R3)
```

In each of these cases, how likely is it that the columns of the matrix spans all of $\mathbb{R}^4$?

<!--chapter:end:Day6.Rmd-->

# (PART) Concepts of Linear Algebra  {-}


# Important Definitions


## Vector Spaces

span 

: A set of vectors $\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n$ **span** a vector space $V$ if for every $\mathsf{v} \in V$ there exist a set of scalars (weights) $c_1, c_2, \ldots, c_n \in \mathbb{R}$ such that 
$$
\mathsf{v} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n.
$$
*Connection to Matrices:* If $A = [\mathsf{v}_1 \mathsf{v}_2 \cdots \mathsf{v}_n]$ is the matrix with these vectors in the columns, then this is the same as saying that $\mathsf{x} = [c_1, \ldots, c_n]^{\top}$ is a solution to $A x  = \mathsf{v}$.

linear independence

: A set of vectors $\mathsf{v}_1, \mathsf{v}_2,\ldots, \mathsf{v}_n$ are **linearly independent** if the only way to write 
$$
\mathsf{0} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n
$$
is with $c_1 = c_2 = \cdots = c_n = 0$. 
<br/>
*Connection to Matrices:* If $A = [\mathsf{v}_1 \mathsf{v}_2 \cdots \mathsf{v}_n]$ is the matrix with these vectors in the columns, then this is the same as saying that $A x = \mathsf{0}$ has only the trivial solution.


linear dependence

: Conversely, a set of vectors $\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n$ are **linearly dependent** if there exist scalars $c_1, c_2,\ldots, c_n \in \mathbb{R}$ that are **not all equal to 0** such that
$$
\mathsf{0} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n
$$
This is called a **dependence relation** among the vectors. 
<br/>
*Connection to Matrices:* If $A = [\mathsf{v}_1 \mathsf{v}_2 \cdots \mathsf{v}_n]$ is the matrix with these vectors in the columns, then this is the same as saying that $\mathsf{x} = [c_1, c_2, \ldots, c_n]^{\top}$ is a nontrivial solution to $A \mathsf{x} = \mathsf{0}$.


linear transformation

: A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is a **linear transformation** when: 

    * $T(\mathsf{u} + \mathsf{v}) = T(\mathsf{u}) + T(\mathsf{v})$ for all $\mathsf{u}, \mathsf{v} \in \mathbb{R}^n$ (preserves addition)
    * $T(c \mathsf{u} ) = c T(\mathsf{u})$ for all $\mathsf{u} \in \mathbb{R}^n$ and $c \in \mathbb{R}$  (preserves scalar multiplication).
It follows from these that also $T(\mathsf{0}) = \mathsf{0}$.

one-to-one 

: A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is a **one-to-one** when:

<center>
for all $\mathsf{y} \in \mathbb{R}^m$ there is **at most** one $\mathsf{x} \in \mathbb{R}^n$ such that $T(\mathsf{x}) = \mathsf{y}$.
</center>

onto 

: A function $T: \mathbb{R}^n \to \mathbb{R}^m$ is a **onto** when:

<center>
for all $\mathsf{y} \in \mathbb{R}^m$ there is **at least** one $\mathsf{x} \in \mathbb{R}^n$ such that $T(\mathsf{x}) = \mathsf{y}$.
</center>

subspace

: A subset $S \subseteq \mathbb{R}^n$ is a **subspace** when:

    * $\mathsf{u} + \mathsf{v}  \in S$ for all $\mathsf{u}, \mathsf{v} \in S$ (closed under addition)
    * $c \mathsf{u} \in S$ for all $\mathsf{u}\in S$ and $c \in \mathbb{R}$ (closed under scalar multiplication)
It follows from these that also $\mathsf{0} \in S$.



basis

: A **basis** of a vector space (or subspace) $V$ is a set of vectors $\mathcal{B} = \{\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\}$  in $V$ such that

    * $\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n$ span $V$
    * $\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n$ are linearly independent
Equivalently, one can say that $\mathcal{B} = \{\mathsf{v}_1, \mathsf{v}_2, \ldots, \mathsf{v}_n\}$ is a basis of $V$ if for every vector $\mathsf{v} \in V$ there is a **unique** set of scalars $c_1, \ldots, c_n$ such that
$$
\mathsf{v} = c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + \cdots + c_n \mathsf{v}_n.
$$
(The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence).

dimension

: The **dimension** of a subspace $W$ is the number of vectors in any basis of $W$. This is also the fewest number of vectors required to span the subspace.


## Matrices


invertible

: The square $n \times n$ matrix $A$ is **invertible** when there exists an $n \times n$ matrix $A^{-1}$ such that $A A^{-1} = I = A^{-1} A$. The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that $A$ is invertible.

null space

: The **null space** $\mbox{Nul}(A) \subset \mathbb{R}^n$ of the $m \times n$ matrix $A$ is the set of solutions to the homogeneous equation $A \mathsf{x} = \mathbf{0}$> We also write this as
$$
\mbox{Nul}(A) = \{ \mathsf{x} \in \mathbb{R}^n : A \mathsf{x} = \mathbf{0} \}
$$
*Connection to Linear Transformations*: If $T(\mathsf{x}) = A \mathsf{x}$, then the kernel of $T$ is the null space of matrix $A$. 


column space

: The **column space** $\mbox{Col}(A) \subset \mathbb{R}^m$ of the $m \times n$ matrix $A$ is the set of all linear combinations of the columns of $A$. 
For $A = \begin{bmatrix} \mathsf{a}_1 & \mathsf{a}_2 & \cdots & \mathsf{a}_n \end{bmatrix}$, we have
$$
\mbox{Col}(A) = \mbox{span} ( \mathsf{a}_1, \mathsf{a}_2, \ldots , \mathsf{a}_n )
$$
We also write this as
$$
\mbox{Col}(A) = \{ \mathsf{b} \in \mathbb{R}^m : \mathsf{b} = A \mathsf{x} \mbox{ for some } \mathsf{x} \in \mathbb{R}^n \}.
$$
*Connection to Linear Transformations*: If $T(\mathsf{x}) = A \mathsf{x}$, then the range (also called the image) of $T$ is the column space of matrix $A$. 


rank

: The **rank** of the $m \times n$ matrix $A$ is the dimension of the column space of $A$. This is also the number of pivot columns of the matrix.

eigenvalue and eigenvector

: For a square $n \times n$ matrix $A$, the scalar $\lambda \in \mathbb{R}$ is an **eigenvalue** for $A$ when there exists a nonzero vector  $\mathsf{x} \in \mathbb{R}^n$ such that $A \mathsf{x} = \lambda \mathsf{x}$. The nonzero vector $\mathsf{x}$ is the **eigenvector** for eigenvalue $\lambda$. The collection of all of these eigenvalues and eigenvectors is called the **eigensystem** of A.


diagonalization

: A square $n \times n$ matrix is **diagonalizable** when $A = P D P^{-1}$ where $D$ is a diagonal matrix and $P$ is an invertible matrix.  In this case, the eigenvalues of $A$ are the diagonal entries of $D$ and their corresponding eigenvectors are the columns of $P$.

dominant eigenvalue

: The eigenvalue $\lambda$ of  the square matrix $A$ is the **dominant eigenvalue** when  $| \lambda | > | \mu |$ where $\mu$ is any other eigenvalue of $A$. The dominant eigenvalue determines the long-term behavior of $A^t$ as $t \rightarrow \infty$.

## Orthogonality

length 

: The **length** of a vector $\mathsf{v}$ is 
$$
\| \mathsf{v} \| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}.
$$

distance and angle

: The **distance** between vectors $\mathsf{u}$ and $\mathsf{v}$ is
$$
\mbox{dist}(\mathsf{u},\mathsf{v}) = \| \mathsf{u} - \mathsf{v} \|.
$$
The **angle** $\theta$ between these vectors is determined by
$$
\cos \theta = \frac{\mathsf{u} \cdot \mathsf{v}}{ \| \mathsf{u} \| \, \| \mathsf{v} \|}.
$$

orthogonal

: The vectors $\mathsf{u}$ and $\mathsf{v}$ are **orthogonal** when $\mathsf{u} \cdot \mathsf{v} = 0$. This means that either one of them is the zero vector, or they are perpendicular to one another.

orthogonal complement

: If $W \subset \mathbb{R}^n$ is a subspace, then its **orthogonal complement** $W^{\perp}$ is the set of all vectors in $\mathsf{R}^n$ that are orthogonal to $W$. We also write
$$
W^{\perp} = \{ \mathsf{v} \in \mathbb{R}^n : \mathsf{v} \cdot \mathsf{w} \mbox{ for all } \mathsf{w} \in W \}.
$$

orthonormal set

: A collection of vectors $\mathsf{u}_1, \mathsf{u}_2, \ldots, \mathsf{u}_k$ are an **orthonormal set** when every vector has length 1 and the vectors are pairwise orthogonal.
orthogonal matrix

orthogonal matrix

: A square $n \times n$ matrix $P$ is an **orthogonal matrix** when its columns are an orthonormal set. As a result, we have $P^{-1} = P^{\top}$.

projection and residual

: The **orthogonal projection**  of vector $\mathsf{y}$ into a subspace $W$ is the unique vector $\hat{\mathsf{y}} \in W$ such that $\mathsf{z} = \mathsf{y} - \hat{\mathsf{y}} \in W^{\perp}$. The vector $\mathsf{z}$ is called the **residual vector** for the projection.


## Spectral Decompostion

orthogonal diagonalization

: Every symmetric $n \times n$ matrix is **orthogonally diagonalizable**, meaning that we have $A = P D P^{\top}$ where $D$ is a diagonal matrix and $P$ is an orthogonal matrix. The diagonal entries of $D$ are the eigenvalues of $A$ and the columns of $P$ are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of $A$ are nonnegative.

spectral decomposition

: A symmetric matrix $A$ can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of $A$. In particular, we have
$$
A = \lambda_1 \mathsf{u}_1 \mathsf{u}_1^{\top} + \lambda_2 \mathsf{u}_2 \mathsf{u}_2^{\top} + \cdots + \lambda_n \mathsf{u}_n \mathsf{u}_n^{\top}. 
$$
This linear combination of rank 1 vectors is called the **spectral decomposition** of $A$.


singular value decomposition (SVD)

: Any $m \times n$ matrix $A$ of rank $r$ can be factored into its **singular value decomposition** $U \Sigma V^{\top}$ where 

* $U$ is an $m \times m$ orthogonal matrix,
* $\Sigma$ is a matrix whose nonzero entries are the positive numbers $\sigma_1, \ldots , \sigma_r$, which appear in decreasing order on the diagonal, and 
* $V$ is an $n \times n$ orthogonal matrix.
The nonzero entries of $\Sigma$ are called the singular values of $A$. The columns of $U$ are  the left singular vectors and the rows of $V^{\top}$ are the right singular vectors.

SVD spectral decomposition

: Any $m \times n$ matrix $A$ of rank $r$ can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of $A$. In particular, we have
$$
A = \sigma_1 \mathsf{u}_1 \mathsf{v}_1^{\top} + \sigma_2 \mathsf{u}_2 \mathsf{v}_2^{\top} + \cdots + \sigma_r \mathsf{u}_r \mathsf{v}_r^{\top}. 
$$
This linear combination of rank 1 vectors is called the **(SVD) spectral decomposition** of $A$.

<!--chapter:end:important-defs.Rmd-->

---
title: "NetworkCentrality"
author: "Math 236"
date: "3/2/2021"
output: html_document
---


#  Network Centralities

[Download this Rmd file](https://github.com/Tom-Halverson/math236_fs21/blob/main/09-network-analysis.Rmd)


In this example, we will use a package called `igraph`. To install it, you need to go to the packages window (bottom right), choose `install`, and search for and install `igraph` from the packages window.

```{r,message=FALSE, echo=TRUE}
library(igraph)
```
The igraph R package isn't all that well documented. Here are some places to look for documentation if you want to learn about other features. Let me know if you find any other good references:

* http://kateto.net/netscix2016
* http://igraph.org/r/doc/aaa-igraph-package.html

## Graphs and Networks

Graphs consists of **vertices** and the **edges** between them. These edges are used to model connections in a wide array of applications, including but not limited to, physical, biological, social, and information networks. To emphasize the application to real-world systems, the term *Network Science* is sometimes used. So we will use the terms **graph** and **network** interchangeably.  In this application, we will see that linear algebra is an important tool in the study of graphs. 

### Adjacency Matrices

Matrices are used to represent graphs and networks in a very direct way: we place a 1 in position $(i,j)$ of the **adjacency matrix** $A$ of the graph $G$, if there is an edge from vertex $i$ to vertex $j$ in $G$. Here is the adjacency matrix we will use today. 
```{r, echo=TRUE}
A = rbind(
  c(0,1,0,1,0,0,0,0,1,0,0,0), c(1,0,1,1,1,0,1,0,0,0,0,0),
  c(0,1,0,0,1,0,0,0,0,0,0,0), c(1,1,0,0,0,1,0,1,0,0,0,0),
  c(0,1,1,0,0,0,1,1,0,0,0,1), c(0,0,0,1,0,0,1,0,0,0,0,0),
  c(0,1,0,0,1,1,0,1,0,0,0,0), c(0,0,0,1,1,0,1,0,0,1,1,0),
  c(1,0,0,0,0,0,0,0,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0),
  c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,1,0,0,0,0,0,0,0))
A
```


We make a graph from the adjacency matrix as follows:
```{r, network1, fig.height=6, fig.width=6, echo=TRUE}
g=graph_from_adjacency_matrix(A,mode='undirected')
plot(g, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

Observe that there is an edge from vertex $i$ to vertex $j$ if and only if there is a 1 in position $(i,j)$ in the matrix. 

This network is the route map of a small airline. We will add vertex labels and change the vertex size:

```{r, network2, fig.height=6, fig.width=6, echo=TRUE}
airports = 
  c("ATL","LAX","ORD","MSP","DEN","JFK","SFO","SEA","PHL","PDX","MDW","LGA")
V(g)$label = airports
plot(g,vertex.size=30, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

### Graph Layouts

There are a variety of graph layout algorithms which place the vertices in the plane. You can find many algorithms [in the igraph documentation](https://igraph.org/c/doc/igraph-Layout.html). For example, here is a layout on a circle

```{r,  network3, fig.height=6, fig.width=6, echo=TRUE}
coords = layout_in_circle(g)
plot(g, layout=coords, vertex.size = 30,vertex.label.cex=0.85, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

The **Fruchterman-Reingold algorithm** is one of the most popular graph vertex layout algorithms. It is a force-directed layout that tries to get a nice-looking graph where edges are similar in length and cross each other as little as possible. The algorithm simulates the graph as a physical system. Vertices are electrically charged particles that repulse each other when they get too close. The edges act as springs that attract connected vertices closer together. As a result, vertices are evenly distributed through the chart area. The resulting layout is intuitive: vertices which share more connections are closer to each other. 

```{r, network4, fig.height=6, fig.width=6, echo=TRUE}
coords = layout_with_fr(g)
plot(g, layout=coords, vertex.size = 30, vertex.label.cex=0.85, vertex.color='tan1', vertex.frame.color="dodgerblue")
```

We can also choose to layout vertices by hand:

```{r, network5, fig.height=6, fig.width=6, echo=TRUE}
locations = rbind(
  c(20,0),c(-10,0),c(11,7),c(10,15),c(3,12),c(25,10),
  c(-10,10),c(-12,15),c(20,6),c(-15,12),c(12,4),c(25,13)
)
plot(g,vertex.size=20, layout=locations, vertex.label.cex=0.85, vertex.color='tan1', vertex.frame.color="dodgerblue")
```


## Degree Centrality


If we are considering placing an office in one of our airport locations, we may want to chose the most **central** hub for that office. It turns out that there are many interesting **centrality measures** for networks. We will talk about two of them today.

The simplest measure centrality is the **degree** of the vertex, or the number of edges connected to that vertex. We calculate the degree centralities from the adjacency matrix as follows:

1. First make a vector $\mathsf{v}$ of all 1's; then 
1. multiply $\mathsf{p} = A\mathsf{v}$ to get the degree proportions; and 
1. divide the vector $\mathsf{p}$ by the sum of its entries. 

The result is a **normalized vector** $\mathsf{p}$ whose entries sum to 1. Each entry of vector $\mathsf{p}$ represents to *proportion* of edges incident with the corresponding vertex.

```{r, echo=TRUE}
v=rep(1,nrow(A)) # all 1s vector
d = A %*% v  # degrees
p=d/sum(d)   # proportion of degrees
dp = cbind(d,p) # show d and p together side-by-side in a matrix
rownames(dp) = airports
colnames(dp) = c("degree","proportion")
dp
```

We can also sort the vertices by degree. 

```{r,echo=TRUE}
ii=order(d,decreasing=TRUE)  # find the decreasing order of d
dp2 = dp[ii,]    # sort the data frame in that order
dp2
```

Now let's create a **data visualization.** We plot the network and size each vertex according to the vector $p$. The larger vertices have more edges connected to them. This conveys information to the viewer about the relative importance of the vertices. 

```{r, network7, fig.height=6, fig.width=6, echo=TRUE}
plot(g, layout=locations, vertex.size=250*p,vertex.label.cex=0.65, vertex.color='tan1', vertex.frame.color="dodgerblue")
```


## Gould's Index

Gould's Index is a measure of centrality that uses the dominant eigenvector of a matrix. It was introduced by geographer P. R. Gould in 1967 to analyze the geographical features on maps. We will build up Gould's Index step-by-step so that we can understand what it measures. 

### Step 1: Add Layovers

The first step is typically to add the identity matrix to the adjancency matrix $A$ to get a new matrix 
$$
B = A + I.
$$ 
The $n \times n$ identity matrix in `R` is obtained by using `diag(n)`. Adding the identity gives a connection from a vertex to itself. This **loop edge** corresponds to staying at the current city during a layover. 

```{r, echo=TRUE}
(B = A + diag(nrow(A)))
```

Here is what the corresponding network (with layovers) looks like. You can see why we refer to these additional edges as "loops." However, we usually do not draw the network with these added loops to keep the figure less cluttered.


```{r, networkB, fig.height=6, fig.width=6, echo=TRUE}
g2=graph_from_adjacency_matrix(B,mode='undirected')
airports = 
  c("ATL","LAX","ORD","MSP","DEN","JFK","SFO","SEA","PHL","PDX","MDW","LGA")
V(g2)$label = airports
coords = layout_with_fr(g2)
plot(g2, layout=coords, vertex.color='tan1', vertex.frame.color="dodgerblue")
```


### Step 2: Dynamical System

Starting with the all 1's vector $\mathsf{v}_0 = [1, 1, \ldots ,1 ]^{\top}$ create the dynamical system
$$
\mathsf{v}_0, \quad  
\mathsf{v}_1 = B \mathsf{v}_0,  \quad 
\mathsf{v}_2 = B \mathsf{v}_1,  \quad
\mathsf{v}_3 = B \mathsf{v}_2,  \quad \cdots , \quad
\mathsf{v}_n  = B \mathsf{v}_{n-1}.
$$
Here we calculate $\mathsf{v}_1, \ldots, \mathsf{v}_{10}$ using a loop:
```{r, echo=TRUE}
N = 10 
X = matrix(0,nrow=nrow(B),ncol=N+1) # make a a table of 0s
X[,1] = rep(1,nrow(B))              # put v0 in first column
for (i in 1:N) {                  # loop N times
   X[,i+1] = B %*%  X[,i]   # apply B to the ith column and make it the (i+1)st column      
}
rownames(X) = airports
colnames(X) = 0:10
X
```

**Discuss with your group:** Each of the entries of the vector $\mathsf{v}_{t}$ in the columns of the table above corresponds to "a trip of length $t$." What kinds of trips does the $i$th entry of $\mathsf{v}_{t}$ count? Here is how you can figure this out:

1. Compare the table of vectors with the picture of the network with layovers.
1. Start by looking at the $t=1$ column. The $i$th entry has something to do with the $i$th city. 
1. Next, look at the $t=2$ column. And so on.
1. Once you have noticed the connection between the network and data, *explain why the rule $\mathsf{v}_t = B \mathsf{v}_{t-1}$ leads to this result*.
1. These numbers get big fast! Let's normalize by dividing by the sum each time. The vectors will always be proportions then, which sum to 1. See the table below. What do the entries in this table tell us?

```{r, echo=TRUE}
N = 10 
X = matrix(0,nrow=nrow(B),ncol=N+1)
X[,1] = rep(1,nrow(B))
for (i in 2:(N+1)) {  
   X[,i] = B %*%  X[,i-1]
   X[,i] = X[,i]/sum(X[,i])
}
rownames(X) = airports
colnames(X) = 0:10
X
```

### Step 4: Eigen-analysis 

We see that the vectors are converging to a common direction, and 
we know that dynamical systems converge to the **dominant eigenvector** (if there is one). We can see below that there is a dominant eigenvector in this case. 
```{r, echo=TRUE}
eigen(B)
```

For an adjacency matrix $A$, the dominant eigenvector of $B + I$, scaled to sum to 1, is called **Gould's Index** of network centrality. Here we extract it, scale it to sum to 1, and we show that the dynamical system is converging to it.

```{r, echo=TRUE}
# Get the dominant eigenvector
vecs = eigen(B)$vectors
gould = vecs[,1]
gould = gould/sum(gould)

# Compute the dynamical system
N = 30
X = matrix(0,nrow=nrow(B),ncol=N+1)
X[,1] = rep(1,nrow(B))/nrow(B)
for (i in 1:N) {  
   X[,i+1] = B %*%  X[,i]
   X[,i+1] = X[,i+1]/sum(X[,i+1])
}

# Display the data
Y = cbind(X[,1],X[,2],X[,3],X[,11],X[,21],X[,31],gould)
rownames(Y) = airports
colnames(Y) = cbind("n=0","n=1","n=2","n=10","n=20","n=30","Gould")
Y
```


### Step 5

Now let's plot the network with:

* the vertices sized by Gould's Index
* the labels sized by degree centrality

```{r, network6, fig.height=6, fig.width=6, echo=TRUE}
plot(g, layout=locations, vertex.size=250*gould,vertex.label.cex=8*p, vertex.color='tan1', vertex.frame.color="dodgerblue" )
```

And we show the data containing Gould's Index and the Degree Centrality. We order the data using the Gould Index and then compare the two. Observe that degree centrality and Gould's Index do not always agree.
```{r, echo=TRUE}
Z = cbind(gould,p) 
rownames(Z)=airports
colnames(Z)=c('Gould', 'Degree')
ii=order(gould,decreasing=TRUE)
Z = Z[ii,]
Z
```

**Discuss with your group:** Degree centrality and Gould's Index give different rankings. Look at the table and observe that:

* LAX, DEN and SEA have the same degree centrality. However LAX and DEN have higher Gould Index than SEA.
* SFO has lower degree centrality than SEA, but higher Gould centrality! So these two centralities give different rankings.
* Why does the Gould Index value SFO more than SEA?
* Find another pair of cities where the rankings of degree centrality and Gould's Index differ. Look at the plot of the network and explain why this is the case.


### Gould Index Summary

Now that we understand what Gould's Index means, let's summarize how to find the Gould Index values for an adjacency matrix $A$.

1. Create the matrix $B = A+I$.
2. Find the dominant eigenvector $\mathbf{v}$ of $B$.
3. Normalize the values of $\mathbf{v}$ so that the entries sum to 1.



## Your Turn: The Rise of Moscow

Russian historians often attribute the dominance and rise to power of Moscow to its strategic position on medieval trade routes (see Figure 1). Others argue that sociological and political factors aided Moscow’s rise to power, and thus Moscow did not rise to power strictly because of its strategic location on the trade routes. The figure below shows the major cities and trade routes of medieval Russia.

![](https://raw.githubusercontent.com/Tom-Halverson/math236_s21/main/images/MedievalRussia.png){width=100%}

Use Gould’s Index to form a geographer's opinion about this debate. Either:

* Moscow’s location was the primary reason for its rise to power, or 
* Other forces must have come into play.

Here is the adjacency matrix for this transportation network into an adjacency matrix and a plot of the network.
```{r, networkRussia, fig.height=8, fig.width=8, echo=TRUE}
RusCity = c("Novgorod", "Vitebsk", "Smolensk", "Kiev", "Chernikov",
"Novgorod Severskij", "Kursk", "Bryansk", "Karachev", "Kozelsk",
"Dorogobusch", "Vyazma", "A", "Tver", "Vishnij Totochek", "Ksyatyn",
"Uglich", "Yaroslavl", "Rostov", "B", "C", "Suzdal", "Vladimir",
"Nizhnij Novgorod", "Bolgar", "Isad'-Ryazan", "Pronsk", "Dubok",
"Elets", "Mtsensk", "Tula", "Dedoslavl", "Pereslavl", "Kolomna",
"Moscow", "Mozhaysk", "Dmitrov", "Volok Lamskij", "Murom")
A = rbind(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(1, 0, 
    1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 1, 0, 1, 0, 0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 
    1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
    0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 
    0, 1, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(1, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 1, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
    1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 
    0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
    0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
    1, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 
    0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 
    0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))
g=graph_from_adjacency_matrix(A,mode='undirected')
V(g)$label = RusCity
# Plot network
plot(g)
```

a. Create a vector containing the normalized Degree Centralities. See Section \@ref(degree-centrality) for help.

a. Create a vector containing the Gould Index values. See Section \@ref(gould-index-summary) for help.

a. Plot the network where the size of the vertices is determined by Gould's Index and the size of the label is determined by degree centrality.

a. Create a data frame that contains Gould's Index and Degree Centralities. The rows should be labeled with the city names and the columns should be named by the centrality measures.  Sort according to Gould's Index.

a. Use Gould's Index to decide whether Moscow's dominance was solely due to its geographic location.

a. Compare the Gould's Index and Degree Centrality rankings and note any interesting findings. See Section \@ref(step-5) for help.

<!--chapter:end:NetworkCentrality.Rmd-->

---
title: "Template for Problem Set 7"
author: "Your Name Here"
date: "Due Date Here"
output:
  html_document: default
---


# Problem Set 7

**Note:** If you are using this PS 7 Template then you need to submit a PDF file.

* The easiest way is to (1) knit to HMTL, (2) click the "Open in Browser" button, (3) In the opened tab, "Print... Save as PDF"
 * There is a way to knit to PDF, but it is not working for me (error: missing framed.sty). I tried to install the missing resources, but I didn't get it to work. Given the effort involved, I think that the "easy way" HMTL -> Print to PDF is faster.

```{r, echo=FALSE}

# run this command to load some practical math functions
require(pracma)

# Creates a trajectory for the dynamical system
# A = the matrix
# start = the initial vector
# N = number of iterations
get_trajectory <- function(A, start, N) {
    ### this code follows the populations for N steps
  m = dim(A)[1]  # m is the number of rows of L
  X = matrix(0, nrow=m, ncol=N)  #  Store the results in a (m x N) matrix called X
  X[,1] = start  # put start in the first column of X
  
  # loop N times and put your results in X
  for (i in 2:N) {
    X[,i] = A %*% X[,i-1] 
  }
  
  return(X)
}


# Plots a trajectory along a time axis
# X  = the trajectory
# title = the title for the plot
# types = a vector of the names for each of the entries of the vector.
plot_trajectory <- function(X, title, types) {

  m = dim(X)[1]
  N = dim(X)[2]

  t = seq(1,N)  # time

  print(dim(X))
  print(dim(t))
    
  # Expand right side of clipping rect to make room for the legend
  par(xpd=T, mar=par()$mar+c(0,0,0,10))
  
  ymin = min(0, 1.1 * min(X))
  ymax = max(0, 1.1 * max(X))
  
  # Plot graph 
  plot(t, X[1,], type='l', col=1, ylim=c(ymin,ymax), ylab="amount", xlab="time",  main=title)
  
  for (i in 1:m) {
    lines(t, X[i,], col=i)
    points(t,X[i,], col=i, pch=20, cex=.8)
  }
  
  # Plot legend where you want
  legend(N *1.1, ymax * .85, types, col=1:m, lty = 1)
  
  # Restore default clipping rect
  par(mar=c(5, 4, 4, 2) + 0.1)
}

```


## Glucose-Insulin System Model


  a. My plot showing the trajectory and my observation about how long it takes for $\begin{bmatrix} G_{t} \\  H_{t} \end{bmatrix}$ to return  to homeostatis.  

  b. My answer about $\lambda = a \pm b i$ and $|\lambda| = \sqrt{a^2 +  b^2}$ and $\theta = \tan^{-1} (a/b)$. I'll include the R commands for these calculations by using an `{r echo=TRUE}' block like this one.
  
```{r, echo=TRUE}

# setting echo=TRUE means that the code will be shown in the knit output

a=1/2
b=sqrt(3)/2
atan(a/b) * 360 / (2 * pi)

```



## Population Interaction Models



a.  My answer about the constants $a,b,c,d$. I'll use the ``tabledata`` variable below.  I will replace each `'\t\t'` with the appropriate sentence **inside the quotemarks**, for example, `'a > 1'`


```{r, echo=FALSE}


tabledata = rbind(c('predation', '\t\t', '\t\t', '\t\t', '\t\t'),
                  c('competition', '\t\t', '\t\t', '\t\t', '\t\t'),
                  c('symbiosis', '\t\t', '\t\t', '\t\t', '\t\t'))


tableframe = data.frame(tabledata)
names(tableframe) = c('model', 'a', 'b', 'c', 'd')



knitr::kable(
  tableframe, booktabs = TRUE
)
```


b. My answer matching each of $A,B,C$ to one of the models. I could use a bullet list by editing the answer below.

* $A$ is predation/competition/symbiosis
* $B$ is predation/competition/symbiosis
* $C$ is predation/competition/symbiosis

c. My answer matching each of $A,B,C$ to one of the vector fields I could use a bullet list by editing the answer below.

* $A$ is Vector Field 1/2/3
* $B$ is Vector Field 1/2/3
* $C$ is Vector Field 1/2/3


d. My answer about the initial population is $[X_0, Y_0]^{\top} = [1, 2.5]^{\top}$ for each model. I could use a bullet list by editing the answer below.

- **Matrix $A$:** My description of the trajectory and its long term behavior, including the limiting population ratios $[a, b]^{\top}$.
- **Matrix $B$:** My description of the trajectory and its long term behavior, including the limiting population ratios $[a, b]^{\top}$.
- **Matrix $C$:** My description of the trajectory and its long term behavior, including the limiting population ratios $[a, b]^{\top}$.

##  Blue Whale Population Dynamics



```{r, echo=TRUE}
(L=rbind(c(0,0,.19,.44,.5,.5,.45),
         c(.77,0,0,0,0,0,0),
         c(0,.77,0,0,0,0,0),
         c(0,0,.77,0,0,0,0),
         c(0,0,0,.77,0,0,0),
         c(0,0,0,0,.77,0,0),
         c(0,0,0,0,0,.77,.78)))

```




a. My plot of the blue whales for 40 years, and my observation about whether this is population growth, equilibirum or  extinction,  


b. The R block with `{r echo=TRUE}` to calculate the eigensystem, and my conclusions about the growth rate and long term population ratios.


c. My final R block with `{r echo=TRUE}` that shows my value for $h$ and confirms that the populations are stable in the long run. My observation about the long-term population ratios.


## The Power Method for Eigenvalue Calculation

 

```{r, echo=TRUE}
A = cbind(c(4,2,4,8), c(2,2,4,4), c(4,4,9,1),c(8,4,1,5))

A

eigen(A)


estimate_dominant_eigenvector <- function(A, numiter) {
  n = dim(A)[1]
  x = rep(1,n)
  
  for (i in 1:numiter) {
    y = A %*% x
    maxval = max(y)
    minval = min(y)
    
    if (abs(maxval) > abs(minval)) {
      m = maxval
    }  else {
      m = minval
    }
    
    x = y/m
  }
  
  est = 'estimate'
  attr(est,'value') = m
  attr(est, 'vector') = x
  
  return(est)
}

estimate_nearest_eigenvector <- function(A, c, numiter) {
  n = dim(A)[1]
  
  B = A - c * diag(n)
  Binv = solve(B)

  est = estimate_dominant_eigenvector(Binv, numiter)
  
  val = attr(est, 'value')
  newval = 1/val+c
  
  attr(est,'value') = newval
  
  return (est)
}
```


a. My R code block with `{r echo=TRUE}` that shows the minimum `numiter` for `estimate_dominant_eigenvector' to converge to the right answer (up to 7 decimal places).


b. My answer to the four parts of this question.

    1. My answer that shows that  $(A - cI)v = (\lambda - c) v$

    2. My answer that shows that the zero vector is the only solution to $(A-cI)x=0$.

    3. My answer that shows $B v = \frac{1}{\lambda -c} v$.

    4. My answer that explains why $\frac{1}{\lambda -c}$ is the dominant eigenvalue.

&nbsp;

c. My R code block with `{r echo=TRUE}` which shows that `estimate_nearest_eigenvalue' does find the correct eigenvalue for an initial guess of $c$.



<!--chapter:end:PS-1B.Rmd-->

---
title: "Problem 2.9"
author: "Your Name"
date: "02/02/2021"
output:
  html_document: default
  pdf_document: default
---

#### Enter the matrix A
We have to load the pracma package in order to get `rref`.
```{r}
require(pracma)
```

Here is the matrix A for this problem.
```{r}
A = cbind(c(12,-7,9,-4,8),c(10,-6,9,-3,7),c(-6,4,-9,-1,-5),c(8,-5,9,0,6),c(4,-7,9,-8,1),c(-18,16,-27,9,-12))
A
```
#### (a) Linear Dependence

Show that the columns of A are linearly dependent by finding two different dependency relations among them. You can write your answer a form  like 5 a1+ 4 a2 + 3 a3 + 2 a4 + a5 = 0, where a1, a2, etc are the columns of A. 

```{r}

```



#### (b) Ax = b

Augment A with b and show that A x = b is consistent and has infinitely many solutions.

```{r}

```


#### (c) Remove the redundancies

Remove the free-variable columns from A to get a new, smaller matrix A'. Show that A' x = b has a unique solution and say what that solution is.

```{r}

```


<!--chapter:end:PS-2.9-template.Rmd-->

# (PART) Problem Sets {-}

# Problem Set 1-A

* Due: Tuesday January 26 by noon CST. 
* Week 1 and Week 8 are half weeks, so those two assignments will be split in two and called Problem Sets 1A and 1B.
* Upload your solutions to problems 1--4 by writing them out by hand, scanning them to pdf using a scanning software such as AdobeScan, assembling them into a single PDF, and uploading it to  Moodle. 
* Problem 1.5 is to be done using RStudio. To solve it, create an Rmarkdown file, knit it to .html, and upload the .html on Moodle along with the PDF for questions 1-4.

## Characterize the Solution Set

The following augmented matrices are in row echelon form. Decide whether the set of solutions  is a point, line, plane, or the empty set in 3-space. Briefly justify your answer.

a.
$\left[
\begin{array}{ccc|c}
1 & 3 & -1 & 4 \\
0 & 1 & 4   & 0\\
0 &  0 & 0  & 2 \\
\end{array}
\right]$

b.
$\left[
\begin{array}{ccc|c}
1 & 3 & -1 & 5 \\
0 & 0 & 0 & 0 \\
0 &  0 & 0 & 0 \\
\end{array}
\right]$

c.
$\left[
\begin{array}{ccc|c}
1 & -1 & 0  & -2 \\
0 & 0 & 1  & 7\\
0 &  0 & 0 & 1\\
\end{array}
\right]$

d.
$\left[ 
\begin{array}{ccc|c}
0 & 1 & 0 & 6 \\
0 & 0 & 1 & -2 \\
0 &  0 & 0 & 0 \\
\end{array}
\right]$


## Find the General Solution

Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. 

a.
$\left[
\begin{array}{cccc|c}
1 & 3 & 0 & -2 & 5\\
0 & 0 & 1 & 4 & -2 \\
\end{array}
\right]$

b.
$\left[
\begin{array}{ccccc|c}
1 & 0 & 4 & 0 & 3 & 6\\
0 & 1 & 1 & 0 & -2& -8 \\
0 & 0 & 0 & 1 & -1 & 3 \\
\end{array}
\right]$

c.
$\left[
\begin{array}{cccc|c}
1 & 4 & 0 & 0 & -2 \\
0 & 0 & 1 & 7  & 6\\
0 & 0 & 0 & 0  & 0 \\
\end{array}
\right]$




## Elementary row operations are reversible 
In each case below, an elementary row operation turns the matrix $A$ into the  matrix $B$. For each of them,

* Describe the row operation that turns $A$ into $B$, and 
* Describe the row operation that turns $B$ into $A$. 

Give your answers in the form: "scale $R_2$ by 3" or "swap $R_1$  and $R_4$" or "replace $R_3$ with $R_3 + \frac{1}{5} R_1$."

a.
$$A=\left[
\begin{array}{cccc}
 1 & 1 & 1 & 3 \\
 1 & -2 & 2 & 1 \\
 2 & 8 & 2 & -4 \\
 3 & 1 & 6 & -1 \\
\end{array}
\right]\longrightarrow 
B=\left[
\begin{array}{cccc}
 1 & 1 & 1 & 3 \\
 1 & -2 & 2 & 1 \\
 2 & 8 & 2 & -4 \\
 0 & 7 & 0 & -4 \\
\end{array}
\right]$$

b.
$$A=\left[
\begin{array}{cccc}
 1 & 1 & 1 & 3 \\
 1 & -2 & 2 & 1 \\
 2 & 8 & 2 & -4 \\
 3 & 1 & 6 & -1 \\
\end{array}
\right]\longrightarrow 
B=\left[
\begin{array}{cccc}
1 & -2 & 2 & 1 \\
 1 & 1 & 1 & 3 \\
 2 & 8 & 2 & -4 \\
 3 & 1 & 6 & -1 \\
\end{array}
\right]$$

c.
$$A=\left[
\begin{array}{cccc}
 1 & 1 & 1 & 3 \\
 1 & -2 & 2 & 1 \\
 2 & 8 & 2 & -4 \\
 3 & 1 & 6 & -1 \\
\end{array}
\right]\longrightarrow 
B=\left[
\begin{array}{cccc}
 1 & 1 & 1 & 3 \\
 1 & -2 & 2 & 1 \\
 1 & 4 & 1 & -2 \\
 3 & 1 & 6 & -1 \\
\end{array}
\right]$$

## Designer Parabolas

In each part below, set up and solve a linear system of equations to find **all** possible parabolas of the form
$$
f(x) = a + b x + c x^2
$$ 
that satisfy the given conditions. For full credit, please solve these by hand, doing all row reductions that bring the system of equations to Reduced Row Echelon Form. On future assignments, you can solve problems like this using either RStudio or WolframAlpha. You are welcome (and, in fact, encouraged) to check your answers using software. 

a. $f(x)$ passes through the three points: $(1,3), (3,11),(2,4)$.

b. $f(x)$ passes through the three points: $(1,3), (3,11),(3,10)$.

c. $f(x)$ passes through the *two* points: $(1,3)$ and $(3,11)$.




## Traffic Flow

Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction.  The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area.  Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). 


<center>

![](images/ps1-traffic.png){width=50%}

</center>


a. Create a system of linear equations to find the possible flow values for the inner streets $x_1, x_2, x_3, x_4$.

b. Using RStudio, enter the augmented matrix of this system, and solve it using the rref command. Type out the general solution to this system of equations.  

c. Your  answer to part b should be an infinite solution set. Give two distinct solutions that are realistic in terms of  **traffic flow**.

d. Is it possible to close down the street labeled by $x_2$ for road construction? That is, is it possible to have $x_2 = 0$ and to meet the other conditions?






<!--chapter:end:PS1A-problem-set-1A.Rmd-->



# Problem Set 2

* Due: Tuesday February 2 by 10:00am CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all row reductions.**
* In problems where you use RStudio for row reduction and are not asked to turn in an R markdown file, you can write something like this:
<center>
![](images/used-R.jpg){width=50%}
</center>

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo = FALSE,
               out.width = "75%", 
               fig.align = "center")
```

```{r, echo=FALSE}
require(pracma)
```


* You can download the [Rmd source file for this problem set](https://github.com/Tom-Halverson/math236_s21/blob/main/PS2-problem-set-2.Rmd).

The Problem Set covers sections 1.3, 1.4, 1.5, and 1.7.


## Parametric Vector Form


Here is the augmented matrix for a system of linear equations $\mathsf{A} \mathsf{x} = \mathsf{b}$, and its RREF.
Give the general solution to this  system in  **parametric vector form**. 

$$
\left[
\begin{array}{ccccc|c}
 1 & 1 & -1 & -1 & 2 & 1 \\
 1 & 0 & -2 & 1 & 1 & 3 \\
 -2 & 1 & 5 & 1 & -6 & 2 \\
 -3 & 0 & 6 & 2 & -8 & 1 \\
 0 & 1 & 1 & 2 & -3 & 6 \\
 1 & 0 & -2 & -1 & 3 & -1 \\
\end{array}
\right] \longrightarrow \left[
\begin{array}{ccccc|c}
 1 & 0 & -2 & 0 & 2 & 1 \\
 0 & 1 & 1 & 0 & -1 & 2 \\
 0 & 0 & 0 & 1 & -1 & 2 \\
 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 & 0 \\
\end{array}
\right]
$$


## RREF for a linear system

Here is the reduced row echelon form of a matrix $\mathsf{A}$ (you are not given the matrix $\mathsf{A}$).
$$
\mathsf{A} \longrightarrow \left[
\begin{array}{cccc}
 1 & -2 & 0 & 4 \\
 0 & 0 & 1 & -5 \\
 0 & 0 & 0 & 0 \\
\end{array}
\right]
$$

  a. Give the parametric equations of the general solution to the homogenous equation $\mathsf{A} \mathsf{x} = {\bf 0}$.
  
  b. Describe the geometric form of your answer to part (a). For example, you answer should be something like: "it is a plane in $\mathbb{R}^3$" or "it is a line in $\mathbb{R}^7$" or "it is a point in $\mathbb{R}^4$."
  
  c.  Suppose that we also know that $\mathsf{A}\begin{bmatrix} 4 \\ 1 \\ -3 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 22 \\ -13 \\ 7 \\ \end{bmatrix}$. Then give the general solution to 
$\mathsf{A} \mathsf{x}= \begin{bmatrix} 22 \\ -13\\ 7 \\ \end{bmatrix}$ in parametric form.






## RREF for a set of vectors

Suppose that we have five vectors 
$\mathsf{v}_1, \mathsf{v}_2,\mathsf{v}_3,\mathsf{v}_4,\mathsf{v}_5$ in $\mathbb{R}^4$ and that the matrix
$$
A = \left[
\begin{array}{ccc}
\mid & \mid  & \mid & \mid & \mid \\
\mathsf{v}_1 & \mathsf{v}_2 & \mathsf{v}_3 &\mathsf{v}_4 &\mathsf{v}_5  \\
\mid & \mid  & \mid & \mid & \mid 
\end{array}
\right]
$$
has reduced row echelon form
$$
\begin{bmatrix}
1 & 0 & -3 & 0 & 2  \\
0 & 1 & 4 & 0 &  1  \\
0 & 0 & 0 & 1 & 1  \\
0 & 0 & 0 & 0 & 0 
\end{bmatrix}.
$$


  a.  Do the vectors $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4, \mathsf{v}_5$ span $\mathbb{R}^4$? Justify your answer.

  b. Is the vector $\mathsf{v}_3$  in $\mathrm{span}(\mathsf{v}_1,\mathsf{v}_2)$? Justify your answer.

  c. Pick any $\mathsf{b}$ in $\mathrm{span}(\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4, \mathsf{v}_5)$. Is there always a unique way to write $\mathsf{b}$ as a linear combination of $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4, \mathsf{v}_5$? Justify your answer.





## Removing free variable columns from a matrix

Consider the matrix

$$
A =\left[
\begin{array}{cccccc}
 6 & 5 & -3 & 4 & 2 & -9 \\
 -7 & -6 & 4 & -5 & -7 & 16 \\
 -4 & -3 & -1 & 0 & -8 & 9 \\
 8 & 7 & -5 & 6 & 1 & -12 
\end{array}
\right].
$$

  a. Use RStudio to show that the columns  of $\mathsf{A}$ span $\mathbb{R}^4$. You don't need to turn in an R file here, just report the reduced row echelon form that you get.
  b. Write down the new matrix $\mathsf{A}'$ gotten by removing the free variable columns from $\mathsf{A}$. 
  c. Without using additional calculations on RStudio, explain why the new system $\mathsf{A}' \mathsf{x} = \mathsf{b}$ is **consistent**  and has a **unique solution** for every choice of $\mathsf{b} \in  \mathbb{R}^4$.



## A square matrix 

Suppose that $A$ is a $5\times 5$ matrix and $\mathsf{b}$ is a vector in $\mathbb{R}^5$ with the property that 
$A\mathsf{x}=\mathsf{b}$ has a unique solution. Explain why the columns of $A$ must span $\mathbb{R}^5$.  Use the reduced row echelon form of $A$ in your explanation.

## Combining solutions to $A \mathsf{x} = \mathsf{b}$

Suppose that $\mathsf{x}_1$ and $\mathsf{x}_2$ are solutions to $\mathsf{A} \mathsf{x} = \mathsf{b}$ (where $\mathsf{b} \not= \mathsf{0}$). 

a. Decide if any of the following are also solutions to $\mathsf{A} \mathsf{x} = \mathsf{b}$.
    i. $\mathsf{x}_1+ \mathsf{x}_2$
    ii. $\mathsf{x}_1 - \mathsf{x}_2$
    iii. $\frac{1}{2} ( \mathsf{x}_1 + \mathsf{x}_2)$
    iv. $\frac{5}{2} \mathsf{x}_1 - \frac{3}{2} \mathsf{x}_2$.

b. Under what conditions on $c$ and $d$ is $\mathsf{x} = c \mathsf{x}_1 + d \mathsf{x}_2$ a solution to $\mathsf{A} \mathsf{x} = \mathsf{b}$? Justify your answer.
c. Let $\mathsf{u}$ be the vector that *points to* $1/3$ of the way from the tip of $\mathsf{v}$ to the tip of $\mathsf{w}$ as depicted below. 
    i. Write $\mathsf{u}$ as a linear combination of $\mathsf{v}$ and $\mathsf{w}$ (hint: think about $\mathsf{w} - \mathsf{v}$)
    ii. If $\mathsf{v}$ and $\mathsf{w}$ are solutions to $A x = \mathsf{b}$ then show that $\mathsf{u}$ is also a solution to $A \mathsf{x} = \mathsf{b}$.

<center>
![](images/ps2-three-vec.png){width=30%}
</center>
<!--
\begin{tikzpicture}

\draw[-latex, very thick,green] (0,0) -- (2,8/3);
\draw[-latex, very thick,blue] (0,0) -- (1,3);
\draw[-latex, very thick,blue] (0,0) -- (4,2);
\draw[-latex, very thick,red] (1,3) -- (4,2);

\node at (1.55,1.75) {$\mathsf{u}$};
\node at (.3,1.75) {$\mathsf{v}$};
\node at (2.75,1) {$\mathsf{w}$};

\end{tikzpicture}
-->

## A Balanced Diet
  
<!-- https://lvs.com.au/wp-content/uploads/2015/10/protein-fat-carb-counter.pdf -->

An athlete wants to consume  a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of  proteins. Here are some of their favorite foods.


```{r}
food.data <- data.frame(
   food = c ("almonds", "avocado",  "beans", "bread", "cheese", "chicken", "egg", "milk", "zucchini"),
   carbs = c(3,15,20,12,1,0,1,12,6),
   fats = c(8,31,1,1,5,13,5,8,0), 
   proteins = c(5,4,8,2,3,50,6,8,2)
)

knitr::kable(
  food.data, booktabs = TRUE,
  caption = 'Food Carb/Fat/Protein (grams)'
)
```

Answer the following  questions, using RStudio for your calculations. Each response must use **two or more** of the following terms:  linear combination, span, linearly dependent, linearly independent. 


1. Explain why they **cannot** achieve their daily goal by eating only almonds, milk and zucchini.

2. Explain why they **cannot** achieve their daily goal by eating only almonds, beans and cheese.

3. Find a valid one-day diet consisting of almonds, chicken, and zucchini. 

## Missing Column

The matrices below are supposed to be $3 \times 3$ but in each case the third column was accdentally deleted. In each case, add a third column, that has no 0s in it and is different from either the first or second column, so that the columns of  $A$ are linearly dependent and so that the columns of $B$ are linearly independent. Briefly describe your strategy.
$$
A=\left[
\begin{matrix}
1& 0 & \quad \\
0& 1& \quad \\
2& 2& \quad \\
\end{matrix}\right]  \qquad\qquad  
B=\left[
\begin{matrix}
1& 0 & \quad \\
0& 1& \quad \\
2& 2& \quad \\
\end{matrix}\right] 
$$

## Linear System

Use R to solve this problem. Do your computations in an R markdown file.  Knit the file to HTML and include it with your homework. Here you [can download a template for doing this problem](https://github.com/Tom-Halverson/math236_s21/blob/main/PS-2.9-template.Rmd) (including the matrix typed out for you!).
$$
A =\left[
\begin{array}{cccccc}
 12 & 10 & -6 & 8 & 4 & -18 \\
 -7 & -6 & 4 & -5 & -7 & 16 \\
 9 & 9 & -9 & 9 & 9 & -27 \\
 -4 & -3 & -1 & 0 & -8 & 9 \\
 8 & 7 & -5 & 6 & 1 & -12 \\
\end{array}
\right] \quad
b = \begin{bmatrix} 14 \\ -12 \\ 9\\ -15 \\6 \end{bmatrix}
$$

a. Show that the columns of $A$ are linearly dependent by finding two different dependency relations among them. You can write your answer in a form  like $5 a1+ 4 a2 + 3 a3 + 2 a4 + a5 = 0$, where $a1, a2,$ etc are the columns of $A$. 

b. Augment $A$ with $b$ and show that $A x = b$ is consistent and has infinitely many solutions.

c. Remove the free-variable columns from $A$ to get a new, smaller matrix $A'$. Show that $A' x = b$ has a unique solution and say what that solution is.









<!--chapter:end:PS2-problem-set-2.Rmd-->


# Problem Set 3

* Due: Tuesday February 9 by noon CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all row reductions.**
* You can download the [Rmd source file  for this problem set](https://github.com/Tom-Halverson/math236_f20/blob/main/PS3-problem-set-3.Rmd).

The Problem Set covers sections 1.8, 1.9, 2.1, 2.2.


## Properties of Linear Transformations
Here are the row reductions pf 4  matrices into reduced row echelon form.
$$
\begin{array}{ll}
A \longrightarrow \begin{bmatrix} 1 & 0 & 5 & -3 & 0\\ 0 & 1 & -2 & 8  & 0 \\ 0 & 0 & 0 & 0 & 1  \\ 0 & 0 & 0 & 0 &  0 \\ 0 & 0 & 0  & 0 &  0 \end{bmatrix} \qquad
& 
B \longrightarrow \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} 
\\
\\
C \longrightarrow \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0  & 0 \\ 0 & 0 & 1 & 0  \\ 0 & 0 & 0 &  1 \end{bmatrix}
&
D \longrightarrow \begin{bmatrix} 1 & 0 & 0 & 0  \\ 0 & 1 & 0  & 0  \\ 0 & 0 & 1 & 1   \end{bmatrix} 
\end{array}
$$

In each case, if $T_M$ is the linear transformation given by the matrix product $T_M(x) = M x$, where $M$ is the given matrix, then $T_M: \mathbb{R}^n \to \mathbb{R}^m$ is a transformation from domain $\mathbb{R}^n$ to codomain (aka target) $\mathbb{R}^m$. 

Determine the appropriate values for $n$ and $m$, and decide whether $T_M$ is one-to-one and/or onto. Submit your answers in table form, as shown below.
$$
\begin{array} {|c|c|c|c|c|} 
\hline
\text{transformation}  &  n  &  m & \text{one-to-one?} & \text{onto?} \\ \hline
T_A &\phantom{\Big\vert XX}&\phantom{\Big\vert XX}&& \\ \hline
T_B &\phantom{\Big\vert XX}&&& \\ \hline
T_C &\phantom{\Big\vert XX}&&& \\ \hline
T_D &\phantom{\Big\vert XX}&&& \\ \hline
\end{array} \hskip5in
$$


## Partial Information about a Linear Transformation

We are given that $T: \mathbb{R}^4 \rightarrow \mathbb{R}^3$ is a linear transformation such that: 
$$
T\left(\begin{bmatrix} 3 \\ ~2~ \\ 1 \\ 2 \end{bmatrix} \right)=\begin{bmatrix}  ~2~ \\ 3 \\ 6 \end{bmatrix}
\qquad\hbox{and}\qquad
T\left(\begin{bmatrix}~~2 \\ -1 \\ 0 \\ -1 \end{bmatrix} \right)=\begin{bmatrix} 2 \\ ~0~  \\ 1 \end{bmatrix}.
$$
If that is all we know about $T$, then do we have enough information to compute the value of $T$ below? 
$$T\left(\begin{bmatrix} 5 \\ 8 \\ ~3~ \\ 8 \end{bmatrix} \right) = \hskip5in$$
If yes, then compute it (showing how you do so). If no, then explain why not. Hint: try to write the third input vector as a linear combination of the first two.







## House Renovations

Find the matrix of a linear transformation $T: \mathbb{R}^2 \to \mathbb{R}^2$ that performs the given transformation of my house. (Hint: use the base, the doorway and the peak of the roof as a guide.)

1. Transformation \#1

![](images/ps3-house1.png){width=40%} $\qquad \qquad$ ![](images/ps3-house2.png){width=40%}


2. Transformation \#2

![](images/ps3-house1.png){width=40%} $\qquad \qquad$ ![](images/ps3-house3.png){width=40%}



## Matrix of a Nonlinear Transformation?

This problem illustrates what happens if you try to make the matrix of a transformation that is not linear. Consider the transformation $T$ defined by
$$
T \left( \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} \right)
=
\begin{bmatrix} x_1 + x_2^2 + x_3 \\ 2 x_2 + x_1 x_3 + 1 \\ 2 x_1 + 3 x_2 + x_3 \end{bmatrix}
$$

This is not a linear transformation. Let's see what happens if we compute its matrix anyway. Compute $T(\mathbf{e}_1)$, $T(\mathbf{e}_2)$, and $T(\mathbf{e}_3)$, and put the vectors you get in the columns of a matrix $A$. Then compute the product below:
$$
\underbrace{\begin{bmatrix} 
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot \\
\end{bmatrix}}_{A}
\begin{bmatrix} x_1 \\ x_ 2 \\ x_3 \end{bmatrix}
= 
$$
Explain how the result of this computation demonstrates that $T$ is not linear.

## A Proof 


Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Suppose that $\{v_1, v_2, v_3, v_4\}$ is a linearly independent set of vectors in $\mathbb{R}^n$ but the set of images $\{T(v_1), T(v_2), T(v_3), T(v_4)\}$ is a linearly dependent set in $\mathbb{R}^m$. In the following steps, you will prove that $T$ is not one-to-one.


a. Write out clearly, using the definition, what it means for $\{v_1, v_2, v_3, v_4\}$ to be linearly independent. 

b. Write out clearly, using the definition, what it means for $\{T(v_1), T(v_2), T(v_3), T(v_4)\}$ to be linearly dependent.

c. Use the definition of linear transformation and parts (a) and (b) above to argue that $T(x) = \vec{0}$ for some nonzero vector $x \in \mathbb{R}^n$.

d. Explain why this tells us that $T$ is not one-to-one.

## Inner and Outer Products

We can also think of vectors as matrix. A column vector is an $n \times 1$ matrix and a row vector is a $1 \times n$ matrix. 

a. Compute the following products. These matrix products are called *inner products* (or dot products) of the vectors.


$$
\begin{bmatrix} 4 & -1 & 2 & 3\end{bmatrix}
\begin{bmatrix} 1 \\ 2 \\1  \\3  \\\end{bmatrix} = \hskip3in
$$
$$
\begin{bmatrix} 4 & -1 & 2 & 3\end{bmatrix}
\begin{bmatrix} 1 \\ 1 \\1  \\1  \\\end{bmatrix} = \hskip3in
$$
$$
\begin{bmatrix} 4 & -1 & 2 & 3\end{bmatrix}
\begin{bmatrix} 2 \\ 5 \\ 0  \\ -1  \\\end{bmatrix} = \hskip3in
$$
b. Now compute the following products. These are called *outer products*.  

$$
\begin{bmatrix} 1 \\ 2 \\1  \\3  \\\end{bmatrix}
\begin{bmatrix} 1 & -5 & 2 & 3\end{bmatrix} = \hskip3in
$$

$$
\begin{bmatrix} 1 \\ 2 \\1  \\3  \\\end{bmatrix}
\begin{bmatrix} 1 & 1 & 1 & 1\end{bmatrix} =\hskip3in
$$

c. Row reduce both of the matrices that you get in part b (this should be easy to do by hand,but you can use R if you want to). How many pivots do you get? Explain why you always get this number of pivots when you row reduce an outer product.


## Archaeological Seriation

The matrix $A$ below is used in  archaeological dating. Its rows correspond to four different grave sites $G_1, G_2, G_3, G_4$ and its columns correspond to five types of pottery$P_1, P_2, P_3, P_4, P_5$. There is a 1 in position $i$-$j$ if pottery type $P_j$ is found in grave $G_i$ (and a 0 otherwise).
$$
A=\begin{array}{c|ccccc}
& P_1 & P_2 & P_3 & P_4 & P_5 \\
\hline
 G_1 & 1 & 1 & 0 & 1 & 1 \\
G_2 &  0 & 0 & 1 & 0 & 1 \\
G_3 &  0 & 1 & 1 & 0 & 1 \\
G_4  & 1 & 1 & 1 & 1 & 1 \\
\end{array}
$$

1. Compute the matrix $\mathbf{G} = A A^T$, where $A^T$ is the *transpose* of $A$, meaning that the rows and columns have been interchanged.

2. Give the meaning of the $i$-$j$ entry of $\mathbf{G}$ (the entry in row $i$ and column $j$). State clearly the meaning of this entry using complete sentences (or sentence) and explain why it has this meaning.

## Rental Car

Solve this problem using R and turn in a markdown file knitted to .html. 

A group of Macalester alumni open a rental car company specializing in  renting electric cars. As a start, they have opened offices in St. Paul, Rochester, and Duluth. Through market research they find that of the cars rented in St. Paul, 85% will get returned in St. Paul, 9% will get returned in Rochester, and 6% will get returned in Duluth. Of the cars rented in Rochester, 30% will get returned in St. Paul, 60% will get returned in Rochester, and 10% in Duluth. Of the cars rented in Duluth, 35% will get returned in St. Paul, 5% in Rochester, and 60% in Duluth. This information is represented in the matrix below.

```{r,echo=TRUE}
StP = c(.85,.09,.06)
Roch = c(.30,.60,.10)
Dul = c(.35,.05,.60) 
M = cbind(StP,Roch,Dul)
M
```
Such a matrix is called a *probability* matrix or a *stochastic* matrix because it  contains numbers between 0 and 1 and each of its columns sum to 1.

a. The owners are trying to use this data to estimate how much of their fleet will be at each location on average in the long run. Assume that initially they locate 20 cars in each city. This can be recorded by the vector `v0 = c(20,20,20)`. Apply, M to v0, call this vector v1, and explain, using how the matrix-vector product works, why v1 represents the number of cars at each location one day later (for simplicity, we assume that each rental is for 1 day).

b. Now apply M to v1 and call it v2. This should represent the number of cars at each location 2 days later. Also compute the square of the matrix M and call it M2. Confirm that M2 times v0 is the same as M times v1. 

c. Write a for loop that applies M over and over again to see what happens to the distribution of cars in the long-run (we will learn how to do this in class but you can also probably just google it). Does this sequence stabilize or does it keep changing after each application? If it does stabilize, how long does it take to stabilize (to within 0.1 cars at each location).

d. Does the starting distribution matter? Try 4 different starting distributions (with a total of 60 cars) and see what the final distribution looks like in each case. For one of your 4 starting distributions, try all 60 cars at one of the locations.


## Adjacency Matrix

You can do this problem in R or by hand. Consider the matrix $A$ defined here
```{r,echo=TRUE}
A = rbind(c( 0 , 1 , 0 , 1 , 1 ,0), c(1 , 0 , 1 , 1 , 0, 0 ),c( 0 , 1 , 0 , 1 , 1, 0 ),
  c( 1 , 1 , 1 , 0 , 1, 0 ),c( 1 , 0 , 1 , 1 , 0, 1 ), c(0, 0, 0, 0, 1, 0))
A
```

This matrix represents the connections in the network diagram below. There is a 1 in position $(i,j)$ of the matrix if there is a connection (an edge) between vertex $i$ and vertex $j$ and there is a 0 if there is not.

```{r,echo=FALSE}
suppressPackageStartupMessages(library(igraph))
g=graph_from_adjacency_matrix(A,mode='undirected')
V(g)$label = c(1,2,3,4,5,6)
plot(g)
```

a. Compute $A v$ where $v$ is the vector of all 1's. Explain what this new vector tells us about the graph.

b. Compute $A^2 = A A$, the square of the matrix $A$.

c. Look at the $(2,5)$ entry of $A^2$. Explain what this entry says about connections in the network. Do the same for the $(2,3)$ and the $(2,6)$ entry of $A^2$.





<!--chapter:end:PS3-problem-set-3.Rmd-->


# Problem Set 4

* Due: Wednesday February 17 by noon CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all row reductions.**
* You can download the [Rmd source file  for this problem set](https://github.com/Tom-Halverson/math236_s21/blob/main/PS4-problem-set-4.Rmd).



The Problem Set covers sections 2.2-2.3 on Matrix Inverses and 4.1-4.3 on Subspaces and Bases.

```{r}
require('pracma')
```


## Rainy Day in LA

In Los Angeles if it rains today, there is a 50% chance it will rain tomorrow, but it if is sunny today, there is a 90% chance it will be sunny tomorrow. This is modeled in the rain-sunshine probability matrix P. 
$$
P = 
\begin{array}{c|cc|}
&\text{rain}&\text{sun}\\
\hline
\text{rain}&1/2&1/10\\
\text{sun}&1/2&9/10\\
\hline
\end{array}
$$
This matrix works as follows: if the rain-sunshine probability today is (40, 60) (that is, 40% chance rain and 60% chance sunshine), then the rain-sunshine probability tomorrow is (26, 74) as seen by the calculation below.
$$
\begin{bmatrix}
1/2 & 1/10 \\
1/2 & 9/10 \\
\end{bmatrix} \begin{bmatrix} 40 \\ 60 \end{bmatrix} = \begin{bmatrix} 26 \\ 74 \end{bmatrix}
$$

a. Find the rain-sunshine probability the day after tomorrow.

b. Compute $P^2$ and explain the meaning of each of the four entries in the matrix.

c. Find $P^{-1}$ and   and use it find the rain-sunshine probability *yesterday* if the rain-sunshine probability today is (40, 60).

## Fibonacci Vectors

The Fibonacci vectors $F$ in $\mathbb{R}^5$ are defined below:
$$
F  = \left\{ \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} ~\Bigg\vert~ \ x_3 = x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \right\} \subseteq \mathbb{R}^5.   \hskip5in
$$

Find a basis for $F$. Be sure to show that your vectors span $F$ and are linearly independent.


## Vectors Rescaled

If the function $T: \mathbb{R}^n \to \mathbb{R}^n$ is a linear transformation, then show that the set below is a subspace of $\mathbb{R}^n$
$$
E_2 = \{\ \vec{x} \in \mathbb{R}^n \mid T(\vec{x}) = 2 \vec{x} \}.
$$
Is there anything special about 2 in the definition? If it were replaced by another scalar, would it still be a subspace?

## Polynomial Vector Spaces

This problem refers to the information about the vector space of polynomials $\mathcal{P}_n$ found in the Day 13 class notes. 

Here are three subsets of $\mathcal{P}_4$. Decide if they are subspaces. In each case, if it is not a subspace, give examples using specific polynomials to show that one of the rules is broken, and if it is a subspace, show that the subspace rule holds for any two polynomials $p(x), q(x)$ and any constant $c \in \mathbb{R}$.

a. $U = \{ p(x) \in \mathcal{P}_4 \mid p(1) = 0 \}$

b. $V = \{ p(x) \in \mathcal{P}_4 \mid p(0) = 1 \}$

c. $W= \{ p(x) \in \mathcal{P}_4 \mid p'(1) = 0 \}$

## Column and Null Space

Find a basis for the column space $Col(A)$ and the null space $Nul(A)$ of the following matrix $A$ below
```{r,echo=TRUE}
(A = rbind(c(1, 2, 0, 2, 0, -1),c(1, 2, 1, 1, 0, -2),
           c(2, 4, -2, 6, 1, 2),c(1, 2,  0, 2, -1, -3 )))
```

## Extend to a basis

I am  interested in the vectors below. I know that they do not span $\mathbb{R}^5$, because there are not enough of them, but I want to extend this set to a basis of $\mathbb{R}^5$ by adding some vectors to the set. 
$$
\begin{bmatrix} 5\\ 4\\ 3\\ 1\\ 2 \end{bmatrix},
\begin{bmatrix} 4\\ 4\\ 3\\ 1\\ 2 \end{bmatrix}, 
\begin{bmatrix} 1\\ 1\\ 1\\ 1\\ 1\end{bmatrix}. 
$$
I searched online for ideas and one suggested that I make the matrix below and row reduce it.
```{r,echo=TRUE}
(A = cbind(c(5,4,3,1,2),c(4,4,3,1,2),c(1,1,1,1,1),diag(5)))
```

a. Row reduce this matrix.

b. Use the result to come up with a basis for $\mathbb{R}^5$ that includes my original 3 vectors

c. Explain why this method works.

## Getting Into a Subspace

Let $S \subset \mathbb{R}^n$ be a subspace and let $\mathsf{v}, \mathsf{w} \in \mathbb{R}^n$. For  each of the following statements, either give  a specific example  or explain why it cannot happen.


a. If $\mathsf{v}$ is in $S$ but  $\mathsf{w}$ is **not** in $S$, can  $\mathsf{v} + \mathsf{w}$ be in $S$?  



b. If $\mathsf{v}$ is **not** in $S$ and  $\mathsf{w}$ is **not** in $S$, can  $\mathsf{v} + \mathsf{w}$ be in $S$? 



c. If $\mathsf{v}$ is **not** in  $S$ and $c$ is a  nonzero constant, can $c\mathsf{v}$ be  in $S$?




## A Vector in Both Col(A) and Nul(A)

Give a $3 \times 3$ matrix $A$ for which the vector $\mathsf{v} = \begin{bmatrix}3 \\ -2 \\ 5   \end{bmatrix}$ is in both $\mathrm{Col}(A)$ and $\mathrm{Nul}(A)$. Be sure to demonstrate that $\mathsf{v} \in \mathrm{Col}(A)$ and $\mathsf{v} \in \mathrm{Nul}(A)$.



<!--chapter:end:PS4-problem-set-4.Rmd-->

---
title: "Problem Set 4"
author: "Your Name Here"
date: "Due 11/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(pracma)
```



# 1. An Invertible Product of Rectangular Matrices

<!------------------------------------------------------>
a. Your answer 

<!------------------------------------------------------>
b. Your answer





# 2. Guessing the Inverse Matrix from a Pattern

<!------------------------------------------------------>
a. Your answer

<!------------------------------------------------------>
b. Your answer

<!------------------------------------------------------>
c. Your answer


# 3. LU Decomposition of an Invertible Matrix


<!------------------------------------------------------>
a. Your answer

<!------------------------------------------------------>
b. Your answer

<!------------------------------------------------------>
c. Your answer


<!------------------------------------------------------>
d. Your answer


```
B=cbind(c(1,2,3,4,5,6,7,8),
        c(0,2,3,4,5,6,7,8),
        c(0,0,3,4,5,6,7,8),
        c(0,0,0,4,5,6,7,8),
        c(0,0,0,0,5,6,7,8),
        c(0,0,0,0,0,6,7,8),
        c(0,0,0,0,0,0,7,8),
        c(0,0,0,0,0,0,0,8)
        )
```


# 4. Homogeneous Coordinates

Original picture

```{r,bird1,fig.height=5,fig.width=5, echo=TRUE}

bird = rbind(c(20, 21.3, 23.3, 21.6, 20.7, 21.7, 20.9, 20.3, 18.9, 18.1, 18.3, 19.6, 19.2, 19.1, 19.2, 19.4, 20), 
             c(20, 21.7, 20.9, 21.1, 19.3, 18.7, 18, 18.9, 18, 15.4, 18.6, 19.6, 20, 20.2, 20.3, 20.3, 20), 
             c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1))

fronds = rbind(c(10, 8, 6, 4, 2, 2.5, 4, 6, 9, 10, 10, 9, 7, 5, 2.5, 4, 6, 8, 9, 10, 10, 11, 11, 10.5, 12, 12.5, 12.5, 10, 10, 13, 15, 15.5, 15.5, 14, 12, 10, 10, 12, 14, 14, 13.5, 13, 12, 10),
               c(16, 16, 16, 15, 11, 15, 17, 17.5, 17, 16, 16, 17.5, 19, 19.5, 18.5, 20, 20.5, 20, 19, 16, 16, 18, 20, 20.5, 20, 19, 18, 16, 16, 17, 16, 15, 13.5, 15, 16, 16, 16, 15, 13, 11, 9, 11, 13, 16),
               c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1))

trunk = rbind(c(10, 9, 8, 7.5, 7.5, 8, 10, 8, 6.5, 6, 5, 5, 6, 7, 10),
              c(16, 14, 12, 10, 8, 5, 2.5, 2, 2, 3, 6, 9, 12, 14, 16),
              c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1))

# intialize the plot
plot(bird[1,],bird[2,],type="n",xlim=c(0,25),ylim=c(0,25),xlab="x",ylab="y")
abline(h=0:25, v=0:25, col="gray", lty="dotted")

# plot the bird, trunk and fronds
polygon(bird[1,], bird[2,], col = "gray", border = "blue")
polygon(fronds[1,], fronds[2,], col = "orange", border = "black")
polygon(trunk[1,], trunk[2,], col = "brown", border = "black")

```


Template for defining the linear transformations and creating an updated picture


```{r,bird2,fig.height=5,fig.width=5, echo=TRUE}

#####################
######## update this code with the appropriate linear transformations

birdmap = cbind(c(1,0,0),c(0,1,0),c(0,0,0))
leafmap = cbind(c(1,0,0),c(0,1,0),c(0,0,0))


#####################
######## you do not need to change this the code below this line

newbird = birdmap %*% bird
newfronds = leafmap %*% fronds


# initialize the plot
plot(newbird[1,],newbird[2,],type="n",xlim=c(0,25),ylim=c(0,25),xlab="x",ylab="y")
abline(h=0:25, v=0:25, col="gray", lty="dotted")

polygon(newbird[1,], newbird[2,], col = "cyan", border = "blue")
polygon(newfronds[1,], newfronds[2,], col = "green", border = "black")
polygon(trunk[1,], trunk[2,], col = "brown", border = "black")
```



Plots both pictures for comparison


```{r,bird3,fig.height=5,fig.width=5, echo=TRUE}
# initialize the plot
plot(newbird[1,],newbird[2,],type="n",xlim=c(0,25),ylim=c(0,25),xlab="x",ylab="y")
abline(h=0:25, v=0:25, col="gray", lty="dotted")

# plot the new bird and new fronds
polygon(newbird[1,], bird[2,], col = "gray", border = "blue")
polygon(newfronds[1,], fronds[2,], col = "orange", border = "black")

# plot the original bird, fronds  and trunk
polygon(newbird[1,], newbird[2,], col = "cyan", border = "blue")
polygon(newfronds[1,], newfronds[2,], col = "green", border = "black")
polygon(trunk[1,], trunk[2,], col = "brown", border = "black")

```

<!--chapter:end:PS4-YourNameHere.Rmd-->


# Problem Set 5

* Due: Tuesday February 23 by 11:59am CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all row reductions.**
* You can download the [Rmd source file  for this problem set](https://github.com/Tom-Halverson/math236_s21/blob/main/PS5-problem-set-5.Rmd).


The Problem Set covers sections 4.4, 4.5, 3.1, 5.1 


## A Tale of Two Bases


I recommend using R on this problem. Consider the subspace $S$ of $\mathbb{R}^5$ below.
$$
S = \textsf{span}\left(
\begin{bmatrix}
 1  \\
 1  \\
 1  \\
 1  \\
 2  \\
\end{bmatrix},
\begin{bmatrix}
1  \\
2  \\
3  \\
0  \\
-1 \\
\end{bmatrix},
\begin{bmatrix}
0  \\
0  \\
0 \\
1  \\
 2  \\
\end{bmatrix},
\begin{bmatrix}
 -1  \\
 1  \\
 3  \\
 0  \\
 -2  \\
\end{bmatrix},
\begin{bmatrix}
2 \\
1 \\
 0 \\
  1 \\
 3 \\
\end{bmatrix}
\right)
$$
```{r,echo=TRUE}
A = cbind(c(1,1,1,1,2),c(1, 2, 3, 0, -1),c(0, 0, 0, 1, 2), c(-1, 1, 3, 0, -2),c(2, 1, 0, 1, 3))
```

a. Give a basis of $S$ consisting of some or all of the vectors used to define $S$ above. 

b. Give a basis of $S$ that has the nice standard basis property (i.e., the 0s and 1s property).

c. For the two vectors below, decide if they are in $S$. If the vector is in $S$ then give its coordinates in each of your bases from parts (a) and (b). If you can do one of these "by hand" then explain how.
$$
\mathbf{w} = \begin{bmatrix} 8 \\ 11 \\ 14 \\ 7 \\ 11 \end{bmatrix}, \qquad 
\mathbf{v} = \begin{bmatrix} 3 \\ 3 \\ 3 \\ 1 \\ 1 \end{bmatrix}.
$$

## Dimension

Find the dimension of the subspace $Z$ of $\mathbb{R}^5$ of *zero-sum* vectors below
$$
Z = \left\{
\begin{bmatrix}x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix}
\ \Bigg\vert\ \  x_1 + x_2 + x_3 + x_4 + x_5 = 0\ 
\right\}.
$$


## Determinant Properties

Turn in an R-markdown file with your solution to this problem. You can compute determinants in R using the `det` command. Here you will explore some properties of determinants.
```{r,echo=TRUE}
(A = rbind(c(3, 0, -1, 1, 2), c(1, 1, -1, 1, 1), c(-2, -3, -2, 3, 1), c(1, 3, 1, 3, 0),
c(1, 3, 0, -2, 0)))
```

a. Compute the determinant of $A$.
b. Compute the determinant of $A^2$. How does it compare to $det(A)$?
c. Compute the determinant of $A^{-1}$. How does it compare to $det(A)$?
d. Swap two rows of $A$ and then compute the determinant of the matrix that you get. 
e. Multiply the 4th row of the original matrix $A$ by 7 and then compute the determinant of the matrix you get. How does it compare to $det(A)$?
f. Compute the determinant of $7A$. How does it compare to $det(A)$?
g. Let $B$ be the matrix below, and compute $det(B)$, $det(A B)$, and $det(A) det(B)$.

```{r,echo=TRUE}
(B = rbind(c(1, 2,1, 1, 1), c(1, 2, 0, 1, -1), c(-2, -1, -2,0, 1), c(1, 0, 1, 3, 0),
c(1, 0, 0, 1,1)))
```

## Matrix Rank

Fill in the entries of the table  with **T** = true or **F** = false or **I** = not enough information to know. (Hint: draw a "picture" of the RREF of $\mathsf{A}$ in each case).  

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>


<div class="col3">
**(a)** $\mathsf{A}$ is invertible

**(b)** $\mathsf{rref}(\mathsf{A}) = I$

**(c)** $\mathsf{A}$ has 8 pivots

**(d)** $\mathsf{A} \mathbf{0} = \mathbf{0}$ 

**(e)** $\mathsf{A} \mathsf{x} = \mathbf{0}$ has more than one solution.

**(f)** $T$ is one-to-one

**(g)** $T$ is onto

**(h)** $\mathsf{A} \mathsf{x} = \mathsf{b}$ has at least one solution for all $\mathsf{b} \in \mathbb{R}^8$.

**(i)** The columns of $\mathsf{A}$ span $\mathbb{R}^8$.

**(j)** There is a vector $\mathsf{b} \in \mathbb{R}^8$ such that $\mathsf{A} \mathsf{x} = \mathsf{b}$ has no solutions.

**(k)** There is a vector  $\mathsf{b} \in \mathbb{R}^8$ such that $\mathsf{A} \mathsf{x} = \mathsf{b}$ has infinitely many solutions.

**(l)** There is a vector $\mathsf{b} \in \mathbb{R}^8$ such that $\mathsf{A} \mathsf{x} = \mathsf{b}$ has exactly 17 solutions.

**(m)** There is a vector $b \in \mathbb{R}^8$ that can be written as a linear combination of the columns of $\mathsf{A}$ in more than one way.

**(n)** The rows of $\mathsf{A}$ span a 7 dimensional subspace of $\mathbb{R}^8$.

**(o)** The columns of $\mathsf{A}$ are linearly independent.

**(p)** The rows of $\mathsf{A}$ are linearly independent 

</div>


$$
\begin{array}{|c|c|c|c|c|}
\hline
& T: \mathbb{R}^8 \to \mathbb{R}^8  & T: \mathbb{R}^8 \to \mathbb{R}^8 & T: \mathbb{R}^7 \to \mathbb{R}^8  & T: \mathbb{R}^9 \to \mathbb{R}^8  \\
&  \text{$\mathsf{A}$  has rank 7} & \text{$\mathsf{A}$ has rank 8} & \text{$\mathsf{A}$  has rank 7} &  \text{$\mathsf{A}$  has rank 8} \\
\hline
(a) & & & & \\ \hline
(b) & & & & \\ \hline
(c) & & & & \\ \hline
(d) & & & & \\ \hline
(e) & & & & \\ \hline
(f) & & & & \\ \hline
(g) & & & & \\ \hline
(h) & & & & \\ \hline
(i) & & & & \\ \hline
(j) & & & & \\ \hline
(k) & & & & \\ \hline
(l) & & & & \\ \hline
(m) & & & & \\ \hline
(n) & & & & \\ \hline
(o) & & & & \\ \hline
(p) & & & & \\ \hline
\end{array}
$$



## A Tetrahedral Basis

In practice, we change bases because problems are computationally easier in another coordinate system or because we learn something by looking at a problem from the point of view of a different coordinate system. The following example illustrates this with ideas that arises both in chemistry and computer graphics. Below is the tetrahedral molecule methane, $\mathsf{CH}_4$. Its coordinates can be described in 3-dimensional space by the vectors below.

$$
\mathsf{C}=\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix},
\mathsf{H}_1=\begin{bmatrix} 0 \\ 0 \\ \frac{3}{2\sqrt{6}} \end{bmatrix},
\mathsf{H}_2=\begin{bmatrix}
 -\frac{1}{2 \sqrt{3}} \\ -\frac{1}{2} \\ -\frac{1}{2 \sqrt{6}}
\end{bmatrix},
\mathsf{H}_3=\begin{bmatrix}
 -\frac{1}{2 \sqrt{3}} \\ \frac{1}{2} \\ -\frac{1}{2 \sqrt{6}} 
 \end{bmatrix},
\mathsf{H}_4=\begin{bmatrix}
 \frac{1}{\sqrt{3}} \\0 \\-\frac{1}{2 \sqrt{6}}
 \end{bmatrix}
$$

![](images/ps5-methane.png){width=35%}

Let $\mathcal{M} = \{ \mathsf{H}_1, \mathsf{H}_2, \mathsf{H_3} \}$. Then $\mathcal{M}$ is a basis of $\mathbb{R}^3$, which we will call the **tetrahedral basis.** You can see from the plot that these vectors are linearly independent (not all on the same plane)

a. Express $\mathsf{H}_4$ in the tetrahedral basis. Hint: first compute $\mathsf{H}_1+\mathsf{H}_2+\mathsf{H}_3+\mathsf{H}_4$ using the coordinates above. You do not need to do any row reductions for this question.

b. Give the change of basis matrix $T$ that converts from the tetrahedral basis $\mathcal{M}$ to the standard basis $\mathcal{S}$ and compute its inverse that converts from the standard basis back to $\mathcal{M}$.

c. Chemists are interested in symmetry operations. These are linear transformations such that the atom looks the same after the transformation as it did before. For example one such operation is rotation $r_4$ by 120$^o$ around the  $\mathsf{H}_4$ axis. This rotation sends $\mathsf{H}_1$ to $\mathsf{H}_3$, $\mathsf{H}_3$ to $\mathsf{H}_2$, and $\mathsf{H}_2$ to $\mathsf{H}_1$. Give the matrix of $r_4$  in the $\mathcal{M}$ basis.

```{r,echo=TRUE}
r4.M = cbind(c(0,0,0),c(0,0,0),c(0,0,0))
rownames(r4.M) <- c("H1","H2","H3")
colnames(r4.M) <- c("H1","H2","H3")
r4.M
```

d. It is a pain to describe these transformations in the standard basis, but it is easy and elegant to do so in the methane basis. We can now use the change-of-basis matrix to get the matrix in the standard basis. Compute the rotation in the standard basis by multiplying out these matrices in R. Use your matrices from parts b and c.

![](images/ps5-change-basis-methane.png){width=100%}

e. Give the matrix in the $\mathcal{M}$-basis for the following symmetry transformations: (ii) The rotation $r_2$ around the $\mathsf{H}_2$ axis sending $\mathsf{H}_1$ to $\mathsf{H}_3$, $\mathsf{H}_3$ to $\mathsf{H}_4$, and $\mathsf{H}_4$ to $\mathsf{H}_1$. (iii) The reflection $\sigma_{1,2}$  across the plane containing $\mathsf{H}_1, \mathsf{H}_2,$ and $\mathsf{C}$. You do not need to give these in standard coordinates.


## Eigenbasis

Consider the matrix $A$ below
$$
A = \frac{1}{3} 
\begin{bmatrix}
 -14 & 13 & -2 \\ 
 -20 & 19 & -2 \\
 -23 & 19 & 1 \\
\end{bmatrix}
$$
and consider the following set basis of $\mathbb{R}^3$ (you don't have to check that it is a basis).
$$
\mathcal{B} = \left\{ 
v_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}, 
v_2 = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix},
v_3 = \begin{bmatrix} -1 \\ -1 \\ 2 \end{bmatrix} 
\right\}
$$

a. By hand, compute $A v_1$, $A v_2$, and $A v_3$ and see that each of these vectors is an eigenvector. From this, find the corresponding eigenvalues $\lambda_1, \lambda_2, \lambda_3$. A basis consisting of eigenvectors of $A$ is called an **eigenbasis**. We will see that they often, but not always, exist.

b. Let $B$ be the change of basis matrix from $\mathcal{B}$ to $\mathcal{S}$ (the standard basis), and compute the product $B^{-1} A B$. You can use `R`. Just report the matrix $B$ and the matrix $B^{-1} A B$. If you look closely at your answer, you should see something nice. Report what you see. 

```{r,echo-TRUE}
A = 1/3 * cbind(c(-14,-20,-23),c(13,19,19),c(-2,-2,1))
```




## House Party


Here is a plot of the grey house and four other houses, colored cyan, red, gold, and purple Reproduce this image using homogeneous coordinates. See [Homogeneous Coordinates][Homogeneous Coordinates].


![](https://raw.githubusercontent.com/Tom-Halverson/math236_s21/main/images/04-house-party.png){width=70%}

```{r,fig.height=5,fig.width=5,echo=TRUE}

#############
# your code for 3x3 matrices that create the transformed houses goes here 
A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1))

A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1))


####################
# you do not need to change this code
house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1));


plot(house[1,], house[2,], type = "n", xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab="x",ylab="y")
abline(h=-4:4, v=-4:4, col="gray", lty="dotted")

house.gold = A.gold %*% house
polygon(house.gold[1,], house.gold[2,], col = "gold", border = "blue")

house.cyan = A.cyan %*% house
polygon(house.cyan[1,], house.cyan[2,], col = "cyan", border = "blue")

house.red = A.red %*% house
polygon(house.red[1,], house.red[2,], col = "red", border = "blue")

house.purple= A.purple %*% house
polygon(house.purple[1,], house.purple[2,], col = "purple", border = "blue")

polygon(house[1,], house[2,], col = "gray", border = "blue")

```


<!--chapter:end:PS5-problem-set-5.Rmd-->


#  Problem Set 6

* Due: Wednesday March 03 by 11:59am CST (since Monday is a wellness day). 
* Upload your solutions to Moodle in a PDF. 
* You can download the [Rmd source file  for this problem set](https://github.com/Tom-Halverson/math236_s21/blob/main/PS6-problem-set-6.Rmd).


The Problem Set covers sections 5.1, 5.2, 5.3, 5.6.



## Rain and Sunshine Revisited

On PS4, we encountered the rain-sunshine matrix $A$ below
$$
A = \begin{bmatrix}
1/2 & 1/10 \\
1/2 & 9/10 \\
\end{bmatrix}.
$$
Perform the following calculations **by hand** and show your work.

a. Find the characteristic polynomial of $A$ and find its eigenvalues.

a. Find an eigenvector for each eigenvalue and describe the eigenspaces.

a. Diagonalize $A$.

a.  Use your answer to (c) to give a formula for $A^n$ and use this formula to compute $\displaystyle{\lim_{n\to \infty}} A^n$.

a. Write a loop in `R` that starts with the vector `v = c(1,0)` (i.e., a rainy day vector) and applies the matrix `A = cbind(c(1/2,1/2),c(1/10,9/10))` over and over again (100 times). Explain how your answer compares to the answer to the previous problem.




## The Square Root of a Matrix?

The matrix $A  =\begin{bmatrix} 7 & 2 \\ -4 & 1 \end{bmatrix}$ has characteristic polynomial $c(\lambda) = \lambda^2 - 8 \lambda + 15 = (\lambda -3)(\lambda - 5).$


a. Describe the eigenspaces of $A$.
a. Diagonalize $A$.
a. Find a matrix that makes sense to call $\sqrt{A}$. Then show that when you square this matrix, you really do get matrix $A$.

## Matrix Reconstruction

An unknown $3 \times 3$ matrix $M$ has eigenvectors and corresponding eigenvalues:
$$
\mathsf{v}_1 = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}, \  \lambda_1 = 1;
\qquad
\mathsf{v}_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix},\  \lambda_2 = \frac{9}{10};
\qquad
\mathsf{v}_3 = \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix},\  \lambda_3 = 0.
$$

a. Without using the matrix $M$, compute $M^{10} \mathsf{v}$ where $\mathsf{v} = \begin{bmatrix}7\\3\\4\end{bmatrix}$. (That is, use only the eigen-information.)

a. Describe all vectors $\mathsf{v}$, if there are any, such that $M^{n} \mathsf{v}  \to {\bf 0}$ as $n \to \infty$.

a. Is it possible to reconstruct $M$ from the evidence given? If so, then do it!  If not, explain what further information is needed.

## Coyotes and Roadrunners

This summer, Macalester's Ordway Natural History Study Area will  be stocked with a population of coyotes and roadrunners so that Math 236 students can study real-life predator-prey dynamics. From similar experiments, we expect the predator-prey dynamics to be governed by linear model below. The eigenvalues of the matrix are also given.
$$
\begin{bmatrix} \phantom{\Big\vert} r_{t+1}\phantom{\Big\vert}  \\ \phantom{\Big\vert} c_{t+1}\phantom{\Big\vert}  \phantom{\Big\vert} \end{bmatrix}
=\left[
\begin{array}{cc}
\phantom{\Big\vert} \frac{57}{50} & -\frac{6}{50} \\
\phantom{\Big\vert} \frac{4}{50} & \frac{43}{50} \\
\end{array}
\right] \begin{bmatrix} \phantom{\Big\vert} r_t \phantom{\Big\vert}  \\ \phantom{\Big\vert} c_t \phantom{\Big\vert} \end{bmatrix}
=
 \begin{bmatrix} \phantom{\Big\vert} 
\frac{57}{50} r_t -  \frac{6}{50}  c_t \\ \phantom{\Big\vert} \frac{4}{50} r_t +  \frac{43}{50} c_t \end{bmatrix},
$$
The eigenvalues and eigenvectors of this matrix are:
$$
\begin{array}{lcl}
\lambda_1 =  \frac{11}{10} =  1.1, & \qquad & \lambda_2 =  \frac{9}{10} = 0.9 \\
\mathsf{v}_1 = \begin{bmatrix} 3 \\ 1 \end{bmatrix} &&
\mathsf{v}_2 = \begin{bmatrix} 1 \\ 2 \end{bmatrix}
\end{array}
$$

```{r,echo=TRUE}
A = cbind(c(57/50,4/50),c(-6/50,43/50))
eigen(A)
```


a. If $r_0 =  10$ roadrunners and $c_0 = 15$ coyotes are introduced to the area, then give \emph{closed formulas} for the population of coyotes $c_t$ and roadrunners $r_t$ after $t$ years.

a.  In the long-term, in this model, what is the ratio of roadrunners to coyotes?

a. When another college tried the same experiment in their Arboretum, they introduced $r_0 =  5$ roadrunners and $c_0 = 10$ coyotes and both populations died off (as is verified in the computation below). Explain why this happens using the eigenvalues and eigenvectors.


```{r,echo=TRUE}
A = cbind(c(57/50,4/50),c(-6/50,43/50))
v = c(5,10)
for (i in 1:100) {v = A %*% v}
v
```

## Same Eigenvectors

Here are two matrices
```{r,echo=TRUE}
(A = cbind(c(-8, 3, 29), c(-40, 24, 46), c(10, 3, 11)))
(B = cbind(c(4, 3, 35), c(-49, 42, 55), c(13, 3, 26)))
```



a. Use R to show that they have the same eigenvectors but different eigenvalues.

a. Show that $A B = B A$ (even though we know that, in general, matrices do not commute).

a. Now let $A$ and $B$ be any $n \times n$ matrices which have the same eigenvectors. Prove that $AB = BA$. Hint: use the diagonalization of these two matrices. 


## Hunt Creek

Age-structured population models like we saw in the [Spotted Owl Example](https://tom-halverson.github.io/math236_s21/eigenvectors.html#northern-spotted-owl) are often called **Leslie Matrices**, named after the British ecologist P.H. Leslie. Here is the Leslie Matrix of a population of brook trout in Hunt Creek in Michigan. The population is categorized into 5 age categories: fingerlings (0,1), yearlings (1-2), young adults (2-3), adults (3-4), and adults (4-5). Right now the population is seen to be dying off. 

The vector $p(t)$ denotes the population at year $t$ broken into the 5 age categories: $$p(t) =  (f (t), y(t), ya(t), a_1(t), a_2(t))^T$$ and the matrix $L$ gives next year's population from this year's population:  $p_{t+1} = L p_t$.  Below is the Leslie matrix for this example.

$$
\begin{bmatrix}
f (t+1) \\
y(t+1) \\
ya(t+1) \\
a_1(t+1) \\
a_2(t+1)
\end{bmatrix}
= \begin{bmatrix}
0 & 0 & 37 & 64 & 82 \\
0.06 & 0 & 0 & 0 & 0 \\
0&0.28 & 0 & 0 & 0  \\
0&0&0.16& 0 & 0   \\
0&0&0&0.08& 0    \\
\end{bmatrix}
\begin{bmatrix}
f (t) \\
y(t) \\
ya(t) \\
a_1(t) \\
a_2(t)
\end{bmatrix}
=
\begin{bmatrix}
37 ya(t) + 64 a_1(t) + 82 a_2(t) \\
0.06 f(t) \\
0.28y(t) \\
0.16 ya(t) \\
0.08 a_1(t)
\end{bmatrix}
$$

```{r,echo=TRUE}
L = cbind(c(0,.06,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00))
```

The trout population in the creek is known to be dying off largely due to poisoning by the insecticide rotenone. The model demonstrates this behavior here, as can be seen in the folowing plot, which starts with 200 trout in each age group. *You shouldn't need to edit this plot.*

```{r,echo=TRUE}
start = c(200,200,200,200,200)  # the starting distribution
N = 35              # N is the number of iterations
X = matrix(0,nrow=5,ncol=N)  #  Store the results in a 3 x N matrix called X
X[,1] = start  # put start in the first column of X
# loop N times and put your results in X
for (i in 2:N) {X[,i] = L %*% X[,i-1]}
# Then plot the results
t = seq(1,N)  # time
plot(t,X[1,],type='l',col=1,ylim=c(0,8000),ylab="population",xlab="time (year)",
     main="Population in Age Group")
for (i in 1:5) {
  lines(t,X[i,],col=i)
  points(t,X[i,],col=i,pch=20,cex=.8)}
legend(22, 7600, legend=c("Fingerlings (0-1)", "Yearlings  (1-2)", "Young Adults (2-3)","Adults (3-4)","Adults (4-5)"), col=1:5, lty=1)
```

a. Give the meaning of the values 37, 64, 82, 0.06, 0.28, 0.16, 0.08 that appear in this matrix.

b. Compute the eigenvectors and eigenvalues of $L$ and relate what you find to population dynamics. In particular, use the eigen-information to

* Give the overall population growth rate.
* Give the limiting age distribution: that is, the long-run distribution of the population into the different age categories. Give your answer as proportions which sum to 1.

c. You are seeking funding from the Michigan DNR to support a cleanup effort. As part of your proposal, you argue that you believe that such a cleanup will most impact the youngest fish and will improve the survival rate of fingerlings to yearlings. Figure out (by trial and error) how high this survival rate will need to grow in order for the population to stop dying off. Justify your answer with eigenvalues and a plot. You should just be able to duplicate the code for the plot above (after changing the matrix).

## Glucose-Insulin

The hormone insulin helps regulate glucose metabolism in your blood. The presene of insulin helps your body absorb excess glucose. Here $G_t$ (glucose) and $H_t$ (insulin) are measued as excess values (in mg per 100 ml of blood) above the steady state. 
$$
\begin{bmatrix} G_{t+1} \\ H_{t+1} \end{bmatrix}
=
\begin{bmatrix}
0.9 & -0.4 \\
0.1 & 0.9 \\
\end{bmatrix}
\begin{bmatrix} G_t \\ H_t \end{bmatrix}= 
\begin{bmatrix} 0.9 G_t - 0.4 H_t \\ 0.1 G_t + 0.9 H_t \end{bmatrix}
$$

Here is what happens if we start at $(1,0)$ and iterate. That is we start with 1 unit excess glucose. Observe that the system spirals back to the steady state of $(0,0)$.
```{r,fig.width=6, fig.height=6}
A = cbind(c(.9,.1),c(-.4,.9))
start = c(1,0)
N = 50

X = matrix(0,nrow=2,ncol=N)
X[,1] = start

for (i in 2:N) {X[,i] = A %*% X[,i-1]}
plot(X[1,],X[2,],col='blue',xlim=c(-0.4,1.0),ylim=c(-0.4,0.4),xlab='glucose',ylab='insulin',pch=20,cex=1)
points(start[1],start[2],col="red")
grid(nx=10,ny=10)
```



We can plot the indivdual glucose and insulin coordinates over time. These are the x and y coordinates of the points in the above plot. You see the insulin responding to the excess glucose, and then the glucose being absorbed by the presence of insulin, and so on ...

```{r}
t = seq(1,N)
plot(t,X[1,],type='l',col='blue',ylim=c(-1,1),ylab="concentration (mg/ml)",xlab="time(hours)",main="Glucose-Insulin Over Time")
lines(t,X[2,],col='darkorange')
legend(10, 0.7, legend=c("Glucose", "Insulin"), col=c("blue", "darkorange"), lty=1)
```

The key point here is that the spiraling in the (x,y) plane or oscillating in the (x,t) plane corresponds to the presence of *complex eigenvalues*.  Your job is to perform an eigen-analysis of this problem:

a. Give a trajectory plot of this matrix using the `trajectory_plot` command from [Dynamical Systems in 2D]. Note: you need to copy the `trajectory_plot` code to your homework markdown file. When you do so, open the R chunk that contains it with ` ```{r,echo=FALSE}`. Then when you knit, it won't include all the code in the output.


a. Use R to find the eigenvalues and eigenvectors. 
a. Write out the eigenvalues in the form $\lambda = a \pm b i$ and the eigenvectors in the form $\vec{v} = \vec{u} \pm \vec{w} i.$
a. Use this information to find the scaling factor $|\lambda|$ for this matrix and the angle of rotation $\arctan(b/a)$. Give your answer in degrees.
a. Compare your answers from part (d) to the plots above to confirm that the system is doing what the eigenvalues predict.



<!--chapter:end:PS6-problem-set-6.Rmd-->


# Problem Set 7

* Due: Tuesday March 09 by 11:59am CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all calculations, including row reduction, matrix multiplication, eigenvector calculation and inverse matrices.**
* You can download the [Rmd source file  for this problem set](https://github.com/Tom-Halverson/math236_s21/blob/main/PS7-problem-set-7.Rmd).

This problem set covers [Network Centralities] and Sections 6.1, 6.2, and 6.3 on Orthogonal Projections.



## The Rise of Moscow

Here is a video I made in Module 1 explaining the [Airline Network](https://macalester.voicethread.com/myvoice/thread/15733200) example.

If you collaborated on this one with classmates (and I hope you did), please include the names of the students that you worked with.

Read [Network Centralities] and analyze the network of trade routes in medieval Russia given to you there.

a. Create a vector containing the normalized Degree Centralities. See Section \@ref(degree-centrality) for help.

a. Create a vector containing the Gould Index values. See Section \@ref(gould-index-summary) for help.

a. Plot the network where the size of the vertices is determined by Gould's Index and the size of the label is determined by degree centrality.

a. Create a data frame that contains Gould's Index and Degree Centralities. The rows should be labeled with the city names and the columns should be named by the centrality measures.  Sort according to Gould's Index.

a. Use Gould's Index to decide whether Moscow's dominance was solely due to its geographic location.

a. Compare the Gould's Index and Degree Centrality rankings and note any interesting findings. See Section \@ref(step-5) for help.


## Orthogonal Complements


Here are two subspaces of $\mathbb{R}^5$ that we have seen before. (See PS4.2 and PS5.2)
$$
\begin{align}
\mathsf{Z} & = \left\{ \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} \ \bigg\vert \ x_1 + x_2 + x_3 + x_4 + x_5 = 0 \right\}.
\\
\mathsf{F} & = \left\{ \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} \ \bigg\vert \  x_3 =  x_1 + x_2, x_4 = x_2 + x_3, x_5 = x_3 + x_4 \right\}.
\end{align}
$$
Find the orthogonal complement of each subspace in $\mathbb{R}^5$. For each example, compute $\dim(W) + \dim(W^\perp)$.

## Orthogonal Diagonalization

Recall that a square $n \times n$ matrix is symmetric when $A^{\top} = A$. We learned that the eigenvectors of a symmetric matrix form an orthogonal basis of $\mathbb{R}^n$. In this problem, you will confirm that this holds for the following symmetric matrix
$$
A = 
\begin{bmatrix}
 0 &  8 & 10 & -4 \\
 8 & 4 & 28 & 6 \\
 10 & 28 & 3 & -4 \\
 -4 & 6 & -4 & -7
\end{bmatrix}. 
$$

a. Find the eigenvalues and eigenvectors of $A$.

b. Confirm that the eigenvectors returned by R are an orthonormal set (do this in a single calculation).

c. Express the vector $\mathsf{v} = \begin{bmatrix} 2 & -4 &  -9  & -2 \end{bmatrix}^{\top}$ as a linear combination of the eigenvectors. Use the fact that the eigenvectors are orthonormal. (Don't augment and row reduce.)

d.  Let $P$ be the matrix of these normalized, orthogonal eigenvectors.
Diagonalize $A$ using $P$. Just write out $A = P D P^{-1}$. Congratulations: you have **orthogonally diagonalized** the symmetric matrix $A$!

**Turn in:** Your R code and the output for each part. For parts (c) and (d), you need to make it clear what your final answers are. 

## Cosine Similarity 

In high dimensional space $\mathbb{R}^n$ a common measure of similarity between two vectors  is  **cosine similarity**:  the cosine of the angle $\theta$ between the vectors. We calculate this value as follows:
$$
\cos(\theta) = \frac{ \mathsf{u} \cdot \mathsf{v}} {\| \mathsf{u}\| \, \|\mathsf{v}\|} = \frac{ \mathsf{u} \cdot \mathsf{v}} {\sqrt{\mathsf{u} \cdot \mathsf{u}} \sqrt{\mathsf{v} \cdot \mathsf{v}}}.
$$
This measure has the following nice properties:

* $-1 \le \cos(\theta) \le 1$,
* $\cos(\theta)$ is close to 1 if $\mathsf{u}$ and $\mathsf{v}$ are closely aligned,
* $\cos(\theta)$ is close to 0 if  $\mathsf{u}$ and $\mathsf{v}$ are are orthogonal, 
* $\cos(\theta)$ is close to $-1$ if $\mathsf{u}$ and $\mathsf{v}$ are polar opposites.
* $\cos(\theta)$ is positive if $\theta$ is acute (less than $90^o$).
* $\cos(\theta)$ is negative if $\theta$ is obtuse (greater than $90^o$).



a. Write a function `cosine_similarity` that takes as input two vectors $\mathsf{u}$ and $\mathsf{v}$ and returns the value of $\cos(\theta)$ for the angle $\theta$ between $\mathsf{u}$ and $\mathsf{v}$. Below is a shell of the code that you need. Right now it always just returns 1.You need to fix that up. Demonstrate that your code works on some vectors in $\mathbb{R}^5$. Use vectors that are orthogonal, closely aligned, and close to polar opposites.

```{r,echo=TRUE}
cosine_similarity <- function(u, v) {
  # your code goes here!
  # find the cosine of the angle between u and v
  cosine = 1
  return (cosine)
}
```


b. In the file [US_Senate_s21.Rmd](https://drive.google.com/open?id=1oN6-rlOBdDj7LLzzwFC4RGkKiLIUJDcV) you will find vectors of the voting record of the Senate in the 109th US Congress (2007-2008). You will see how to take the dot product of the voting record of two different senators. The dot product is always an integer. Explain what it counts. It is the number of something or possibly the difference of two things.

c. Use your `cosine_similary` function to find the cosine similarity between every pair of the following senators:

    + Hilary Clinton (D, NY), presidential candidate 2016
    + John McCain (R, AZ), presidential candidate 2008
    + Barack Obama (D, IL), president 2008-2016
    + Susan Collins (R, ME), moderate Republican

Does the cosine similarity pick up on the fact that Senator Collins is a "moderate Republican"?

d. The senate majority leader of the 109th Congress was Bill Frist (R, TN), and the senate minority leader was Harry Reid (D, NV). 

    * Create a function  `classify_senator(senator)` that returns "R" or "D" depending on the cosine similarity of `senator` to `frist` and to `reid`. You will have to write [an "if ... else statement" (here is the syntax)](https://www.tutorialspoint.com/r/r_if_else_statement.htm).
  
    * There is a chunk of code in the R file I've given you that gets you started.

    * Then run the my_classification code that we have given. I dentify any senators that have been *misclassified* using this method, meaning that their votes are more similar to the leader of the opposing party. 
    
    * Jim Jeffords (I, VT) was a Republican who became and Independent in 2001 and then caucused with the Democrats. How does your classifier handle Jeffords?


## Fibonacci Orthogonality

In problem 1, you saw that the vector space of Fibonacci vectors $\mathsf{F} \subseteq \mathbb{R}^5$ is a 2-dimensional subspace of $\mathbb{R}^5$. 


a. Below is an orthogonal basis of $\mathsf{F}$. Check that the basis vectors $\{\mathsf{f}_1, \mathsf{f}_2\}$ are in $\mathsf{F}$ and are orthogonal.
$$
\mathsf{F} = \mathsf{span} \left\{
 \mathsf{f}_1 = \begin{bmatrix} 1 \\ 0 \\ 1 \\ 1 \\ 2 \end{bmatrix}, 
 \mathsf{f}_2 = \begin{bmatrix} -9 \\ 7 \\ -2 \\ 5 \\ 3\end{bmatrix}
 \right\}, \hskip.5in
 \mathsf{v}  =  \begin{bmatrix} 1 \\ 2 \\ 3 \\ 5 \\ 7\end{bmatrix}.\hskip.5in
$$
![](images/projectFib.png){width=70%}

b. The vector $\mathsf{v}$ above is **not** in $\mathsf{F}$ (check that!). Its **projection** $\mathsf{p}$ onto $\mathsf{F}$ is given by the formula below. Compute $\mathsf{p}$. This is closest approximation of $\mathsf{f}$ with a Fibonacci vector.
$$
\mathsf{p} = \frac{ (\mathsf{v} \cdot  \mathsf{f}_1)}{ ( \mathsf{f}_1 \cdot  \mathsf{f}_1)} \mathsf{f}_1 +  \frac{ (\mathsf{v} \cdot  \mathsf{f}_2)}{ ( \mathsf{f}_2 \cdot  \mathsf{f}_2)}  \mathsf{f}_2.
$$
This formula requires that the basis be orthogonal.

c. The **residual vector** is the vector $\mathsf{r}  = \mathsf{v} - \mathsf{p}$. Compute $\mathsf{r}$.

d. Show that $\mathsf{r}$ is orthogonal to $\mathsf{f}_1$ and $\mathsf{f}_2$.

d. Compute $||\mathsf{r}||$. This is the distance from $\mathsf{v}$ to $\mathsf{F}$ (i.e., how far $\mathsf{v}$ is from being Fibonacci).

## Least Squares Solution to $\mathsf{A} x = \mathsf{b}$

Find the least-squares solution to the following inconsistent matrix equation $\mathsf{A} x  = \mathsf{b}$. 
$$
\left[
\begin{array}{ccc}
 1 & 1 & 2 \\
 1 & 0 & 2 \\
 -1 & 1 & 1 \\
 1 & 0 & 1 \\
 1 & 1 & 1 \\
\end{array}
\right] \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 
\begin{bmatrix} 7 \\  11 \\ -3 \\ 10 \\ 7 \end{bmatrix}. \hskip5in
$$
Here are the steps you should follow:

a. Show that this system of equations is inconsistent.
b. Make the normal equations: $\mathsf{A}^T \mathsf{A} x = \mathsf{A}^T \mathsf{b}$.
c. Solve the normal equations for $\hat{x}$ (by hand or software).
d. Get the projection $\hat{\mathsf{b}}$ of $\mathsf{b}$ onto $\mathrm{Col}(\mathsf{A})$ by $\hat{\mathsf{b}} = \mathsf{A} \hat{x}$.
e. Get the residual $\mathsf{r}  = \mathsf{b} - \hat{\mathsf{b}}$ and compute its length.





<!--chapter:end:PS7-problem-set-7.Rmd-->


# Problem Set 8

* Due: Tuesday March 9 by 11:55pm CST. 
* Upload your solutions to Moodle in a PDF. 
* Please feel free to **use RStudio for all calculations**, including row reduction, matrix multiplication, eigenvector calculation, inverse matrices, and the Gram-Schmidt process.
* You can download the [Rmd source file  for this problem set](https://github.com/mathbeveridge/math236_f20/blob/main/PS8-problem-set-8.Rmd).
* Four out of five questions require you to use RStudio. I've provided a [PS8 solution template](https://github.com/mathbeveridge/math236_f20/blob/main/PS8-template.Rmd) if you want to use it.

This problem set covers Sections 6.1 - 6.5 Orthogonality and the explorations on [Voting Patterns in the US Senate][Voting Patterns in the US Senate] and [Least Squares Approximation][Least Squares Approximation].  

```{r}
require(pracma)
```



## Two Orthogonality Properties

Give rigorous proofs for each of the following problems. Here "rigorous" means that the heart of your explanation must use equations and calculations to explicitly justify any intuitive reasoning that you provide.

a. Let $W$ be a subspace of $\mathbb{R}^5$ with orthonormal basis $\mathsf{w}_1, \mathsf{w}_2, \mathsf{w}_3$.  Let $W^{\perp}$ be its orthogonal complement with orthonormal basis $\mathsf{v}_1, \mathsf{v}_2$.  Prove that if $\mathsf{u}$ is in both $W$ and $W^{\perp}$ then $\mathsf{u}$ must be the zero vector. 

b. Let $U$ and $V$ both be $n \times n$ orthogonal matrices. Prove that the matrix $UV$ is also an orthogonal matrix.


## Bases for a Subspace and its Orthogonal Complement

Let $W \subset \mathbb{R}^6$ be the set of solutions to
\begin{align}
x_1 + x_2 + x_3 + x_4 + x_5 + x_6 &= 0 \\
x_1 + x_2 - x_3 + x_4 + x_5 - x_6 &= 0 
\end{align}

a. Find a basis for $W$ by using row reduction on the appropriate matrix.
b. Use your answer from (a) to find a "human readable" orthogonal basis for $W$ where the entries have integer values.
c. Find a basis for $W^{\perp}$ without using the Gram-Schmidt process.
d. Use your answer from (c) to find a "human readable" orthogonal basis for $W^{\perp}$ where the entries have integer values.

**Turn in:** Your code and its output. You must make it clear what your final answers are for each problem. 

## Eigensystem of a Symmetric Matrix

Recall that a square $n \times n$ matrix is symmetric when $A^{\top} = A$. We learned that the eigenvectors of a symmetric matric form an orthogonal basis of $\mathbb{R}^n$. In this problem, you will confirm that this holds for the following symmetric matrix

$$
A = 
\begin{bmatrix}
 0 &  8 & 10 & -4 \\
 8 & 4 & 28 & 6 \\
 10 & 28 & 3 & -4 \\
 -4 & 6 & -4 & -7
\end{bmatrix}
. 
$$

a. Find the eigenvalues and eigenvectors of $A$.

b. Confirm that the eigenvectors returned by R are an orthonormal set. (Pro tip: you can do this in one calculation!)

c. Express the vector $\mathsf{v} = \begin{bmatrix} 2 & -4 &  -9  & -2 \end{bmatrix}^{\top}$ as a linear combination of the eigenvectors. Use the fact that the eigenvectors are orthonormal. (Don't augment and row reduce.)

d.  Let $P$ be the matrix of these normalized, orthogonal eigenvectors.
Diagonalize $A$ using $P$. Just write out $A = P D P^{-1}$. Congratulations: you have **orthogonally diagonalized** the symmetric matrix $A$!

**Turn in:** Your R code and the output for each part. For parts (c) and (d), you need to make it clear what your final answers are. 

## Modeling Fertility in 1888 Switzerland

RStudio comes with some sample datasets, including `swiss`,  an 1888 dataset of Swiss socio-economic data. In 1888, Switzerland was entering a demographic transition: birth rates were beginning to decrease as the country entered the industrial age. 

The rows correspond to 47 French-speaking provinces. These provinces were at different stages of industrialization.
There are 6  socio-economic columns, all scaled to the range $[0,100]$. 

* Fertility `fert`: standardized fertility rate
* Agriculture `agric`: percent of men in agricultural occupations
* Examination `exam`: percent of draftees receiving highest mark on army examination
* Education `educ`: percent education beyond primary school for draftees
* Catholic `cath`: percent Catholic (instead of Protestant)
* Infant.Mortality `mort`: percent of live births who live less than one year

We want to find the best-fit linear model
<center>
fert = $c_1$ + $c_2$ agric + $c_3$ exam + $c_4$ educ + $c_5$ cath + $c_6$ mort.
</center>

Obviously, *this data set shows its age*. We are attempting to model fertility rates of **women** by using indirect measures about **male occupation and education levels**. This is problematic and would have to be addressed in a thorough analysis. We limit our use of this classic data to illustrate the method of least squares.

a. Model this problem as a least-squares approximation of an inconsistent system $A \mathsf{x} = \mathsf{y}$. Find the least squares solution $\hat{\mathsf{x}}$ and confirm that the residual vector $\mathsf{z}$ is orthogonal to the columns of $A$.

    To get you started, here is some convenient code to get the column vectors of the `swiss` dataset.

```{r echo=TRUE}

fert = swiss[,1]
agric = swiss[,2]
exam = swiss[,3]
educ = swiss[,4]
cath = swiss[,5]
mort = swiss[,6]

```

**Turn in:** Your code and the output.

b. The residual vector $\mathsf{z}$ measures  the quality of fit of our model. But how do we turn this into a meaningful quantity?
One method is to look at the **coefficient of determination**, which is more commonly refered to as the "$R^2$ value."

    Let $\mathsf{y} = [ y_1, y_2, \ldots, y_n ]^{\top}$ be our target vector with least squares solution $\hat{\mathsf{y}} = A \hat{\mathsf{x}}$ and residual vector $\mathsf{z} = \mathsf{y} - \hat{\mathsf{y}}$. Let 
$$
a = \frac{1}{n} ( y_1 + y_2 + \cdots + y_n)
$$
    be the average of the entries of target vector $\mathsf{y}$ and let $\mathsf{y}^* = [a, a, \ldots, a]$. (We call this vector "y star", so `ystar` would be a fine name in R.) The $R^2$ value is
$$
R^2 = 1 - \frac{\| \mathsf{y} - \hat{\mathsf{y}} \| }{\| \mathsf{y} - \mathsf{y}^* \|} = 1 - \frac{\| \mathsf{z} \|}{\| \mathsf{y} - \mathsf{y}^* \|}.
$$
    This is a number in $[0,1]$. If the $R^2$ value is near 1, then our model does a good job at "explaining" the behavior of $\mathsf{y}$ via a linear combination of the columns of $A$. 

    Find the $R^2$ value for our least squares solution to the Swiss data. Here are some helpful functions:

    * `sum(vec)` returns the sum of the entries of the vector `vec`
    * `length(vec)` returns the number of entries in the vector `vec`
    * `rep(a, n)` creates a constant vector of length $n$ where every entry is $a$.
    * `Norm(vec)` from the `pracma` package returns the magnitude (Euclidean length) of the vector `vec`.

    **Turn in:** Your code and the $R^2$ value that you calculated.


    **Remark:** There is certainly more to say about ethical use of data, as well as $R^2$ values and fitting models to data. To learn more, you should take STAT 155 Introduction to Statistical Modeling.


## Cosine Similarity for US Senators

In high dimensional space $\mathbb{R}^n$ a common measure of similarity between two vectors  is  **cosine similarity**:  the cosine of the angle $\theta$ between the vectors. We calculate this value as follows:
$$
\cos(\theta) = \frac{ \mathsf{v} \cdot \mathsf{w}} {\| \mathsf{v}\| \, \|\mathsf{w}\|} = \frac{ \mathsf{v} \cdot \mathsf{w}} {\sqrt{\mathsf{v} \cdot \mathsf{v}} \sqrt{\mathsf{w} \cdot \mathsf{w}}}.
$$
This measure has the following nice properties:

* $-1 \le \cos(\theta) \le 1$,
* $\cos(\theta)$ is close to 1 if $\mathsf{u}$ and $\mathsf{v}$ are closely aligned,
* $\cos(\theta)$ is close to 0 if  $\mathsf{u}$ and $\mathsf{v}$ are are orthogonal, 
* $\cos(\theta)$ is close to $-1$ if $\mathsf{u}$ and $\mathsf{v}$ are polar opposites.

Let's use cosine similarity to compare the 99 US Senators (one senate seat was not filled at the time) from the 109th US Congress (2007-2008).

Here is code that loads in the voting records. Each row contains the senator's name, party affiliation, state, and then their record on 46 resolutions. The votes are encoded as a sequence of
0s, 1s, and -1s, where 1 means a 'aye' vote, -1 means a 'nay' vote, and 0 means the senator abstained. For example, the row corresponding to Joseph Biden is
<center>
`biden = ["Biden", "D", "DE", -1, -1, 1, 1, ... , 1, -1]`.
</center>




```{r, echo=TRUE}

library(readr)

senate.vote.file = "https://raw.github.com/mathbeveridge/math236_f20//main/data/SenateVoting109.csv"

record <- read_csv(senate.vote.file, col_names = TRUE)

clinton = record[record$Name == 'Clinton',]
collins = record[record$Name == 'Collins',]
frist = record[record$Name == 'Frist',]
mccain = record[record$Name == 'McCain',]
obama = record[record$Name == 'Obama',]
reid = record[record$Name == 'Reid',]


knitr::kable(
  head(record[,1:10],10), booktabs = TRUE,
  caption = 'First 10 rows of the 109th US Senate voting records'
)


```

a. Your first task is to implement our cosine function.

    * `get_vote_cosine_similarity(senator1, senator2)` returns the cosine of the angle between the vote vectors of `get_votes(senator1)` and `get_votes(senator2)`

    **Note:** The R command `length(vec)` returns the number of entries in the vector `vec`. You can get the magnitude (Euclidean length) of vector `vec` using the `pracma` function `Norm(vec)`. 

    To get you started, we have written the helper function `get_votes(senator)`. This function returns the vote vector for the senator. These are the values in columns 3 to 49.
    
    **Turn In:** Your implementation of the function.


```{r, echo=TRUE}


# returns the vote vector for the senator
get_votes <- function(senator) {
  #unlist makes this dataframe row into a regular vector
  return (unlist(senator[,4:49])) 
}

# Implement this function!
# Return the cosine of the angle between the vote vectors 
# of senator1 and senator2
get_vote_cosine_similarity <- function(senator1, senator2) {
  votes1 = get_votes(senator1)
  votes2 = get_votes(senator2)
  
  # your code goes here!
  # find the cosine of the angle between votes1 and votes2
  cosine = 1
  
  return (cosine)
}

```


b. Find the cosine of the angles between every pair of the following senators of note:

    + `clinton`: Hilary Clinton (D, NY), presidential candidate 2016
    + `mccain`: John McCain (R, AZ), presidential candidate 2008
    + `obama`: Barack Obama (D, IL), president 2008-2016
    + `collins`: Susan Collins (R, ME), moderate Republican

    Does the cosine similarity pick up on the fact that Senator Collins is a "moderate Republican"?
  

    **Turn In:** Your code and the output. Comment on the similarities between all pairs of senators.

c. The senate majority leader of the 109th Congress was Bill Frist (R, TN). The senate minority leader was Harry Reid (D, NV). 

    * Create a function  `classify_senator(senator)` that returns "R" or "D" depending on the cosine similarity of `senator` to `frist` and to `reid`.
You will have to write [an "if ... else statement" (here is the syntax)](https://www.tutorialspoint.com/r/r_if_else_statement.htm).

    * Then run the `classify_all()` function that we have written for you. You will see how many senators are *misclassified*, meaning that their votes are more similar to the leader of the opposing party. 

    **Note:** Your misclassified list will include Jim Jeffords (I, VT), a Republican who became and Independent in 2001 and then caucused with the Democrats. Your classifer correctly aligns the Independent Jeffords with the Democrats.


    **Turn In:** Your code and the output. Comment on how many senators are misclassified, as well as their party affiliations.

```{r, echo=TRUE}



# Implement this function!
# return "R" if senator is closer to frist
# return "D" if senator is closer to reid
classify_senator <- function(senator) {

  # your code goes here!
  # use the get_vote_cosine_similarity() method to compare senator with frist and reid
  val = "R"
  
  return (val)
}

# returns a dataframe containing all misclassified senator names
# and their party affiliations
classify_all <- function() {

  mismatch = vector()
  
  for (i in 1:99) {
    senator = record[i,]
    party = classify_senator(senator)
    if (party !=  record[i,2]) {
      mismatch = c(mismatch, i)
    }
  }
  
  record[mismatch,1:2]
}


# Uncomment the next line to classify all senators!

# classify_all()


```


<!--chapter:end:PS8-problem-set-8.Rmd-->

---
title: "Problem Set 8"
author: "Your Name Here"
date: "Due: Friday March 12 at noon CST"
output:
  html_document: default
---



```{r}
require(pracma)
```


# Problem Set 8

Download the [Rmd source file  for this problem set](https://raw.githubusercontent.com/Tom-Halverson/math236_s21/main/PS8.Rmd).



Upload a completed, knitted .html version of this file on Moodle. If you have collaborated with others on this assignment (encouraged), please include their names here (no penalty).


## Subspace Projection

Last week we learned how to project onto a subspace, but our method required that we have an orthogonal basis. Here we will see that our least-squares method allows us to project onto a subspace with any (not-necessarily-orthgoonal) basis. Consider the following subspace of $\mathbb{R}^4$. We can turn it into a least-squares problem by making it the column space of a matrix $A$. That is $W = Col(A)$.
$$
W = span\left\{
\begin{bmatrix} 1 \\ 2 \\ -1 \\ -2 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \end{bmatrix}
\right\}, \hskip.6in
b = \begin{bmatrix} 9 \\ 5 \\ 5 \\ 8 \end{bmatrix}.
$$
```{r}
(A = cbind(c(1,2,-1,-2),c(1,2,3,4),c(1,0,1,0)))
b = c(9,5,5,8)
```


a. Perform a matrix computation on A to show that the basis is not orthogonal. 
b. Show that b is not in W. 
c. Find the least-squares projection of b onto W. Find both $\hat x$ and $\hat b$.
d. Calculate the residual vector r, show that $r \in W^\perp$, and find $||r||$.
e. Consider the following derivation from the normal equations:
$$
A^T A x = A^T b \qquad \Longrightarrow \qquad \hat x = (A^T A)^{-1} A^T b.
$$
The **pseudoinverse** is the matrix 
$$
A^+ = (A^T A)^{-1} A^T
$$
From what we see above it gives the *least-squares* solution to $A x = b$. Compute the matrix $A^+$, multiply it by $b$, and show that you get $\hat x$.

f. Continuing this story,
$$
\hat b = A \hat x \qquad \Longrightarrow \qquad \hat b =  A (A^T A)^{-1} A^T b.
$$
The **projection** matrix onto the subspace $W$ is the matrix
$$
P = A (A^T A)^{-1} A^T.
$$
Compute the matrix $P$, apply it to $b$, and see that you get the projected value $\hat b$.
g. Compute $P^2$ and compare it to $P$. Explain why this happens.
f. Use it to project `b2 = c(1,2,3,4)` onto $W$.
g. Find the eigenvalues of $P$. They are nice. Explain (briefly) where the eigenvectors of this matrix are in relation to $W$.


## Least-Squares Polynomials

Here is the problem that we discussed in class with a quadratic fit to it. Make a cubic, quartic, and quintic fit to this data. Turn in a plot of each. Comupute the length of the residual in each case. Which do you think is the best model of the data?


```{r, echo=TRUE}
x = c(1,2,3,4,5,6)
y = c(7,2,1,3,7,7)
(A = cbind(x^0,x,x^2))
xhat = solve(t(A)%*%A,t(A)%*%y)
yhat = A %*% xhat
r = y - yhat
t(r) %*% r
```

```{r ps8-quadplot, fig.width=4.5, fig.height=4.5, echo=FALSE}
#plot the original set of points
plot(x,y,pch=19,xlim=c(0,7),ylim=c(0,10), main='the best-fit quadratic function')
# generate points for the fitted line and plot it
tt = seq(0,7,len=100)  
lines(tt,xhat[1]+xhat[2]*tt+xhat[3]*tt^2,col='blue')
# add the residuals to the plot
for (i in 1:length(x)) {
  lines(c(x[i],x[i]),c(y[i],yhat[i]), col='red')
}
#add yhat to the plot
points(x,yhat,pch=19,col='orange')
#put the original points back on the plot last so we can see them 
points(x,y,pch=19,col="black")
grid()
```

## Fuel Efficiency

Below is a classic data set of fuel efficiency in 38 different automobiles.
```{r}
MPG=c(16.9,15.5,19.2,18.5,30,27.5,27.2,30.9,20.3,17,21.6,16.2,20.6,20.8,18.6,18.1,17,17.6,16.5,18.2,26.5,21.9,34.1,35.1,27.4,31.5,29.5,28.4,28.8,26.8,33.5,34.2,31.8,37.3,30.5,22,21.5,31.9)
lbs=c( 3967.6,3689.14,3280.55,3585.4,1961.05,2329.6,2093,2029.3,2575.3,2857.4,2543.45,3103.1,3075.8,2793.7,3294.2,3103.1,3494.4,3389.75,3599.05,3485.3,2352.35,2648.1,1797.25,1742.65,2429.7,1810.9,1942.85,2429.7,2361.45,2457,2325.96,2002,1838.2,1938.3,1992.9,2561.65,2366,1925)
HP= c(155,142,125,150,68,95,97,75,103,125,115,133,105,85,110,120,130,129,138,135,88,109,65,80,80,71,68,90,115,115,90,70,65,69,78,97,110,71)
Cyl=c(8,8,8,8,4,4,4,4,5,6,4,6,6,6,6,6,8,8,8,8,4,6,4,4,4,4,4,4,6,6,4,4,4,4,4,6,4,4)
Car = c("BuickEstateWagon", "FordCountrySquireWagon", "ChevyMalibuWagon", "ChryslerLeBaronWagon", "Chevette", "ToyotaCorona", "Datsun510", "DodgeOmni", "Audi5000", "Volvo240GL", "Saab99GLE", "Peugeot694SL", "BuickCenturySpecial", "MercuryZephyr", "DodgeAspen", "AMCConcordD/L", "ChevyCapriceClassic", "FordLTD", "MercuryGrandMarquis", "DodgeStRegis", "FordMustang4", "FordMustangGhia", "MazdaGLC", "DodgeColt", "AMCSpirit", "VWScirocco", "HondaAccordLX", "BuickSkylark", "ChevyCitation", "OldsOmega", "PontiacPhoenix", "PlymouthHorizon", "Datsun210", "FiatStrada", "VWDasher", "Datsun810", "BMW320i", "VWRabbit")
df = data.frame(cbind(lbs,HP,Cyl,MPG)) #Convert to data frame
rownames(df)=Car
df
```

a. Fit a linear model of the form
$$
mpg = a_0 + a_1 lbs + a_2 HP + a_3 Cyl.
$$
Find the coefficients $a_0,a_1,a_2,a_3$ and the length of the residual. If you have taken Stat 155, you can see that we are doing the exact same thing by comparing your results with

```{r}
cars_lm = lm( MPG ~ lbs + HP + Cyl)
summary(cars_lm)
```
b. Add the cars weight in tons to your model and solve $mpg = a_0 + a_1 (lbs) + a_2 (HP) + a_3 (Cyl) + a_4 (tons).$ Compare the coefficients you get with those that you got in part a. Give a short explanation of what you see using some of the linear algebra language that we have learned in the course. 
```{r}
tons =  c(1.98, 1.84, 1.64, 1.79, 0.98, 1.16, 1.05, 1.01, 1.29, 1.43, 1.27, 1.55, 1.54, 1.40, 1.65, 1.55, 1.75, 1.69, 1.80, 1.74, 1.18, 1.32, 0.90, 0.87, 1.21, 0.91,0.97, 1.21, 1.18, 1.23, 1.16, 1.00, 0.92, 0.97, 1.00, 1.28, 1.18, 0.96)
```

c. The residual vector $\mathsf{r}$ measures the quality of fit of our model. But how do we turn this into a meaningful quantity? One method is to look at the **coefficient of determination**, which is more commonly refered to as the "$R^2$ value." 

* You can see the $R^2$ value of your fit in part (a) under the "Multiple R-squared" output in the linear model summary above. 

* If $\mathsf{y} = [ y_1, y_2, \ldots, y_n ]^{\top}$ is our target vector with least-squares solution $\hat{\mathsf{y}} = A \hat{\mathsf{x}}$ and residual vector is $\mathsf{r} = \mathsf{y} - \hat{\mathsf{y}}$. Let 
$$
a = \frac{1}{n} ( y_1 + y_2 + \cdots + y_n)
$$
be the average or mean of the entries of target vector $\mathsf{y}$ and let $\mathsf{y}^* = [a, a, \ldots, a]$. (We call this vector "y star", so `ystar` would be a fine name in R.) The $R^2$ value is
$$
R^2 = 1 - \frac{\| \mathsf{y} - \hat{\mathsf{y}} \|^2 }{\| \mathsf{y} - \mathsf{y}^* \|^2} = 1 - \frac{\| \mathsf{r} \|^2}{\| \mathsf{y} - \mathsf{y}^* \|^2}.
$$
* The $R^2$ value is a number in $[0,1]$. The squared-length $|| \mathsf{y} -\mathsf{y}^*||^2$ is the total variance: that is, how much the data varies from the mean, and $\frac{\| \mathsf{r} \|^2}{\| \mathsf{y} - \mathsf{y}^* \|^2}$ tells us the fraction of the total variance that is explained by our model. Thus, if  $R^2$  is near 1, then our model does a good job at "explaining" the behavior of $\mathsf{y}$ via a linear combination of the columns of $A$. 

 *   **To do**: Find the $R^2$ value for our least squares solution to the cars data in part (a). Here are some helpful functions:
    + `mean(vec)` returns the mean (average) of the entries of the vector `vec`
    + `rep(a, n)` creates a constant vector of length $n$ where every entry is $a$.
    + `Norm(vec)` from the `pracma` package returns the magnitude (Euclidean length) of the vector `vec`.
To learn more, you should take STAT 155: Introduction to Statistical Modeling.


## Fourier Analysis

In Fourier analysis one uses trigonometric functions to model oscillatory behavior in data. These methods have important applications in the study of sound or video signals, financial data, medicine, and engineering (to mention just a few). For example, consider the following set of 200 data points.
```{r,echo=FALSE}
t = xx = seq(0,19.9,.1)
y = c(3.407646, 3.656257, 4.567893, 3.692689, 4.650019, 4.180795, 4.220037, 4.842083, 4.600134, 3.695645, 3.739377,
4.807793, 4.290227, 4.351877, 4.659800, 4.706735, 4.603592, 4.657165, 5.135868, 4.486025, 4.644551, 4.624029,
5.329163, 5.639380, 5.693772, 4.806000, 5.427808, 5.673742, 5.121300, 5.394885, 4.739374, 5.084819, 5.460250,
4.578189, 4.612040, 4.534047, 4.201825, 4.290607, 3.887900, 3.349325, 3.660084, 3.200437, 2.490044, 2.720811,
2.762054, 3.041436, 2.018788, 2.188567, 2.054767, 2.047622, 2.294727, 2.699933, 3.242642, 3.325224, 3.411680,
2.590417, 3.118911, 2.916444, 3.081886, 4.100586, 4.210242, 3.835767, 3.546563, 4.456711, 3.970233, 4.128838,
4.774915, 3.610540, 4.395443, 3.764436, 4.407476, 4.243399, 3.684473, 3.779193, 3.815080, 4.567609, 4.576654,
4.774486, 4.847797, 3.970489, 4.631950, 4.535347, 5.292626, 4.844237, 5.243421, 4.949116, 4.824773, 4.830172,
5.379016, 5.289537, 5.832770, 4.872205, 4.833122, 4.641696, 4.584196, 5.279393, 4.307142, 4.926093, 3.904820,
3.748701, 3.460324, 3.726250, 3.636625, 3.896051, 3.505842, 2.723539, 3.432293, 2.788161, 2.873195, 2.347629,
2.515592, 2.618861, 2.622653, 2.263514, 2.580999, 2.675959, 3.071311, 3.375476, 2.769042, 3.177973, 3.808895,
3.088136, 3.101224, 3.828743, 4.070292, 4.477982, 3.982855, 4.213733, 4.396489, 4.036487, 4.475438, 4.534266,
3.885322, 4.555555, 4.776902, 4.577201, 4.374555, 4.184732, 3.960706, 3.885492, 4.246883, 4.885794, 5.117945,
4.213779, 4.734693, 5.359801, 4.680284, 5.586846, 4.995826, 5.074366, 4.647961, 4.935794, 5.074724, 5.092661,
4.660553, 5.386633, 5.101599, 5.585815, 4.399249, 4.799980, 4.546865, 4.375893, 4.305302, 3.382458, 3.915698,
2.980115, 3.711861, 3.260457, 2.493755, 2.267661, 2.994923, 2.447978, 2.093928, 2.379100, 2.836308, 2.904491,
2.084674, 2.050629, 2.370026, 2.877150, 3.372492, 3.679573, 3.158224, 3.345067, 3.600110, 3.381230, 4.116003,
3.785123, 4.519719, 3.966509, 3.808330, 4.551462, 3.838009, 3.758539, 3.816730, 4.618030, 3.926753, 4.593788,
3.894390, 4.779126)
plot(t,y,col="orange",xlim=c(0,20),ylim=c(2,6),pch=19)
```

A first Fourier approximation would fit a model of the form 
$$
f_1(t) = c_0 + c_1 \sin(t) + c_2 \cos(t).
$$
Thus, we make the following matrix (we show here only the first 10 rows; there are 200 rows).
```{r}
A = cbind(t^0, sin(t),cos(t))
A[1:10,]
```

Now we solve the normal equations 
```{r}
(xhat = solve(t(A) %*% A, t(A) %*% y))
```

and plot the solution
```{r,echo=FALSE}
plot(t,y,col="orange",xlim=c(0,20),ylim=c(2,6),pch=19)
tt = seq(0,20,len=1000)
yy = xhat[1] + xhat[2]*sin(tt) + xhat[3]*cos(tt)
points(tt,yy,type='l',col="blue")
```
Your task:

a. Update this to add the second Fourier coefficient terms by fitting the following function to the data. Plot your result.
$$
f_2(t) = c_0 + c_1 \sin(t) + c_2 \cos(t) + c_3 \sin(2t) + c_4 \cos(2t)
$$
b. Compute the length of the residul vector for both the $f_1(t)$ and the $f_2(t)$ model. Which approximation looks better visually. That is, does the second approximation capture more of the shape of the data, or do you think that the first is a better model?


## Global Fossil Fuel Emissions


Below is a plot of global fossil fuel emmissions between 1751 and 1998 measured in megatons of carbon. 

```{r sqemission, echo=TRUE}
year=c(1751:1998)
emissions = c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 10, 9, 9, 9, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12, 13, 14, 14, 14, 14, 14, 15, 16, 16, 17, 17, 18, 18, 18, 24, 23, 23, 24, 24, 25, 29, 29, 30, 31, 33, 34, 36, 37, 39, 43, 43, 46, 47, 50, 54, 54, 57, 59, 69, 71, 76, 77, 78, 83, 91, 95, 97, 104, 112, 119, 122, 130, 135,
142, 147, 156, 173, 184, 174, 188, 191, 194, 196, 210, 236, 243, 256, 272, 275, 277, 281, 295, 327, 327, 356, 372, 374, 370, 383, 406, 419, 440, 465, 507, 534, 552, 566, 617, 624, 663, 707, 784, 750, 785, 819, 836, 879, 943, 850, 838, 901, 955, 936, 806, 932, 803, 845, 970, 963, 975, 983, 1062, 1065, 1145, 1053, 940, 847, 893, 973, 1027, 1130, 1209, 1142, 1192, 1299, 1334, 1342, 1391, 1383, 1160, 1238, 1392, 1469, 1419, 1630, 1767, 1795, 1841, 1865, 2043, 2177, 2270, 2330, 2463, 2578, 2595, 2701, 2848, 3009, 3146, 3306, 3412, 3588, 3802, 4075, 4227, 4394, 4633, 4641, 4613, 4879, 5018, 5078, 5368, 5297, 5125, 5080, 5067, 5241, 5405, 5573, 5701, 5926, 6035, 6096, 6186, 6089, 6090, 6236, 6378, 6530, 6628, 6608)
plot(year,emissions,pch=20,cex=.7,col="red")
```

The data suggest that the fossil fuel emissions $f$ follow an exponential model with respect to the year $y$:
$$f = a e^{k(y-1750)},$$
where $a$ and $k$ are the unknown constants. This model is not linear in the unknowns $a$ and $k$, but (this is a great idea!) it becomes linear if we take the logarithm of both sides. Doing so yields the following linear system:
$$\log(f)=\log(a)+k(y-1750).$$

* **Note:** This process works for any logarithm, but it is common to use the natural logarithm (use `log()` in R).
* **Note:** To simplify even further,  we will define `time=year-1750` so that time represents years after 1750. 

This results in the model

$$ \log(f)=d+kt,$$
where $d=\log(a)$ and $t$ is time (since 1750), 

Your task: Use least-squares projection to find the best fitting exponential function for this data. This will give you the values for $d$ and $k$, and once you know $d$, you can find $a = \exp(d)$. We have started the code for you by defining `x=year-1750` and `y=log(emissions)`.

```{r, echo=TRUE}
### your code goes here. 
# be sure to define d and k and A
t=year-1750
y=log(emissions)
A=cbind(1)  # this isn't right, yet!
d=1
k=0
#####
# your code above has found b and k
(a = exp(d))
k
```


Run the code below to plot the original data along with your exponential model curve $f(t)$. **Note:** This code assumes that you have already defined the values for `k` and `a`. Otherwise, it will not work!


```{r, echo=TRUE}
  f=function(y){a * exp(k*(y-1750))}
  plot(year,f(year),type="l",lwd=3,ylim=c(0,10000),ylab="emissions", main="best fit exponential function")
  points(year,emissions,pch=20,cex=.7,col="red")
```

<!--chapter:end:PS8.Rmd-->

# (PART) Quiz Review {-}

# Quiz 1 Review


## Overview

Our first quiz covers sections 1.1 - 1.8 and part of 1.9 in Lay's book. This corresponds to Problem Sets 1 and 2. In 1.8 and 1.9, it will cover the definition of a linear transformation as well as one-to-one and onto, but it will not cover the matrix of a linear transformation.


### Vocabulary and Concepts

You should understand these concepts and be able to read and use these terms correctly:

* elementary row operations 
* REF and RREF
* pivot position
* linear combination
* span
* linear independence
* homogeneous and nonhomogeneous equations
* Understand the geometric relationship between the solutions to $Ax = 0$ and $Ax=b$
* Understand Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an $m \times n$ matrix $A$
    + For each $b$ in $\mathbb{R}^m$, $A x = b$ has a solution
    + Each $b$ in $\mathbb{R}^m$ is a linear combination of the columns of $A$
    + The columns of $A$ span $\mathbb{R}^m$
    + $A$ has a pivot in every row.
* The columns of $A$ are linearly independent if and only if $Ax=0$ only has the trivial solution
* Understand Theorem 8 in Section 1.7: if you have more than $n$ vectors in $\mathbb{R}^n$ they must be linearly dependent.
* linear transformation
* one-to-one and onto
* Understand Theorem 12 in Section 1.9 which states that 
    + $T(x)=Ax$ is onto if and only if the column of $A$ span $\mathbb{R}^m$
    + $T(x)=Ax$ is one-to-one if and only if the columns of $A$ are linearly independent.


### Skills

You should be able to perform these linear algebra tasks.

* Identify linear systems from nonlinear systems
* Make the augmented matrix from a set of equations
* Row reduce a system of equations into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF)
* Write the solution set to $Ax=b$ as a parametric vector equation.
* Convert back and forth between systems of equations, vector equations, and matrix equations.
* Compute the matrix-vector product $Ax$
* Determine whether a set of vectors is linearly dependent or independent
* Find a dependence relation among a set of vectors
* Decide if a set of vectors span $\mathbb{R}^n$
* Manipulate matrix vector products using: $A(x + y) = Ax + Ay$ and $A(c x) = c A x$
* Determine whether a linear transformation is one-to-one and/or onto


## Practice Problems


###

I have performed some row operations below for you on a matrix $A$. Write out the complete set of solutions to $A \mathsf{x} = {\bf 0}$.
$$
A=
\begin{bmatrix}
1& 2& 0& 2& 0& -1 \\
1& 2& 1& 1& 0& -2 \\
2& 4& -2& 6& 1& 2 \\
1& 2&  0& 2& -1& -3 \\
\end{bmatrix} \longrightarrow
\begin{bmatrix}
1& 2& 0& 2& 0& -1\\
0& 0& 1& -1& 0& -1\\
0& 0& 0& 0& 1& 2\\
0& 0& 0& 0& 0& 0\\
\end{bmatrix}
$$   

###

I have performed some row operations below for you on a matrix $B$.
$$
B=
\begin{bmatrix}
1& 1& 0 \\ 0& 1& 1 \\ 2& 1& 2 \\ 1& -1& 1 \\ 2& 3& 1 \\
\end{bmatrix}
\longrightarrow
\begin{bmatrix}
1& 0& 0 \\ 0& 1& 0 \\ 0& 0& 1 \\ 0&0&0 \\ 0&0&0 \\
\end{bmatrix} 
$$
a. Describe the solutions to the equation $B \mathsf{x} = {\bf 0}$.

b. Fill in the boxes:   the transformation $T(\mathsf{x}) = B\mathsf{x}$ is a linear transformation from $\mathbb{R}^{\square}$ to $\mathbb{R}^{\square}$.

###

I want to know if it is possible to write $\mathsf{w}$ as a linear combination of the vectors $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3$ below. Write down,
**but do not solve**, a matrix equation that would solve this problem.  Your answer should be of the form $A \mathsf{x} = \mathsf{b}$, where I can clearly see what $A, \mathsf{x}$, and $\mathsf{b}$ are. I should also be able to tell how many unknowns there are.
$$
\mathsf{v}_1 = \left[ \begin{matrix} 1 \\  2 \\ 3 \\ 4 \\  \end{matrix}\right] ,  \quad
\mathsf{v}_2 = \left[ \begin{matrix} 1 \\  0 \\ 1 \\ 0 \\  \end{matrix}\right] ,  \quad
\mathsf{v}_3 = \left[ \begin{matrix} 1 \\  1 \\ 0 \\ -2 \\  \end{matrix}\right] ,  \quad
\mathsf{w} = \left[ \begin{matrix} 1 \\  -8 \\ -11 \\ -24 \\  \end{matrix}\right] . 
$$

###

Describe all vectors that are not in the span of the columns of the matrix $A$ below:
$$
A=
\begin{bmatrix}
1& 2& 4  \\
-3& -5& -11\\
1& 1& 3 \\
\end{bmatrix}
$$

###

The matrix below is $3 \times 3$ but the third column is missing.    Add a nonzero third column so that the columns of  $A$ are linearly dependent and add a 3rd column so that the columns of $A$ are linearly independent. Briefly describe your strategy.
$$
\begin{bmatrix}
1& 0 & \quad \\
0& 1& \quad \\
2& 2& \quad \\
\end{bmatrix}  \qquad\qquad 
\begin{bmatrix}
1& 0 & \quad \\
0& 1& \quad \\
2& 2& \quad \\
\end{bmatrix}
$$

###

In each case below, find the **matrix** of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is **not linear**.  

a. The transformation $T$ is given by:
$$T \left( \begin{bmatrix}  x_1 \\ x_2 \\ \end{bmatrix}\right) = 
\begin{bmatrix} x_1 + x_2 \\ 2 x_1 \\ -x_2 \\\end{bmatrix}.
$$

b. The transformation $T$ is given by:
$$T \left( \begin{bmatrix}  x_1 \\ x_2 \\ x_3 \end{bmatrix} \right)= 
  \begin{bmatrix} x_1 + x_2 + x_3 \\  x_1 x_2 \\ -x_2 + 2 x_3 \end{bmatrix}.
$$

   c. The transformation $L: \mathbb{R}^2 \to \mathbb{R}^2$ sends the shaded region on the  left to the the shaded region on the right such that $A$ maps to $A$, $B$ maps to $B$, $C$ maps to $C$, and $D$ maps to $D$.

![](images/q1-blockA.png){width=30%} $\qquad \qquad$ ![](images/q1-blockB.png){width=30%} 

d. The transformation $R: \mathbb{R}^2 \to \mathbb{R}^2$ sends the shaded region on the  left to the the shaded region on the right such that $A$ maps to $A$, $B$ maps to $B$, $C$ maps to $C$, and $D$ maps to $D$.

![](images/q1-blockA.png){width=30%} $\qquad \qquad$ ![](images/q1-blockD.png){width=30%} 



###

Write the following systems of equations in vector and matrix form.

$$
\begin{array} {ccccccccccc}
5 x_1 &+& 3 x_2 &+& x_3 &+& 11 x_4 &-&  x_5 &=& 10 \\
4 x_1 &+& x_2 &+& 3 x_3 &+& 2 x_4 &+& 6 x_5 &=& 11 \\
- x_1 &+& 3 x_2 &-& 2 x_3 &+&  x_4 &+& 6 x_5 &=& 12 \\
\end{array} 
$$


###

Let $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4$ be the vectors in the  columns of the matrix $A$ below.
$$
A = \left[
\begin{array}{cccc}
 1 & 1 & 3 & 1 \\
 2 & 0 & 2 & 3 \\
 1 & 1 & 3 & 1 \\
 -1 & 0 & -1 & 0
\end{array}
\right]
\longrightarrow
\left[
\begin{array}{cccc}
 1 & 0 & 1 & 0 \\
 0 & 1 & 2 & 0 \\
 0 & 0 & 0 & 1 \\
 0 & 0 & 0 & 0
\end{array}
\right]
$$
a. Are the vectors  $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4$ linear independent or dependent? If they are linearly dependent, please give a dependence relation among them. 
b. Describe the span of the vectors $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_4$ inside of $\mathbb{R}^4$?


###

Find a solution to $A \mathsf{x}=0$ that no one else in the class has.
$$
A = 
\begin{bmatrix}
 1 & 1 & 1 & 1 & 4 \\
 2 & 0 & 4 & 1 & 4 \\
 1 & 1 & 1 & 1 & 4 \\
 1 & 0 & 2 & 1 & 3
\end{bmatrix}
 \longrightarrow 
\begin{bmatrix}
 1 & 0 & 2 & 0 & 1 \\
 0 & 1 & -1 & 0 & 1 \\
 0 & 0 & 0 & 1 & 2 \\
 0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$


## Solutions to Practice Problems

###

The parametric vector form of the solution is 

$$\begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6 \\
\end{bmatrix} = s \begin{bmatrix}
-2 \\ 1 \\ 0 \\ 0 \\0  \\ 0
\end{bmatrix}
+ t \begin{bmatrix}
-2 \\ 0 \\ 1 \\ 1 \\0  \\ 0
\end{bmatrix}
u \begin{bmatrix}
1 \\ 0 \\ 1 \\ 0 \\-2  \\ 1
\end{bmatrix}$$

###


a. There is one solution: $\mathsf{x} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$.
b. The transformation $T(\mathsf{x}) = B\mathsf{x}$ is a linear transformation from $\mathbb{R}^{3}$ to $\mathbb{R}^{5}$.
  
###

$$
\begin{bmatrix}
1 & 1 & 1 \\
2 & 0 & 1 \\
3 & 1 & 0 \\
4 & 0 & -2
\end{bmatrix}
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}
=
\begin{bmatrix}
1 \\ -8 \\ -11 \\ -24
\end{bmatrix}
$$

###

 We want to find all target vectors $\mathsf{b}$ such that $A \mathsf{x} = \mathsf{b}$ is inconsistent. So we want the augmented matrix $\begin{bmatrix} A \,| \, b \end{bmatrix}$ to have a pivot in the last column.
$$
\left[ \begin{array}{ccc|c}
1& 2& 4 & b_1 \\
-3& -5& -11 & b_2\\
1& 1& 3 & b_3 \\
\end{array} \right]
\longrightarrow
\left[ \begin{array}{ccc|c}
1& 2& 4 & b_1 \\
0& 1& 1 & 3b_1 +b_2\\
0& -1& -1 & -b_1+b_3 \\
\end{array} \right]
\longrightarrow
\left[ \begin{array}{ccc|c}
1& 2& 4 & b_1 \\
0& 1& 1 & 3b_1 +b_2\\
0& 0& 0 & 2b_1+b_2+b_3 \\
\end{array} \right]
$$
So the set of target vectors that are not in the span of the columns of $A$ are the vectors
$$
\begin{bmatrix}
b_1 \\ b_2 \\ b_3
\end{bmatrix}
\qquad \mbox{where} \qquad
2b_1 + b_2 + b_3 \neq 0.
$$

###

This is on PS2.



###
 
a. This is a linear transformation with 
$$A = \begin{bmatrix} 1 & 1 \\ 2 & 0 \\ 0 & -1 \end{bmatrix}.$$
b. This is not a linear transformation because
$$
2 \, T \left( \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \right)= 2 \begin{bmatrix} 3 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 6 \\ 2 \\ 2 \end{bmatrix} \quad \mbox{while} \quad 
T \left( \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix} \right)= 2 \begin{bmatrix} 6 \\ 4 \\ 2 \end{bmatrix}.
$$
c. $A= \begin{bmatrix} 1/2 & 1/2 \\ 1/2 & 1/2 \end{bmatrix}$

d. $A= \begin{bmatrix} 0 & -1 \\ -1 & 0 \end{bmatrix}$

###

 Vector Form:
$$
x_1 \begin{bmatrix} 5 \\ 4 \\ -1 \end{bmatrix} + 
x_2 \begin{bmatrix} 3 \\ 1 \\ 3 \end{bmatrix} + 
x_3 \begin{bmatrix} 1 \\ 3 \\ -2 \end{bmatrix} + 
x_4 \begin{bmatrix} 11 \\ 2 \\ 1 \end{bmatrix} + 
x_5 \begin{bmatrix} -1 \\ 6 \\ 6 \end{bmatrix} =
 \begin{bmatrix} 10 \\ 11 \\ 12 \end{bmatrix} 
$$
Matrix Form:
$$
\begin{bmatrix} 
5 & 3 & 1 & 11 & -1 \\
4 & 1 & 3 & 2 & 6 \\
-1 & 3 & -2& 1 & 6
\end{bmatrix} 
 \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \end{bmatrix} 
 =
 \begin{bmatrix} 10 \\ 11 \\ 12 \end{bmatrix} 
$$

###

a. $-\mathsf{v}_1 - 2\mathsf{v}_2 + \mathsf{v}_3 + 0 \mathsf{v}_4 = 0$.
b. $\mathrm{span}(\mathsf{v}_1,\mathsf{v}_2,\mathsf{v}_3,\mathsf{v}_4)$ looks like a copy of $\mathbb{R}^3$ sitting inside $\mathbb{R}^4$. In other words, is 3-dimensional subset of $\mathbb{R}^4$. 

###

 The general solution is
$$
\begin{bmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
\end{bmatrix}
= s \begin{bmatrix} -2 \\ 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}
+ t \begin{bmatrix} -1 \\ -1 \\ 0 \\ -2 \\ 1 \end{bmatrix}.
$$
My solution is
$$77,083,679 
\begin{bmatrix}
-2 \\ 1 \\ 1 \\ 0 \\ 0
\end{bmatrix}
- 72,159,215 
\begin{bmatrix}
-1 \\ -1 \\ 0 \\ -2 \\ 1
\end{bmatrix}.
$$


<!--chapter:end:q1-review.Rmd-->



# Quiz 2 Review




## Overview

Our second quiz covers the following sections:

* Linear Transformations
    + 2.1: Matrix Multiplication and the Matrix of a Linear Transformation
    + 2.2: Matrix Inverses
    + 2.3: [The Invertible Matrix Theorem](https://moodle.macalester.edu/pluginfile.php/59258/mod_resource/content/1/IVT.pdf)

* Subspaces
    + 4.1: Subspaces of $\mathbb{R}^n$
    + 4.2: Null Space and Column Space
    + 4.3: Bases
    + 4.4: Coordinates 

This corresponds to Problem Sets 3 and 4.

The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice:

* Make sure that you have mastered the Vocabulary, Skills and Concepts  listed below.
* Look over the Edfinity homework assingments
* Do practice problems from the Edfinity Practice assignments. These allow you to "Practice Similar" by generating new variations of the same problem.
* Redo the Jamboard problems
* Try to resolve the Problem Sets and compare your answers to the solutions.
* Do the practice problems below. Compare your answers to the solutions.

### Vocabulary and Concepts



You should understand these concepts and be able to read and use these terms correctly:

* all of the [Important Definitions found here][Important Definitions].
* matrix multiplication
* matrix inverses
* the Invertible Matrix Theorem
* subspaces
* null space and column space of a matrix
* kernel and image of a linear transformation
* basis (span and linearly independent)
* coordinate vector with respect to a basis $\mathcal{B}$
* change-of-coordinates matrix
* dimension



### Skills

You should be able to perform these linear algebra tasks.

* solve matrix algebra equations
* find a matrix inverse
* show that a subset is a subspace or demonstrate that it is not a subspace
* describe the null space and the column space
* determine if a vector is in a null space or column space
* find a basis of a subspace
* answer questions about the connections between all these ideas
* write short proofs of basic statements using the [Important Definitions][Important Definitions]


## Practice Problems

###

In each case below, find the **matrix** of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is **not linear**.  

a. The transformation $T$ is given by:
$$T \left( \begin{bmatrix}  x_1 \\ x_2 \\ \end{bmatrix}\right) = 
\begin{bmatrix} x_1 + x_2 \\ 2 x_1 \\ -x_2 \\\end{bmatrix}.
$$

b. The transformation $T$ is given by:
$$T \left( \begin{bmatrix}  x_1 \\ x_2 \\ x_3 \end{bmatrix} \right)= 
  \begin{bmatrix} x_1 + x_2 + x_3 \\  x_1 x_2 \\ -x_2 + 2 x_3 \end{bmatrix}.
$$

c. The transformation $L: \mathbb{R}^2 \to \mathbb{R}^2$ sends the shaded region on the  left to the the shaded region on the right such that $A$ maps to $A$, $B$ maps to $B$, $C$ maps to $C$, and $D$ maps to $D$.

![](images/q1-blockA.png){width=30%} $\qquad \qquad$ ![](images/q1-blockB.png){width=30%} 

d. The transformation $R: \mathbb{R}^2 \to \mathbb{R}^2$ sends the shaded region on the  left to the the shaded region on the right such that $A$ maps to $A$, $B$ maps to $B$, $C$ maps to $C$, and $D$ maps to $D$.

![](images/q1-blockA.png){width=30%} $\qquad \qquad$ ![](images/q1-blockD.png){width=30%}


<!-- ### -->

<!-- Here is a picture of some boats. -->

<!-- <center> -->
<!-- ![](images/q2r-boat.png){width=60%} -->
<!-- </center> -->

<!-- The blue, yellow and gray boats are linear transformations of the red boat (using homogeneous coordinates). Find the $3 \times 3$ matrix corresponding to the linear transformation that creates: -->

<!-- a. The shadow gray boat -->
<!-- a. The fast blue boat -->
<!-- a. The funny yellow boat. -->

<!-- For your convenience, here is the code to draw the red boat as well as some commented code that you can adapt to create the other boats. -->


<!-- ``` -->
<!-- '```{r,fig.height=4,fig.width=4} -->

<!-- # the red boat -->
<!-- boat =cbind(c(0,0), c(-6,0), c(-7,3), c(-1,3), c(-1,5), c(-6,5), c(-1,10), c(-1,11), -->
<!--             c(-.5,11), c(-.5,3), c(2,3) ) -->
<!-- boat = rbind(boat,rep(1,11)) -->

<!-- ##### update these matrices -->
<!-- graymap = cbind(c(1,0,0), c(0,1,0),c(0,0,1)) -->
<!-- bluemap = cbind(c(1,0,0), c(0,1,0),c(0,0,1)) -->
<!-- yellowmap = cbind(c(1,0,0), c(0,1,0),c(0,0,1)) -->


<!-- # plot all of the boats -->
<!-- grayboat = graymap %*%  boat -->
<!-- blueboat = bluemap %*%  boat -->
<!-- yellowboat = yellowmap %*%  boat -->

<!-- plot(boat[1,],boat[2,],type="n",xlim=c(-16,16),ylim=c(-16,16),xlab="x",ylab="y") -->
<!-- abline(h=-16:16, v=-16:16, col="gray", lty="dotted") -->

<!-- polygon(grayboat[1,], grayboat[2,], col = "gray", border = "gray") -->
<!-- polygon(blueboat[1,], blueboat[2,], col = "blue", border = "gray") -->
<!-- polygon(yellowboat[1,], yellowboat[2,], col = "yellow", border = "gray") -->
<!-- polygon(boat[1,], boat[2,], col = "red", border = "blue") -->

<!-- ``` -->





###

Find the inverse of the matrix
$$
\left[
\begin{array}{rrr}
1 & -2 & 2  \\
1 & 0 & 0 \\
2 &-4 & 5
\end{array}
\right]
$$



###

Suppose that a linear transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ has the property that 
$T(\mathsf{u}) = T(\mathsf{v})$ for some pair of distinct vectors $\mathsf{u}, \mathsf{v} \in \mathbb{R}^n$. Can $T$ map $\mathbb{R}^n$ onto $\mathbb{R}^n$? Why or why not?



###

Let $U$ and $W$ be subspaces of a vector space $\mathbb{R}^n$. 
Prove or disprove the following statements. Prove them by showing that the conditions are being a subspace are satisfied. Disprove them with a specific counter example.

a. $U \cap W = \{ \mathsf{v} \in \mathbb{R}^n \mid \mathsf{v} \in U \mbox{ and } \mathsf{v} \in W \}$ is a subspace

b. $U \cup W = \{ \mathsf{v} \in \mathbb{R}^n \mid \mathsf{v} \in U \mbox{ or } \mathsf{v} \in W \}$ is a subspace

c. $U+W = \{\mathsf{u} + \mathsf{w} \mid \mathsf{u} \in U \mbox{ and } \mathsf{w} \in W \}$ is a subspace





###

Let $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a linear transformation. 

a. Suppose that $T: \mathbb{R}^n \to \mathbb{R}^m$ is one-to-one. Prove that if $\mathsf{v}_1,  \mathsf{v}_2,  \mathsf{v}_3$ are linearly independent, then $T(\mathsf{v}_1), T(\mathsf{v}_2), T(\mathsf{v}_3)$ are linearly independent.

b. Suppose that $T: \mathbb{R}^n \to \mathbb{R}^m$ is onto. Prove that if $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3$ span $\mathbb{R}^n$ then $T(\mathsf{v}_1), T(\mathsf{v}_2), T(\mathsf{v}_3)$ span $\mathbb{R}^m$.


###

I have performed some row operations below for you on a matrix $A$. Find a basis for the column space and the null space of $A$. 
$$
A=
\left[
\begin{matrix}
1& 2& 0& 2& 0& -1 \\
1& 2& 1& 1& 0& -2 \\
2& 4& -2& 6& 1& 2 \\
1& 2&  0& 2& -1& -3 \\
\end{matrix}\right] \longrightarrow
\left[
\begin{matrix}
1& 2& 0& 2& 0& -1\\
0& 0& 1& -1& 0& -1\\
0& 0& 0& 0& 1& 2\\
0& 0&   0& 0& 0& 0\\
\end{matrix}\right] 
$$   






###

Consider the matrix 
\[
A = \left[
\begin{array}{cccc}
1 & 5 & 2 & -4 \\
3 & 10 & 2 & 8 \\
4 & 15 & 4 & 4 
\end{array}
\right]
\]
Find a basis for $\mathrm{Col}(A)$. Find a basis for $\mathrm{Nul}(A)$.




###

Are the vectors in ${\mathcal B}$ a basis of $\mathbb{R}^3$? If not, find a basis of $\mathbb{R}^3$ that consists of as many of the vectors from ${\mathcal B}$ as is possible. Explain your reasoning.
$$
\mathcal{B}=\left\{ \begin{bmatrix} 1 \\ -1 \\ -2 \end{bmatrix},\begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix},\begin{bmatrix} -1 \\ -1 \\ -8 \end{bmatrix}
\right\}
$$


<!-- ### -->

<!-- I have the vectors below:  -->
<!-- $$ -->
<!-- \begin{bmatrix} 5\\ 4\\ 3\\ 1\\ 2 \end{bmatrix}, \begin{bmatrix} 4\\ 4\\ 3\\ 1\\ 2 \end{bmatrix}, \begin{bmatrix} 1\\ 1\\ 1\\ 1\\ 1\end{bmatrix}.  -->
<!-- $$ -->
<!-- I know that they do not span $\mathbb{R}^5$, but  I want to extend them to a basis of $\mathbb{R}^5$ by adding some vectors to the set. I created the  matrix below and row reduced. Give a basis for $\mathbb{R}^5$ that uses my three vectors and explain why this method works in general. -->
<!-- $$ -->
<!-- \left[ -->
<!-- \begin{array}{cccccccc} -->
<!--  5 & 4 & 1 & 1 & 0 & 0 & 0 & 0 \\ -->
<!--  4 & 4 & 1 & 0 & 1 & 0 & 0 & 0 \\ -->
<!--  3 & 3 & 1 & 0 & 0 & 1 & 0 & 0 \\ -->
<!--  1 & 1 & 1 & 0 & 0 & 0 & 1 & 0 \\ -->
<!--  2 & 2 & 1 & 0 & 0 & 0 & 0 & 1 \\ -->
<!-- \end{array} -->
<!-- \right] \rightarrow \left[ -->
<!-- \begin{array}{cccccccc} -->
<!--  1 & 0 & 0 & 1 & 0 & 0 & 2 & -3 \\ -->
<!--  0 & 1 & 0 & -1 & 0 & 0 & -3 & 4 \\ -->
<!--  0 & 0 & 1 & 0 & 0 & 0 & 2 & -1 \\ -->
<!--  0 & 0 & 0 & 0 & 1 & 0 & 2 & -3 \\ -->
<!--  0 & 0 & 0 & 0 & 0 & 1 & 1 & -2 \\ -->
<!-- \end{array} -->
<!-- \right] -->
<!-- $$ -->


###

Find the coordinates of $\mathsf{w}$ in the standard basis and of $\mathsf{v}$ in the $\mathcal{B}$-basis.
$$
\mathcal{B} = \left\{
\mathsf{v}_1=\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix},
\mathsf{v}_2=\begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix},
\mathsf{v}_3=\begin{bmatrix} 1 \\ 1 \\ 1 \\ 0 \end{bmatrix},
\mathsf{v}_4=\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}
\right\},
$$
$$
\mathsf{w} = 
\begin{bmatrix} 3 \\ -2 \\ 0 \\ -1 \end{bmatrix}_{\mathcal{B}},
\qquad
\mathsf{v} = 
\begin{bmatrix} 10 \\ 9 \\ 7 \\ 4 \end{bmatrix}_{\mathcal{S}}
$$


###

The subspace $S \subset \mathbb{R}^5$ is given by 
$$ \mathsf{S} = \mathsf{span}
\left(
\begin{bmatrix}1\\ 1\\ 0\\ -1\\ 2 \end{bmatrix},
\begin{bmatrix} 0\\ 1\\ 1\\  1\\ 1 \end{bmatrix},
\begin{bmatrix} 3\\ 1\\ -2\\ -5\\ 4 \end{bmatrix},
\begin{bmatrix} 1\\ 0\\ 1\\ 0\\ 1 \end{bmatrix},
\begin{bmatrix} 2\\ -1\\ -1\\ -3\\ 1 \end{bmatrix},
\right)$$

a. Use the following matrix to find a basis for $S$. What is the dimension of  $S$?
$$
A=\left[
\begin{array}{ccccc}
 1 & 0 & 3 & 1 & 2 \\
 1 & 1 & 1 & 0 & -1 \\
 0 & 1 & -2 & 1 & -1 \\
 -1 & 1 & -5 & 0 & -3 \\
 2 & 1 & 4 & 1 & 1 \\
\end{array}
\right] \rightarrow
\left[
\begin{array}{ccccc}
 1 & 0 & 3 & 0 & 1 \\
 0 & 1 & -2 & 0 & -2 \\
 0 & 0 & 0 & 1 & 1 \\
 0 & 0 & 0 & 0 & 0 \\
 0 & 0 & 0 & 0 & 0 \\
\end{array}
\right]
$$

a. Find a basis for $\mathrm{Nul}(A)$. What is the dimension of this nullspace?


###

A $6 \times 8$ matrix $A$ contains 5 pivots. For each of  $\mathrm{Col}(A)$ and $\mathrm{Nul}(A)$,

* Determine the dimension of the subspace,
* Indicate whether it is  subspace of $\mathbb{R}^6$ or $\mathbb{R}^8$, and 
* Decide how you would find a basis of the subspace.  








## Solutions to Practice Problems

###
 
a. This is a linear transformation with 
$$A = \begin{bmatrix} 1 & 1 \\ 2 & 0 \\ 0 & -1 \end{bmatrix}.$$
b. This is not a linear transformation because, for example,
$$
2 \, T \left( \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} \right)= 2 \begin{bmatrix} 3 \\ 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 6 \\ 2 \\ 2 \end{bmatrix} \quad \mbox{while} \quad 
T \left( \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix} \right)= 2 \begin{bmatrix} 6 \\ 4 \\ 2 \end{bmatrix}.
$$
c. $A= \begin{bmatrix} 1/2 & 1/2 \\ 1/2 & 1/2 \end{bmatrix}$

d. $A= \begin{bmatrix} 0 & -1 \\ -1 & 0 \end{bmatrix}$


<!-- ### -->

<!-- Here  are the matrices  to make the boats. -->

<!-- a.  Shadow gray  boat: -->
<!-- $$  -->
<!-- \begin{bmatrix} 1 & 0  & 0 \\ 0 & -1 & 0 \\  0 & 0 & 1 \end{bmatrix} -->
<!-- $$   -->
<!-- a. Fast blue boat: -->
<!-- $$  -->
<!-- \begin{bmatrix} 1 & -0.25  & 13 \\ 0 & 1 & 0 \\  0 & 0 & 1 \end{bmatrix} -->
<!-- $$ -->
<!-- a. Funny yellow boat: -->
<!-- $$  -->
<!-- \begin{bmatrix} 0.35 & -0.35  & -3 \\ 0.35 & 0.35 & 8 \\  0 & 0 & 1 \end{bmatrix} -->
<!-- $$ -->


<!-- And here is the updated code -->

<!-- ``` -->
<!-- '```{r,fig.height=4,fig.width=4,echo=TRUE} -->

<!-- # the red boat -->
<!-- boat =cbind(c(0,0), c(-6,0), c(-7,3), c(-1,3), c(-1,5), c(-6,5), c(-1,10), c(-1,11), -->
<!--             c(-.5,11), c(-.5,3), c(2,3) ) -->
<!-- boat = rbind(boat,rep(1,11)) -->


<!-- # updated mappings -->
<!-- graymap = cbind(c(1,0,0), c(0,-1,0),c(0,0,1)) -->
<!-- bluemap = cbind(c(1,0,0), c(0,1,0),c(13,0,1))  %*% cbind(c(1,0,0), c(-.25,1,0),c(0,0,1)) -->
<!-- t=pi/4 -->
<!-- yellowmap = cbind(c(1,0,0), c(0,1,0),c(-3,8,1)) %*% cbind(c(cos(t),sin(t),0), c(-sin(t),cos(t),0),c(0,0,1)) %*% cbind(c(1/2,0,0),c(0,1/2,0),c(0,0,1)) -->



<!-- # plot all of the boats -->
<!-- grayboat = graymap %*%  boat -->
<!-- blueboat = bluemap %*%  boat -->
<!-- yellowboat = yellowmap %*%  boat -->

<!-- plot(boat[1,],boat[2,],type="n",xlim=c(-16,16),ylim=c(-16,16),xlab="x",ylab="y") -->
<!-- abline(h=-16:16, v=-16:16, col="gray", lty="dotted") -->

<!-- polygon(grayboat[1,], grayboat[2,], col = "gray", border = "gray") -->
<!-- polygon(blueboat[1,], blueboat[2,], col = "blue", border = "gray") -->
<!-- polygon(yellowboat[1,], yellowboat[2,], col = "yellow", border = "gray") -->
<!-- polygon(boat[1,], boat[2,], col = "red", border = "blue") -->

<!-- ``` -->



###

The inverse is
$$
\begin{bmatrix}
0 & 1 & 0 \\ 
-5/2  & 1/2 &1 \\
-2 & 0 & 1
\end{bmatrix}
$$


```{r, echo=FALSE}
require(pracma)
```



###

No $T$ cannot be an onto mapping by the Invertible Matrix Theorem.  Since  $T$ is not  one-to-one,  the mapping cannot be onto.


###


a. **True**

+ Since $U$ and $W$ are subspaces, we know that $\mathbb{0} \in U$ and $\mathbb{0} \in W$. Therefore $\mathbb{0} \in U \cap W$.
+ Let $\mathsf{v}_1 \in U \cap W$ and $\mathsf{v}_2 \in U \cap W$. 
+ We know that $\mathsf{v}_1 \in U$ and $\mathsf{v}_2 \in U$. Since $U$ is a subspace, we have $\mathsf{v}_1 + \mathsf{v}_2 \in U$. 
+  We know that $\mathsf{v}_1 \in W$ and $\mathsf{v}_2 \in W$. Since $W$ is a subspace, we have $\mathsf{v}_1 + \mathsf{v}_2 \in W$. 
Therefore $\mathsf{v}_1 + \mathsf{v}_2 \in U \cap W$.

+  Let $\mathsf{v} \in U \cap W$ and $c \in \mathbb{R}$.
+ We know that $\mathsf{v} \in U$ and $c \in R$. Since $U$ is a subspace, we have $c \mathsf{v}  \in U$. 
+  We know that $\mathsf{v} \in W$ and $c \in R$. Since $W$ is a subspace, we have $c \mathsf{v}  \in W$. 
+ Therefore $c \mathsf{v} \in U \cap W$.



b.  **False.** Here is an example that shows this is not always true. 
Let $V= \mathbb{R}^2$, $U = \{ { x \choose 0} \mid x \in \mathbb{R} \}$ and $W= \{ { 0 \choose y} \mid y \in \mathbb{R} \}$. The set $U \cup W$ is not closed under addition. For example,
${1 \choose 0} + {0 \choose 1} = { 1 \choose 1} \notin U \cup W$.

c. **True.**

+  Since $U$ and $W$ are subspaces, we know that $\mathbb{0} \in U$ and $\mathbb{0} \in W$. Therefore $\mathbb{0}  = \mathbb{0} + \mathbb{0} \in U + W$.
+ Let $\mathsf{u}_1 + \mathsf{w}_1 \in U + W$ and $\mathsf{u}_1 + \mathsf{w}_2 \in U + W$, where $\mathsf{u}_1, \mathsf{u}_2 \in U$ and $\mathsf{w}_1, \mathsf{w}_2 \in W$. Then
$$
(\mathsf{u}_1 + \mathsf{w}_1) + (\mathsf{u}_2 + \mathsf{w}_2) = (\mathsf{u}_1 + \mathsf{u}_2) + (\mathsf{w}_1 + \mathsf{w}_2)
$$
and $\mathsf{u}_3 = (\mathsf{u}_1 + \mathsf{u}_2) \in U$ (because $U$ is a subspace) and $\mathsf{w}_3 = (\mathsf{w}_1 + \mathsf{w}_2) \in W$ (because $W$ is a subspace). 
+ Therefore $(\mathsf{u}_1 + \mathsf{v}_1) + (\mathsf{u}_2 + \mathsf{w}_2) = \mathsf{u}_3 + \mathsf{w}_3 \in U + W$.

+ Let $\mathsf{u} + \mathsf{w}  \in U + W$ and $c \in \mathbb{R}$. Then
$c(\mathsf{u} + \mathsf{w}) = c \mathsf{u} + c \mathsf{w}$. We know that $c \mathsf{u} \in U$ (since $U$ is a subspace) and $c \mathsf{w} \in W$ (since $W$ is a subspace). Therefore $c(\mathsf{u} + \mathsf{w}) = c \mathsf{u} + c \mathsf{w} \in U+W$.


###


a. Suppose that $c_1 T(\mathsf{v}_1) + c_2 T(\mathsf{v}_2) +  c_3 T(\mathsf{v}_3) = 0$. We must show that $c_1 = c_2 = c_3 = 0$. 

+ Since $T$ is a linear transformation, this means that  $T(c_1 \mathsf{v}_1+ c_2 \mathsf{v}_2 + c_3 \mathsf{v}_3 )= 0$.
+ Since $T$ is one-to-one and $T(\mathbf{0}) = \mathbf{0}$, we must have $c_1 \mathsf{v}_1+ c_2 \mathsf{v}_2 + c_3 \mathsf{v}_3 = \mathbf{0}$.
+ Because $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3$ are linearly independent, this means that $c_1 = c_2 = c_3 = 0$.
    
This proves that $T(\mathsf{v}_1), T(\mathsf{v}_2), T(\mathsf{v}_3)$ are linearly independent.

b. Given $\mathsf{w} \in W$. We must show that there exist constants $c_1, c_2, c_3$ such that
$\mathsf{w} =  c_1 T(\mathsf{v}_1) + c_2 T(\mathsf{v}_2) + c_3 T(\mathsf{v}_3)$. Here we go!

+ Since $T$ is onto, we know that there exists $\mathsf{v} \in \mathbb{R}^n$ such that $T(\mathsf{v}) = \mathsf{w}$.
+ Since $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3$ span $\mathbb{R}^n$, we know that there exist constants 
$c_1, c_2, c_3$ such that $\mathsf{v}=  c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + c_3 \mathsf{v}_k$
+  Therefore
$$
\mathsf{w} = T(\mathsf{v})=  T(c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2 + c_3 \mathsf{v}_k) = c_1 T(\mathsf{v}_1) + c_2 T(\mathsf{v}_2) + c_3 T(\mathsf{v}_k)
$$
because $T$ is a linear transformation.
This proves that $T(\mathsf{v}_1), T(\mathsf{v}_2), T(\mathsf{v}_3)$ span $W$.



###

A basis for $\mathrm{Col}(A)$ is
$$
\begin{bmatrix}
1 \\1  \\ 2 \\ 1 
\end{bmatrix}, \quad
\begin{bmatrix}
0 \\1  \\ -2 \\ 0 
\end{bmatrix}, \quad
\begin{bmatrix}
0 \\0  \\ 1 \\ -1 
\end{bmatrix}
$$
and a basis for $\mathrm{Nul}(A)$ is
$$
\begin{bmatrix}
-2 \\1  \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix}, \quad
\begin{bmatrix}
-2 \\0  \\ 1 \\ 1 \\ 0 \\ 0 
\end{bmatrix}, \quad
\begin{bmatrix}
1 \\0   \\ 1\\ 0 \\ -2 \\ 1 
\end{bmatrix}.
$$



###

Using RStudio we find:

```{r}
A =  cbind(c(1,3,4),c(5,10,15),c(2,2,4),c(-4,8,4))
A
rref(A)
```

A basis for $\mathrm{Col}(A)$ is
$$
\begin{bmatrix}
1 \\ 3 \\ 4 
\end{bmatrix}, \quad
\begin{bmatrix}
5 \\ 10 \\ 15
\end{bmatrix}.
$$


A basis for $\mathrm{Nul}(A)$ is
$$
\begin{bmatrix}
2 \\ -0.8 \\ 1 \\ 0 
\end{bmatrix}, \quad
\begin{bmatrix}
-16 \\ 4  \\ 0 \\ 1
\end{bmatrix}.
$$



###


```{r echo=TRUE}
A =  cbind(c(1,-1,-2),c(2,-1,1),c(-1,-1,-8))
A
rref(A)
```

No they are not a basis. The  corresponding matrix only has two pivots. Let's add the three elementary vectors to create matrix $B$ and then row reduce.

```{r echo=TRUE}
B =  cbind(A, c(1,0,0),c(0,1,0),c(0,0,1))
B
rref(B)
```

From this matrix, we can see  that the  vectors
$$
\begin{bmatrix}
1 \\ -1 \\ -2 
\end{bmatrix},
\quad
\begin{bmatrix}
2 \\ -1 \\ 1 
\end{bmatrix},
\quad
\begin{bmatrix}
1 \\ 0 \\ 0 
\end{bmatrix}.
$$
are  linearly independent because they correspond to the basis columns of $B$.



<!-- ### -->

<!-- A basis for $\mathbb{R}^5$ is -->
<!-- $$ -->
<!-- \begin{bmatrix} 5\\ 4\\ 3\\ 1\\ 2 \end{bmatrix}, \begin{bmatrix} 4\\ 4\\ 3\\ 1\\ 2 \end{bmatrix}, \begin{bmatrix} 1\\ 1\\ 1\\ 1\\ 1\end{bmatrix}, -->
<!-- \begin{bmatrix} 0\\ 1\\ 0\\ 0\\ 0\end{bmatrix}, \begin{bmatrix} 0\\ 0\\ 1\\ 0\\ 0 \end{bmatrix} -->
<!-- $$ -->

<!-- because these are basic columns in the given matrix. This  will always work: place your desired vectors in the first columns.  Since they are linearly independent,  they will be basic  columns. The remaining $n$ elementary basis vectors span  $\mathbb{R}. So the columns of the matrix span  $\mathbb{R}^n$ and Gaussian Elimination will identify $n$ pivots. The corresponding columns are a basis. -->


###

We use the change of basis matrix.
$$
P_{\cal  B} = 
\begin{bmatrix}
1 & 1  & 1 & 1 \\
0 & 1 & 1 & 1 \\
0  & 0 &  1 & 1 \\
0 & 0 & 0 & 1
\end{bmatrix}
$$
Then, the desired coordinate vectors are
$$
\mathsf{w} = 
\begin{bmatrix} 0 \\ -3 \\ -1 \\ -1 \end{bmatrix}_{\mathcal{S}},
\qquad
\mathsf{v} = 
\begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix}_{\mathcal{B}}
$$
You can find these vectors by multiplying by $P_\mathcal{B}$ and by augmenting and row reducing as seen here.
```{r, echo=TRUE}
A =  cbind(c(1,0,0,0),c(1,1,0,0),c(1,1,1,0),c(1,1,1,1))
w = c(3,-2,0,-1)
v = c(10,9,7,4)
A  %*% w
Av = cbind(A,v)
rref(Av)
```
Or we can use the inverse of $P_\mathcal{B}$.
$$
P_{\cal  B}^{-1} = 
\begin{bmatrix}
1 & -1  & 0 & 0 \\
0 & 1 & -1 & 0 \\
0  & 0 &  1 &-1 \\
0 & 0 & 0 & 1
\end{bmatrix}
$$


```{r, echo=TRUE}
Ainv = solve(A)
Ainv %*% v
```

###

a. $\dim(S) =  3$ and a basis for $S$  is
$$
\begin{bmatrix} 1 \\ 1 \\ 0 \\ -1 \\2  \end{bmatrix}, \quad
\begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \\ 1  \end{bmatrix}, \quad
\begin{bmatrix} 1 \\ 0 \\ 1 \\ 0 \\1  \end{bmatrix}.
$$

b. $\dim(\mathrm{Nul}(A))=2$ and a basis is
$$
\begin{bmatrix} -3 \\ 2 \\ 1 \\ 0 \\0\end{bmatrix}, \quad
\begin{bmatrix} -1 \\ 2 \\ 0 \\ -1 \\1  \end{bmatrix}.
$$


###
 
* $\mathrm{Col}(A)$ has dimension 5, and it is a subspace of  $\mathbb{R}^6$. You would  find a basis  by taking the pivot  columns of  $A$.
* $\mathrm{Nul}(A)$ has dimension 3, and it is a subspace of  $\mathbb{R}^8$. You would  find a basis  by finding the parametric solution to $A \mathsf{x}= \mathbb{0}$.



<!--chapter:end:q2-review.Rmd-->



# Quiz 3 Review


## Overview

Our third quiz covers sections 5.1-5.3  and 5.5-5.6 in Lay's book. This corresponds to Problem Set 6 and the corresponding Edfinity questions.

The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice:

* Make sure that you have mastered the Vocabulary, Skills and Concepts  listed below.
* Look over the Edfinity homework assingments
* Redo the Jamboard problems.
* Look at Jamboard problems that your group didn't get to.
* Try to resolve the Problem Sets and compare your answers to the solutions.
* Do the practice problems below. Compare your answers to the solutions.

### Vocabulary, Concepts and Skills

See the [Week 5-6 Learning Goals](week-5-6-learning-goals) for the list of vocabulary, concepts and skills.



## Practice Problems



###

Consider the $3 \times 3$ matrix
$$
A = 
\left[
\begin{array}{rrr}
2 & -1 & 0 \\
0 & 1 & 0 \\
-2 & 5 & -2 \\
\end{array}
\right]
$$
with characteristic equation 
$$
p(\lambda) = -(\lambda -1)(\lambda -2)(\lambda +2).
$$
Find the eigenvalues and corresponding eigenvectors for $A$.





###

Let $A$ be a $2 \times 2$ matrix. We view $A$ as a linear transformation from $\mathbb{R}^2$ to $\mathbb{R}^2$. Describe the eigenvalues for each of the following types of matrices.

a. $A$ maps all of $\mathbb{R}^2$  onto a line through the origin in $\mathbb{R}^2$.
a. $A$ is a reflection  of $\mathbb{R}^2$ about a line through the origin
a. $A$ is a reflection  of $\mathbb{R}^2$ through the origin
a.  $A$ is a horizontal shear





###

Below are the eigenvalues of four different $5 \times 5$ matrices.  For each, decide if the matrix is invertible  and if it is diagonalizable. 
Answer Yes, No or "Not enough information to determine this." 


a. $A$ has eigenvalues $\lambda =  -4, -3,0,1, 2$
a. $B$ has eigenvalues $\lambda = -3, -1, 1, \sqrt{2}, 8.$
a. $C$  has eigenvalues $\lambda =  1, 2, 2, 7, 8.$
a. $D$ has eigenvalues $\lambda =  -1, 0, 3,3, 10$



###

Here the diagonalization of a matrix:
 $$
 \mathsf{A}=\left[
\begin{array}{ccc}
 5 & 2 & -1 \\
 2 & 1 & 0 \\
 -1 & 0 & 1 \\
\end{array}
\right] = 
\left[
\begin{array}{ccc}
 -5 & 0 & 1 \\
 -2 & 1 & -2 \\
 1 & 2 & 1 \\
\end{array}
\right] \left[
\begin{array}{ccc}
 6 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 0 \\
\end{array}
\right]\left[
\begin{array}{ccc}
 -\frac{1}{6} & -\frac{1}{15} & \frac{1}{30} \\
 0 & \frac{1}{5} & \frac{2}{5} \\
 \frac{1}{6} & -\frac{1}{3} & \frac{1}{6} \\
\end{array}
\right].
$$

a. Is the matrix $\mathsf{A}$  invertible?
b. Find a nonzero vector in $\mathrm{Nul}(\mathsf{A})$ if one exists.
c. Find a steady-state vector $\mathsf{v}$ such that $\mathsf{A} \mathsf{v} = \mathsf{v}$ if one exists.
d. Give the coordinates of $\mathsf{v} = [1,2,3]^T$ in the eigenbasis without row reductions.
e. Find a formula for $\mathsf{A}^{2020} \mathsf{v}$ if $\mathsf{v} = [1,2,3]^T$ in terms of the eigenbasis.




###

The eigensystem of matrix  $A$ is given below.  It has complex eigenvalues. What angle  does it rotate by? What factor does it scale by?
$$
\begin{bmatrix} 3 & -5 \\ 1 & -1 \end{bmatrix}, \qquad
\lambda = 1 \pm i,
\qquad
v = \begin{bmatrix} 2 \\ 1 \end{bmatrix} \pm \begin{bmatrix} 1 \\ 0 \end{bmatrix} i.
$$



###

Using the matrix $B = = \begin{bmatrix} .97 & -.71 \\ .71 & .97 \end{bmatrix}$  and the starting vector $\mathsf{v} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$, I plotted the points 
$$\mathsf{v}, B \mathsf{v}, B^2\mathsf{v}, B^3 \mathsf{v}, \ldots.$$ 
I saw that these points are, roughly, going around in a circle. 

a. How many multiplications by $B$ does it take to get back around to the positive $x$-axis?   

a. When I come full circle, am I closer to the origin, farther from the origin, or the same distance to the origin?  

###

For each matrix below,  decide if it is diagonalizable.  You do  not need to diagonalize the matrix (though you can!), but you must give a reason for why the matrix is or is not diagonalizable.

a. $A = \begin{bmatrix} 0 & -4 & 2 \\ 2 & -4 & -1 \\ -6 & 4 & 7 \end{bmatrix}$ has eigenvalues $4, -1, 0$.

b. $B = \begin{bmatrix} 3 & -1 & 2 \\ -1 & 3 & 2 \\ 2&2 & 0 \end{bmatrix}$ has eigenvalues $4,4,-2$.

 
 
 
 

###

Consider the matrix with  eigenvalues and eigenvectors
$$
A = \begin{bmatrix} 0.7 & 0.2 \\ 0.3 & 0.8 \end{bmatrix}
\qquad
\begin{array}{cc}
\lambda_1 = 1  & \lambda_2 = .5 \\
\mathsf{v}_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix} & \mathsf{v}_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix} 
\end{array}
$$

a.  Diagonalize $A$.
a. What can you say about $\displaystyle{\lim_{n \to \infty}} A^n$?
a. Give a  formula for $A^n \mathsf{x}_0$ if $\mathsf{x}_0 = \begin{bmatrix} 25 \\ 0 \end{bmatrix}$ in terms of the eigenbasis.
a. What is $\displaystyle{\lim_{n \to \infty}} A^n \begin{bmatrix} 25 \\ 0 \end{bmatrix}$?

 

###

The matrix $A$ below has the given eigenvalues and eigenvectors. 
$$
A = \left[
\begin{array}{cc}
 \frac{1}{2} & \frac{1}{5} \\
 -\frac{2}{5} & \frac{9}{10} \\
\end{array}
\right]
\qquad
\begin{array}{c}
\lambda = .7  \pm .2 i \\
\mathsf{v} = \begin{bmatrix} \frac{1}{2} \\ 1 \end{bmatrix} \pm   \begin{bmatrix} -\frac{1}{2} \\ 0 \end{bmatrix} i
\end{array}\hskip5in
$$ 

a. Factor $A=PCP^{-1}$ where $C$ is a rotation-scaling matrix.
a. What is the angle of rotation?
a. What is the factor of dilation?

 
 


###

In a 1962 study of rainfall in Tel Aviv, it was determined that if today is a wet day, then the probability that tomorrow will be wet is 0.662 and the probability that tomorrow it will be dry is 0.338.  If today is a dry day, then the probability that tomorrow is wet is 0.250 and the probability that tomorrow is dry will be 0.75. From this I computed the following:
$$
A = \begin{bmatrix} 0.662 & 0.25 \\ 0.338 & 0.75\end{bmatrix};
\qquad
\begin{array}{cc}
 \lambda_1 = 1.0 & \lambda_2 = 0.412 \\
 \mathsf{v}_1 = \begin{bmatrix}-0.595 \\ -0.804 \end{bmatrix} & \quad \mathsf{v}_2 = \begin{bmatrix}-0.707\\ 0.707 \end{bmatrix}
 \end{array}
$$


a.  If Monday is a dry day, what is the probability that Wednesday will be wet?
a.  In the long-run, what is the distribution of wet and dry days?






###

A population of female bison is split into three groups: juveniles who are
less than one year old; yearlings between one and two years old; and adults who
are older than two years. Each year,
* 80% of the juveniles survive to become yearlings.
* 90% of the yearlings survive to become adults.
* 80% of the adults survive.
* 40% of the adults give birth to a juvenile
Let
$\mathsf{x}_t = \begin{bmatrix} J_t  \\ Y_t \\ A_t \end{bmatrix}$ be the
state of the system in year $t$.

a. Find the Leslie matrix $L$ such that $\mathsf{x}_{t+1} = B \mathsf{x}_t.$.
b. Find the eigenvalues of $L$.
c. The matrix  $L$ has two complex eigenvalues and  one real eigenvalue. How  do  the complex eigenvectors manifest in the trajectory of a population?
d. What is the long-term behavior of the herd? Will the size of the herd grow, stablilize  or shrink?  What will be the proportions of juveniles, yearlings and  adults in the herd?

###

Let $A$ and $B$ be $n \times n$ matrices. Suppose that $v$ is an eigenvector of $A$  with eigenvalue $\lambda$ and $v$ is an eigenvector of $B$ with eigenvalue $\mu$ such that $\lambda \not= \mu$. Is $v$ an eigenvector of either of the matrices below? If so give its eigenvalue.

a. $A + B$
b. $AB$


### 

Suppose that $A$ is invertible.

a. Show that if $v$ is an eigenvector of $A$ with eigenvalue $\lambda$, then $v$ is an eigenvector of $A^{-1}$ with eigenvalue $1/\lambda$.

b.  If $A$ is diagonalizable with diagonalization $A = P D P^{-1}$, then show that $A^{-1}$ is diagonalizable and find its diagonalization from that of $A$.

### 

Suppose that $A$ is an $n \times n$ matrix with eigenvector $\vec w$ of eigenvalue 5 and eigenvector $\vec v$ of eigenvalue -3. 

a. Is $\vec v + \vec w$ an eigenvector of $A$, and if so, what is its eigenvalue?

b. Is $2021 \vec v$ an eigenvector of $A$, and if so what is its eigenvalue?

c. Is $\vec w$ an eigenvector of $A^2$, and if so what is its eigenvalue?

d. Is $\vec v$ an eigenvector of $A - 2021 I_n$ and if so, what is its eigenvalue?
 
## Solutions to Practice Problems

```{r}
require(pracma)
```




###

There are three eigenvalues: 1, 2, and  $-2$. We find an eigenvector for each of  them.
* Eigenvalue  $\lambda = 1$
$$
A - I = 
\left[
\begin{array}{rrr}
1 & -1 & 0 \\
0 & 0 & 0 \\
-2 & 5 & -3 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
1 & -1 & 0 \\
0 & 3 & -3 \\
0 & 0 & 0 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
1 & -1 & 0 \\
0 & 1 & -1 \\
0 & 0 & 0 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
1 & 0 & -1 \\
0 & 1 & -1 \\
0 & 0 & 0 \\
\end{array}
\right]
$$
So one eigenvector is $[1,1,1]^{\top}$


* Eigenvalue $\lambda = 2$
$$
A - 2I = 
\left[
\begin{array}{rrr}
0 & -1 & 0 \\
0 & -1 & 0 \\
-2 & 5 & -4 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
-2 & 5 & -4 \\
0 & -1 & 0 \\
0 & 0 & 0 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
-2 & 0 & -4 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
1 & 0 & 2 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{array}
\right]
$$
So one eigenvector is $[-2,0,1]^{\top}$
  
* Eigenvalue $\lambda = -2$
$$
A - 2I = 
\left[
\begin{array}{rrr}
4 & -1 & 0 \\
0 & 3 & 0 \\
-2 & 5 & 0 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
4 & 0 & 0 \\
0 & 1 & 0 \\
-2 & 0 & 0 \\
\end{array}
\right]
\sim
\left[
\begin{array}{rrr}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 \\
\end{array}
\right]
$$
So one eigenvector is $[0,0,1]^{\top}$




###

In this problem, we are to think about the geometry of a 2D transformation, and see if we can find any vectors which get re-scaled by the transformation. The direction of these vectors cannot change (other than to flip to the opposite direction).

a. This maps all of $\mathbb{R}^2$ to a line. Therefore it is not one-to-one, nor onto, and so it is not invertible. This means that $\lambda = 0$ is an eigenvalue Any vector that is already on the line must stay on the line, so it is an eigenvector, but we don't know its eigenvalue. Thus, the eigenvalues are $\lambda_1 = 0$ and  $\lambda_2$ we don't know.


  
b. There are two kinds of eigenvectors. Those vectors on the line are fixed, so they are eigenvectors of eigenvalue 1. Vectors that are perpendicular to the line get sent to their negatives, so they are eigenvectors of eigenvalue $-1$. Thus, the eigenvalues are $\lambda_1 = 1$ and  $\lambda_2=-1$.


* Perhaps this one is easiest to understand by looking at a particular example. Let's look at a reflection across the $y$-axis. This means that $A [ x, y]^{\top} = [-x, y]^{\top}$. In particular, we have $A [ 1, 0]^{\top} =  [ -x, 0]^{\top} = - [ x, 0]^{\top}$. So $-1$ is an eigenvalue. Meanwhile, we also have $A [ 0, 1]^{\top} =  [ 0, 1]^{\top}$ So 1 is an eigenvalue.
  
c. In this transformation, every vector gets sent to its negative. 
$$
T\left( \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \right)
= \begin{bmatrix} -x_1 \\ -x_2 \end{bmatrix}
= \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
$$
This means that every vector is an eigenvector of eigenvalue $-1$. The eigenvalues are $\lambda_1 = \lambda_2=-1$.

d. A horizontal shear (we did not talk about these very much) has a matrix of the form
$$
\begin{bmatrix} 1 & a \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
= \begin{bmatrix} x + a y \\ y \end{bmatrix}
$$
It fixes the $x$-axis since $(x,0)^\top$ maps to $(x,0)^\top$, but no other directions are fixed. You can see by the fact that the matrix is upper triangluar that the eigenvalues are on the diagonal and are $\lambda_1 = \lambda_2 = 1$. Note: if you calculate, you find that the geometric multiplicity of $\lambda = 1$ is 1 (only the $x$-axis), and this matrix is not diagonalizable. The only eigenspace is the $x$-axis.



###
a. $A$ is not invertible because $0$ is an eigenvalue. $A$ is diagonalizable because it have 5 distinct eigenvalues.
a. $B$ is  invertible because $0$ is not an eigenvalue. $B$ is diagonalizable because it have 5 distinct eigenvalues.
a. $C$ is  invertible because $0$ is not an eigenvalue. We cannot tell whether $C$ is diagonalizable without more information. The eigenvalue $\lambda=2$ has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2.
a. $D$ is not invertible because $0$ is  an eigenvalue. We cannot tell whether $D$ is diagonalizable without more information. The eigenvalue $\lambda=3$ has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2.
  

###

a. No, $A$ is not invertible because $0$ is an eigenvalue.
b. $\mathsf{v} = [1, -2, 1]^{\top}$ is an eigenvector for $\lambda=0$. Therefore $\mathsf{v} \in \mbox{Nul}(A)$.
c. The vector $\mathsf{v} = [0,1,2]^{\top}$ is an eigenvector for $\lambda=1$. So this is a steady-state vector. (However, the dynamical system will not converge to this steady state because $\lambda=6$ is the dominant eigenvalue.)
d. When $A=P D P^{-1}$, we can find the coordinates of a vector with respect to the eigenbasis via multiplication by $P^{-1}$.
  
```{r, echo=TRUE}
Pinv =cbind(c(-1/6,0,1/6),c(-1/15,1/5,-1/3),c(1/30,2/5,1/6))
v = c(1,2,3)

Pinv %*% v
```

So  $[ \mathsf{v}]_{\mathcal{B}} = [-1/5, 8/5, 0]^{\top}$.


e. $-\frac{1}{5} \cdot 6^{2020} \cdot \begin{bmatrix} -5 \\ -2 \\ 1 \end{bmatrix}  + \frac{8}{5}  \cdot \begin{bmatrix} 0 \\ 1 \\ 2 \end{bmatrix}$
  
  

###

This system scales by $\sqrt{1+1} = \sqrt{2}$ and it rotates by $\tan^{-1} (1/1) = \pi/4$. 


###

We have
$$
\begin{bmatrix} a & -b \\ b & a \end{bmatrix} = \begin{bmatrix} .97 & -.71\\ .71 & .97 \end{bmatrix} 
$$

Let's turn to RStudio
```{r, echo=TRUE}
a = .97
b = .71

scale = sqrt(a^2+b^2)
angle = atan (b/a)

scale
angle

2 * pi / angle
```

a. It takes 10 iterations to rotate past the $x$-axis. 
b. We are further from the origin because $| \lambda| \approx 1.2 > 1$.



###
 
a. The matrix $A$ is diagonalizable because it has 3 distinct eigenvalues
b. We must see whether $\lambda=4$ has geometric multiplicty 2 (to match its algebraic multiplicity).

```{r, echo=TRUE}
rref( cbind(c(-1,-1,2), c(-1,-1,2), c(2,2,-4)))
```

We see that $B - 4I$ has two free columns, so $\dim ( \mbox{Nul}(B-4I))=2$. This means that $\lambda=4$ has geometric multiplicity 2. Therefore $B$ is diagonalizable.


###
 
a. We set $P = \begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix}$. So
$$
P^{-1} =  - \frac{1}{5} \begin{bmatrix} -1 & -1 \\ -3 & 2 \end{bmatrix}
= \begin{bmatrix} 0.2 & 0.2 \\ 0.6 & -0.4 \end{bmatrix}
$$
Or we can find this inverse using RStudio.
```{r, echo=TRUE}
A = cbind(c(2,3),c(1,-1))
solve(A)
```

Therefore 
$$
A = \begin{bmatrix} 0.7 & 0.2 \\ 0.3 & 0.8 \end{bmatrix} = 
\begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix}
\begin{bmatrix} 1 & 0 \\ 0 & 0.5 \end{bmatrix}
\begin{bmatrix} 0.2 & 0.2 \\ 0.6 & -0.4 \end{bmatrix}
$$

b. $\lim_{n \rightarrow \infty} A^n = \begin{bmatrix} 0.4 & 0.6 \\ 0.4 & 0.6 \end{bmatrix}$ because $\lambda=1$ is the dominant eigenvalue. So each column of $A$ convergence to vector in the eigenspace for the dominant eigenvalue. Basically, we treat each column of $A$ as a "starting vector" repeated multiplication by $A$ converges to the dominant eigenspace. Here are the details.

Let $\mathsf{v}_1 = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$ and $\mathsf{v}_2 = \begin{bmatrix} 1 \\ -1 \end{bmatrix}$. Let's see why we can treat each column $\mathsf{a}_1, \mathsf{a}_2$ of $A$ as a "starting vector." We can choose to view
$$A^2 = A \begin{bmatrix} \mathsf{a}_1 &  \mathsf{a}_2  \end{bmatrix}  =  \begin{bmatrix} A \mathsf{a}_1 & A \mathsf{a}_2  \end{bmatrix}
\quad \mbox{and} \quad 
A^3 = A \begin{bmatrix} A \mathsf{a}_1 & A \mathsf{v}_2  \end{bmatrix} =  \begin{bmatrix} A^{2} \mathsf{a}_1 & A^{2} \mathsf{v}_2  \end{bmatrix}
$$
and in general
$$
A^{n} = \begin{bmatrix} A^{n-1} \mathsf{a}_1 & A^{n-1} \mathsf{a}_2  \end{bmatrix}.
$$
Therefore
$$
\lim_{n \rightarrow \infty} A^n = \lim_{n \rightarrow \infty} \begin{bmatrix} A^{n-1} \mathsf{a}_1 & A^{n-1} \mathsf{a}_2  \end{bmatrix}.
$$
  
  
So we need to write the columns of $A$ as linear combinations of the eigenvectors $\mathsf{v}_1, \mathsf{v}_2$.  For any constants $c_1, c_2$, we have
$$
\lim_{n \rightarrow \infty} A^n (c_1 \mathsf{v}_1 + c_2 \mathsf{v}_2) 
= \lim_{n \rightarrow \infty} \left( c_1 A^n  \mathsf{v}_1 + c_2 A^n \mathsf{v}_2 \right)
= \lim_{n \rightarrow \infty} \left( c_1   \mathsf{v}_1 + c_2 (0.5)^n \mathsf{v}_2 \right)
= c_1   \mathsf{v}_1.
$$
So we need to find $c_1,c_2$ for each column of $A$. Let's use R Studio.

```{r, echo=TRUE}
a1 = c(.7,.3)
a2 = c(.2,.8)
P = cbind(c(2,3), c(1,-1))

solve(P,a1)
solve(P,a2)
```
In each case, $c_1 = 0.2$. Therefore
$$
\lim_{n \rightarrow \infty} A^n = \begin{bmatrix} 0.4 & 0.4 \\ 0.6 & 0.6 \end{bmatrix}.
$$


c. We need to find the coefficients for $x_0 = [25, 0]^{\top}$.

```{r, echo=TRUE}
P = cbind(c(2,3), c(1,-1))
v = c(25,0)
solve(P,v)
```

So the formula is
$$
5 \begin{bmatrix} 2 \\ 3 \end{bmatrix} + 15 \left( \frac{1}{2}  \right)^n \begin{bmatrix} 1 \\ -1 \end{bmatrix} 
$$

d. This converges to $5 \begin{bmatrix} 2 \\ 3 \end{bmatrix}$.


###
 
a.  We have
$$
A = \left[
\begin{array}{cc}
 \frac{1}{2} & \frac{1}{5} \\
 -\frac{2}{5} & \frac{9}{10} \\
\end{array}
\right]
=
\begin{bmatrix}
-1/2 & 1/2 \\ 0 & 1
\end{bmatrix}
\begin{bmatrix}
0.7 & -0.2 \\ 0.2 & 0.7
\end{bmatrix}
\begin{bmatrix}
-2 & 1 \\ 0 & 1
\end{bmatrix}
$$
Here are some R calculations to check the answer for (a) and to find the values for (b) and (c).

```{r, echo=TRUE}

A = cbind(c(1/2,-2/5), c(1/5,9/10))
A
eigen(A)

P = cbind(c(-1/2,0),c(1/2,1))
C = cbind(c(.7,.2),c(-.2,.7))
Pinv = solve(P)

Pinv

P %*% C %*% Pinv

atan(.2/.7)
sqrt(.7^2 + .2^2)

```

b. The angle of rotation is $\tan^{-1} (.2/.7) = 0.278$ radians
c. The dilation factor is $\sqrt{0.49 + 0.04} = \sqrt{0.53} = 0.728$.



###

Let's use RStudio.

```{r, echo=TRUE}
A = cbind(c(0.662, 0.338),c(0.25,  0.75))
A %*% A %*% c(0,1)

v1 = c(-0.595, -0.804 )
v1/sum(v1)
```

a. If Monday is dry, then  the probability of a wet Wednesday is $0.353$. The easiest way to calculate this $A^2 \begin{bmatrix} 1 \\ 0 \end{bmatrix}.$
b. In the long run, $42.5\%$ of days are wet  and $57.5\%$ of days are dry.




###


a. Here is the Leslie matrix, as well as some eigensystem computations.
```{r, echo=TRUE}
L = cbind(c(0,.8,0),c(0,0,.9),c(.4,0,.8))
L
vecs = eigen(L)$vectors
v = vecs[,1]
Re(v/sum(v)) # get it to sum to 1 AND remove the 0 imaginary part
```

b. The eigenvalues are $1.058, -0.129 \pm 0.506 i$.
c. If we start outside of the span of the dominant eigenvalue, then the trajectory will wiggle with a mild oscillation with an overall growth trend of $1.058$, or $5.8\%$.
d. The size of the herd grows. The proportions are $[0.227, 0.172, 0.601]$.

###


a. $(A + B) v = A v + B v = \lambda v + \mu v = (\lambda + \mu) v$, so yes, $v$ is an eigenvector of $A+B$ of eigenvalue $\lambda + \mu$.

b. $A B v = A (B v) = A (\mu v) = \mu (A v) = \mu \lambda v$, so  yes, $v$ is an eigenvector of $AB$ of eigenvalue $\lambda\mu$.

### 

a. We are given $A v = \lambda v$. Thus,

$$
\begin{array}{cccl}
A v & = & \lambda v & \text{given} \\
A^{-1} A v & = & \lambda A^{-1} v & \text{multiply on the left by $A^{-1}$} \\
v & = & \lambda A^{-1} v \\
\frac{1}{\lambda} v & = &  A^{-1} v \\
\end{array}
$$
This shows that $A^{-1} v = \frac{1}{\lambda} v$ so $v$ is an eigenvector of $A^{-1}$ with eigenvalue $\frac{1}{\lambda}$

b. (method 1)  If $A$ is diagonal, then there is a basis $\{v_1, v_2, \ldots, v_n\}$ of eigenvectors of $A$ with eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$. By the previous part, $\{v_1, v_2, \ldots, v_n\}$ are eigenvectors of $A^{-1}$ with eigenvalues $1/\lambda_1, 1/\lambda_2, \ldots, 1/\lambda_n$. Thus $A^{-1}$ has the same eigenbasis, and the diagonalization of $A^{-1}$ is
$$
A^{-1} = 
\underbrace{
\begin{bmatrix} 
\vert &\vert &&\vert \\
v_1 & v_2 & \cdots & v_n \\
\vert &\vert &&\vert \\
\end{bmatrix}
}_P
\begin{bmatrix} 
1/\lambda_1 & & & \\
&  1/\lambda_2 & & \\
&    & \ddots \\
& &   &   1/\lambda_n \\
\end{bmatrix} P^{-1}
$$
(method 2) If $A = P D P^{-1}$ then by the fact that the order reverses when computing inverses (the shoes-and-socks property), we have $A^{-1} = (P D P^{-1})^{-1} = (P^{-1})^{-1} D^{-1} P^{-1} = P D^{-1} P^{-1}.$ Furthermore $D^{-1}$ is a diagonal matrix such that
$$
\text{if} \qquad D = 
\begin{bmatrix} 
\lambda_1 & & &  \\
&  \lambda_2 & &  \\
&    & \ddots & \\
&   &   & \lambda_n \\
\end{bmatrix}
\qquad\text{then}\qquad
D^{-1} = 
\begin{bmatrix} 
1/\lambda_1 & & &  \\
&  1/\lambda_2 & &  \\
&   & & \ddots & \\
&   & &  & 1/\lambda_n \\
\end{bmatrix}
$$
Note that $A$ is invertible, 0 is not an eigenvalue, so each $1/\lambda_i$ does not cause division by 0.

 
###

We are given that $A w = 5 w$ and $A v = -3 v$.

a. $A (v + w) = A v + A w = -3 v + 5 w \not = \lambda(v + w)$ for any $\lambda$, so $v + w$ is not an eigenvector of $A$. Note: it would be if they had the same eigenvalue.

b. $A (2021 v) = 2021 A v = 2021 (-3) v = (-3) (2021 v)$ so $2021 v$ is an eigenvector also of eigenvalue $-3$.

c. $A^2 w = A (A w) = A (5 w) = 5 (A w) = 5 (5 w) = 25 w$, so $w$ is an eigenvector of $A^2$ of eigenvalue 25.

d. $(A - 20201I_n)v = A v - 2021 I_n v = -3 v - 2021 v = -2024 v$, so $v$ is an eigenvector of $(A - 20201I_n)$ of eigenvalue $-2024$.




<!--chapter:end:q3-review.Rmd-->



# Quiz 4 Review


## Overview

Our fourth quiz has two goals

1. It will cover the new material from Sections 6.1-6.5.
2. It will have some comprehensive material. 

The comprehensive material will not be tricky. My goal is to be sure that you understand the fundamentals of the course. 


The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice:

* Make sure that you have mastered the Vocabulary, Skills and Concepts  listed below.
* Look over the Edfinity homework assingments
* Do practice problems from the Edfinity Practice assignments. These allow you to "Practice Similar" by generating new variations of the same problem.
* Redo the Jamboard problems
* Try to resolve the Problem Sets and compare your answers to the solutions.
* Do the practice problems below. Compare your answers to the solutions.

## Vocabulary, Concepts and Skills

See the [Week 7-8 Learning Goals](week-7-8-learning-goals) for the list of vocabulary, concepts and skills.

## Comprehensive Review

Here are some terms and ideas that you should know:

* Pivot position.
* Elementary row operations.
* Ways to compute and think about $A v$ in both words and symbols.
* Linear combination.
* $Span(v_1, . . . , v_k).$ In particular, you should be able to visualize $Span(v)$, $Span(u,v)$ and give geometric interpretations of these sets  in $\mathbb{R}^2$ or $\mathbb{R}^3$.
* Linearly independent and linearly dependent.
* Linear transformation.
* Domain, codomain, image, range, onto, and one-to-one.
* The transpose of a matrix, the inverse of a matrix, invertible matrix.
* Subspace.
* Null space, column space, and row space of a matrix.
* Kernel and range of a linear transformation.
* Basis and dimension. 
* Rank.
* Eigenvalue, eigenvector, eigenspace.
* Characteristic polynomial and characteristic equation.
* Diagonalizable matrix.
* Dot product, length of a vector, angle between vectors, cosine similarity
* Orthogonal vectors, orthogonal spaces.
* Orthogonal complement of a subspace.
* Orthogonal basis, orthonormal basis, orthogonal matrix.
* Orthogonal projection, least squares solutions.


## Skills

* Form an augmented matrix and reduce a matrix or augmented matrix into row echelon or reduced row echelon form. Determine whether a given matrix is in either of those forms. Determine whether a particular form of a matrix is a possible row echelon or reduced echelon form.
*
Determine whether a system is consistent and if it has a unique solution. Write the general solution in parametric vector form. Describe the set of solutions geometrically.

* Interpret a system of equations as  (i) a vector equation (ii) a matrix equation.
*
Determine when a vector is in a subset spanned by specified vectors.
Exhibit a vector as a linear combination of specified vectors.
Determine whether a specified vector is in the range of a linear transformation.
*
Determine whether the columns of an $m \times n$ matrix span $\mathbb{R}^m$.
Determine whether the columns are linearly independent.
*
Compute a matrix-vector product, and interpret it as a linear combination of the columns of $A$. Use linearity of matrix multiplication to compute $A(u + v)$ or $A(c u)$.
* Find the  matrix of a linear transformation.
* Determine whether a transformation is linear.
Determine whether a linear transformation $T( x)=A x$ is one-to-one or onto, using the properties of the matrix $A$.
* Determine whether a subset of vectors is a subspace.
* Determine whether a set of vectors is linearly independent and whether it is a basis for some subspace or vector space. Find a basis for a vector space, or for the null space or column space of a matrix, or for the kernel or range of a linear transformation. Find the dimension of a vector space or subspace. Find the dimension of the null space (number of free variables) and column space (number of pivot columns) of a matrix.
* Find and interpret the rank of a matrix.
* Calculate the characteristic equation, eigenvalues, and eigenvectors of a square matrix. Find eigenvectors for a specific eigenvalue. Check if a vector is an eigenvector of a given matrix.
* Determine whether a square matrix is diagonalizable. Factor a diagonalizable matrix into $A=PDP^{-1}$, where $D$ is a diagonal matrix.
* Use eigenvalues and eigenvectors to analyze the long-term behavior of discrete dynamical systems.
* Find the length of a vector, the distance between two vectors, or the angle between two vectors. 
* Determine whether a set of vectors are orthogonal. Determine whether a vector is orthogonal to a subspace or whether two subspaces are orthogonal, by checking whether their basis vectors are orthogonal. 
* Find the orthogonal projection of (i) one vector onto another vector, or (ii) one vector onto a subspace (using an orthogonal basis for the subspace). Find the distance between a vector and a space (by computing the residual).
* Set up the matrix equation to find the ``best-fitting'' function to a set of data using least squares. Interpret the normal equations $A^{\top}A x = A^{\top}b$. Find the least squares approximation by solving the normal equations $\hat{x}=(A^{\top}A)^{-1}A^{\top}b$. 



## Practice Problems


###

Let $\mathsf{v} =  \begin{bmatrix}1 \\ -1 \\ 1 \end{bmatrix}$ and $\mathsf{w}= \begin{bmatrix}5 \\ 2 \\ 3 \end{bmatrix}$.


a. Find $\| \mathsf{v} \|$ and $\| \mathsf{w} \|$.



b. Find the distance between $\mathsf{v}$ and $\mathsf{w}$.

c. Find the cosine of the angle between $\mathsf{v}$ and $\mathsf{v}$.

d. Find $\mbox{proj}_{\mathsf{v}} \mathsf{w}$. 

e. Let $W=\mbox{span} (\mathsf{v}, \mathsf{w})$. Use the residual from the previous projection to create an orthonormal basis $\mathsf{u}_1, \mathsf{u}_2$ for $W$ such that $\mathsf{u}_1$ is a vector in the same direction as $\mathsf{v}$.


###

Let $\mathsf{u} \neq 0$ be a vector in $\mathbb{R}^n$. Define the function $T: \mathbb{R}^n \rightarrow \mathbb{R}^n$ by
$T(\mathsf{x}) = \mbox{proj}_{\mathsf{u}} \mathsf{x}$. Recall that the kernel of $T$ is the subspace $\mbox{ker}(T) = \{ \mathsf{x} \in \mathbb{R}^n \mid T(x) = \mathbf{0} \}$. Describe  $\mbox{ker}(T)$ as explicitly as you can.




### 

 The vectors $\mathsf{u}_1, \mathsf{u}_2$  form an orthonormal basis of a subspace $W$ of $\mathbb{R}^4$. Find the projection of $\mathsf{v}$ onto $W$ and determine how close $\mathsf{v}$ is to $W$.
$$
\mathsf{u}_1 = \frac{1}{2}\begin{bmatrix} 1\\ -1\\ -1\\ 1 \end{bmatrix}, \quad
\mathsf{u}_2 =  \frac{1}{2}\begin{bmatrix} 1\\ -1\\ 1\\ -1  \end{bmatrix}, \quad
\mathsf{v} =  \begin{bmatrix}   2\\ 2\\ 4\\ 2 \end{bmatrix} 
$$

###



Consider vectors 
$\mathsf{v}_1 = \begin{bmatrix} 1 \\ 1 \\-1 \end{bmatrix}$ and 
$\mathsf{v}_2= \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}$ in $\mathbb{R}^3$.
Let $W=\mbox{span}(\mathsf{v}_1, \mathsf{v}_2)$.

a.  Show that $\mathsf{v}_1$ and $\mathsf{v}_2$ are orthogonal.

b.  Find a basis for $W^{\perp}$.

c. Use orthogonal projections to find the representation of 
$\mathsf{y} = \begin{bmatrix} 8 \\ 0 \\ 2 \end{bmatrix}$ as
$\mathsf{y} = \hat{\mathsf{y}} + \mathsf{z}$ where
$\hat{\mathsf{y}} \in W$ and $\mathsf{z} \in W^{\perp}$.



###
Let $W$ be the span of the vectors
$$
\begin{bmatrix}
1 \\ -2 \\ 1 \\ 0 \\1 
\end{bmatrix}, \quad
\begin{bmatrix}
-1 \\ 3 \\ -1 \\ 1 \\ -1 
\end{bmatrix}, \quad
\begin{bmatrix}
0 \\ 0 \\ 1 \\ 3 \\1 
\end{bmatrix}, \quad
\begin{bmatrix}
0 \\ 2 \\ 0 \\ 0 \\4 
\end{bmatrix}
$$

a. Find a basis for $W$. What is the dimension of this subspace?
b. Find a basis for $W^{\perp}$



###

Consider the system $A \mathsf{x} = \mathsf{b}$ given by
$$
\begin{bmatrix}
1 & 1 & 1 \\
1 & 2 & -1 \\
1 & 1 & -1 \\
1 & 2 & 1
\end{bmatrix}
\begin{bmatrix}
x_1\\ x_2 \\ x_3
\end{bmatrix}
=
\begin{bmatrix}
4\\ 1 \\ -2 \\ -1
\end{bmatrix}.
$$

a. Show that this system is inconsistent.
b. Find the projected value $\hat{\mathsf{b}}$,  and the residual $\mathsf{z}$.
c. How close is your approximate solution to the desired target vector?


###

Here is an inconsistent system of equations:
$$
\begin{bmatrix} 1 & 2 \\ 1 & 2 \\ 1 & -1 \end{bmatrix}
 \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 
  \begin{bmatrix} 6\\ 4 \\ -4 \end{bmatrix} 
$$

a. State the normal equations for this problem (be sure to do all of the necessary matrix multiplications).

b. Find the least squares solution to the problem.

c. How close is your approximate solution to the desired target vector?



###
According to the [COVID Tracking Project](https://covidtracking.com/), Minnesota had $54,463$ positive COVID-19 cases between March 6 and 31 July, 2020. As of 16 December, 2020, that count has reached $386,412$. 

The vector `covid.mn` lists the total number of new COVID-19 cases in Minnesota between August 1, 2020 and December 16, 2020 (on top of the previously reported $54,463$).
 

```{r, echo=TRUE}

covid.start = 54463

covid.mn = c(725, 1484, 2097, 2699, 3316, 4177, 4722, 5638, 6435, 7053, 7376, 7840, 8530, 9260, 9950, 10689, 11253, 11598, 12155, 12845, 13670, 14404, 15121, 15835, 16244, 16773, 17927, 18777, 19794, 20726, 21401, 21892, 22622, 23660, 24503, 25417, 26124, 26762, 27145, 27405, 27786, 28253, 29125, 29848, 30486, 30888, 31350, 32259, 33344, 34258, 35554, 36479, 36959, 37637, 38549, 39726, 41196, 42271, 43175, 43984, 44671, 45737, 46903, 48324, 49363, 50336, 51277, 52188, 53459, 54849, 56365, 57805, 58976, 60111, 61480, 62643, 64933, 66627, 68349, 69976, 71068, 72128, 73689, 75400, 77659, 79339, 80909, 83073, 84981, 87848, 91002, 94009, 96209, 99157, 102633, 106460, 110402, 115844, 120491, 126399, 130325, 135218, 140107, 147332, 152876, 161565, 169118, 176555, 182486, 187580, 195443, 202237, 208489, 215694, 222037, 228453, 234840, 234840, 240538, 249560, 258506, 264300, 267849, 273014, 279163, 284510, 290818, 296399, 301689, 304740, 309256, 312755, 316505, 320935, 324360, 327378, 329701, 331949)

```
a. Find the best fitting exponential function $f(t) = a e^{k t}$ for the number of  COVID-19 cases in Minnesota since 31 July, 2020. Here $t$ is the number of days since 31 July.


b. Run the following code to plot your function. This code assumes that your least squares solution is given by `xhat`. Does it look like a good fit?

```{r}
xhat = c(0,0)  #this is wrong
a = exp(xhat[1])
k = xhat[2]
x = 0:(length(covid.mn)-1)

f=function(y){a * exp(k*(y))}

plot(x,f(x)+ covid.start,type="l",lwd=3,ylab="new positive COVID-19 cases", xlab="days since July 31, 2020", main="best fit exponential function")
points(x,covid.mn + covid.start,pch=20,cex=.7,col="red")
```


###

Consider the symmetric matrix 
$$
A = \begin{bmatrix}
   3  &  0 & 34  &  3 \\
   0  &  6 & -34  &  0 \\
  34 & -34 &  74  & 34 \\
   3  &  0  & 34  &  3
\end{bmatrix}
$$


a. Use RStudio to find the eigenvalues $\lambda_1 > \lambda_2 > \lambda_3 > \lambda_4$ and their corresponding eigenvectors $\mathsf{v}_1, \mathsf{v}_2, \mathsf{v}_3, \mathsf{v}_3$. Confirm that these eigenvectors form an orthonormal set.

b. Is the linear transformation $T(\mathsf{x}) = Ax$ invertible? How do you know?

c. Confirm that 
$$
A = \lambda_{1} \mathsf{v}_1 \mathsf{v}_1^{\top} + \lambda_{2} \mathsf{v}_2 \mathsf{v}_2^{\top} + \lambda_{3} \mathsf{v}_3 \mathsf{v}_3^{\top} + \lambda_{4} \mathsf{v}_4 \mathsf{v}_4^{\top}.
$$

d. Use your answer in part (d) to find the best rank 2 approximation for $A$. (Be careful!)


###

Here is a matrix $A$ and its reduced row echelon form $B$
$$
A = \begin{bmatrix}
   1  &  3  & -3  &  1  &  0 \\
    2  &  1  &  0  &  6 &   5 \\
   3  &  3  & -3  &  6  &  3 \\
  -1  &  4  & -3  & -3  & -1
\end{bmatrix}
\qquad \longrightarrow \qquad
B = \begin{bmatrix}
    1 &   0  &  0 & 2.5 & 1.5 \\
    0  &  1  &  0 & 1.0 & 2.0 \\
  0  & 0  &  1 & 1.5 & 2.5 \\
   0  &  0  &  0 & 0.0 & 0.0
\end{bmatrix}.
$$


a. Find a basis for $\mbox{Nul}(A)$ and $\mbox{Col}(A)$.

b. Is the linear transformation $T(\mathsf{x}) = A \mathsf{x}$ one-to-one? Onto?

c. How is the SVD for $A$ related to the SVD for $B$? What properties will they share? What properties will be different? Make some conjectures.

d. Now find the SVD of both $A$ and $B$, and test your conjectures. Compare the singular values, the right singular vectors and the left singular vectors. Be sure to compare each of the four fundamental subspaces: $\mbox{Nul}(M), \mbox{Col}(M), \mbox{Row}(M), \mbox{Nul}(M^{\top})$. 

##  
$\mathsf{A}$  is a $4 \times 5$ matrix and $b \in \mathbb{R}^4$. The augmented matrix $[\,\mathsf{A}\mid b\,]$ row reduces as shown here. 
$$
[\,\mathsf{A}\mid b\,] = 
\left[
\begin{array}{ccccc|c}
 \vert & \vert & \vert & \vert & \vert &\vert\\
 v_1 & v_2 &  v_3 &  v_4 &  v_5 & b\\
 \vert & \vert & \vert & \vert & \vert &\vert\\
 \end{array}
\right] 
  \longrightarrow
\left[
\begin{array}{rrrrr|r}
 1 & 0 & 2 & 0 & -1 &1\\
 0 & 1 & -1 & 0 & -1 &1\\
 0 & 0 & 0 & 1 & 1 &-2\\
 0 & 0 & 0 & 0 & 0 & 0
 \end{array}
\right]  
$$
a. Give all of the solutions to o $\mathsf{A} x = b$ in parametric form.
 
b. Give a dependence relation among the columns of $\mathsf{A}$.

c. These true-false questions refer to the coefficient matrix $\mathsf{A}$ above. Decide if the statement is **T** = True or **F** = False. No justification necessary. 

+ $\mathsf{A} x = b$ has a solution for all $b \in \mathbb{R}^4$.
+  The columns of $\mathsf{A}$ span $\mathbb{R}^4$.
+ If $\mathsf{A} x = b$ has a solution, then it has infinitely many solutions.
+  The linear transformation $x \mapsto \mathsf{A} x$ is one-to-one
+ The linear transformation $x \mapsto \mathsf{A} x$ is onto


\newcommand{\A}{\mathsf{A}}
\newcommand{\x}{\vec{\mathsf{x}}}
\newcommand{\b}{\vec{\mathsf{b}}}
\newcommand{\y}{\vec{\mathsf{y}}}
\newcommand{\0}{\vec{\mathbf{0}}}
\newcommand{\RR}{\mathbb{R}}

##  

Suppose that  $\A$ is an $n \times n$ matrix and that $\x_1$ and $\x_2$ are two solutions to $\A x = \b$ with $\b \not= \0$ and $\x_1 \not= \x_2$.

a. Give a nonzero solution to $\A \x = \0$.


b. Give a solution to $\A \x = \b$ that no one else in the class has.

Decide if the statement is **T** = True or **F** = False, or  **I**  = there is not enough information to know.

+ The equation $\A x = \y$ has a solution for all $\y \in \RR^n$
+ $\lambda = 0$ is an eigenvalue of $\A$
+ $\A$ is invertible
+ $\A$ is diagonalizable



## Watch this! 

The answer to at least one question on Quiz 4 is contained in [this video](https://www.youtube.com/watch?v=O4KCoNvRi6Y).

## Solutions to Practice Problems

```{r}
require(pracma)
```

###

a.
\begin{align}
\| \mathsf{v} \| &= \sqrt{ \mathsf{v} \cdot \mathsf{v}} = \sqrt{1+1+1} = \sqrt{3} \\
\| \mathsf{w} \| &= \sqrt{ \mathsf{w} \cdot \mathsf{vw}} = \sqrt{25+4+9} = \sqrt{38} \\
\end{align}

b. We have $\mathsf{v} - \mathsf{w} = \begin{bmatrix} -4 \\ -3 \\ -2 \end{bmatrix}$ and so
$$
\| \mathsf{v} - \mathsf{w}\|  = \sqrt{16+9+4} = \sqrt{29} 
$$
c. 
$$ \cos \theta = \frac{\mathsf{v} \cdot \mathsf{w}}{\| \mathsf{v}  \| \, \|\mathsf{w} \| }
= \frac{5-2+3}{\sqrt{3} \, \sqrt{38} } = \frac{2\sqrt{3}}{\sqrt{38} }
$$

d. 
$$
\hat{\mathsf{w}} = \mbox{proj}_{\mathsf{v}} \mathsf{w} = \frac{\mathsf{v} \cdot \mathsf{w}}{ \mathsf{v} \cdot \mathsf{v} } \, \mathsf{v} = \frac{5-2+3}{1+1+1}  \mathsf{v}  = 2 \mathsf{v} = 
\begin{bmatrix}
2 \\ -2 \\ 2 
\end{bmatrix}
$$
e. Using $\hat{\mathsf{w}}$ from the previous problem, we know that 
$$
\mathsf{z} = \mathsf{w} - \hat{\mathsf{w}} = \begin{bmatrix} 5 \\ 2 \\ 3 \end{bmatrix}  -  \begin{bmatrix} 2 \\ -2 \\ 2  \end{bmatrix} =
\begin{bmatrix} 3 \\ 4 \\ 1  \end{bmatrix}
$$ 
is orthogonal to $\mathsf{v}$.So an orthonormal basis is
$$
\frac{1}{\sqrt{3}} \begin{bmatrix}
1 \\ -1 \\ 1
\end{bmatrix} \quad \mbox{and} \quad
\frac{1}{\sqrt{26}} \begin{bmatrix}
3 \\ 4 \\ 1
\end{bmatrix}
$$

###

 Here are a few ways to describe $\mbox{ker}(T)$.
    + $\mbox{ker}(T) = \{ \mathsf{x} \in \mathbb{R}^n \mid \mathsf{x} \cdot \mathsf{u} = 0 \}$.
    + $\mbox{ker}(T)$ is the set of vectors that are orthogonal to $\mathsf{u}$.
    + Let $A$ be the $1 \times n$ matrix $\mathsf{u}^{\top}$. Then $\mbox{ker}(T)= \mbox{Nul}(A)$.


###

We have $\mathsf{u}_1 \cdot \mathsf{v} = 2-2-4+2=-2$ and $\mathsf{u}_1 \cdot \mathsf{v} = 2-2+4-2=2$ so
$$
\hat{\mathsf{v}} = \mbox{proj}_W \mathsf{v} = -2 \mathsf{u}_1 + 2 \mathsf{u}_2 = 
\begin{bmatrix}
1 \\ -1 \\ -1 \\ 1 
\end{bmatrix}
+
\begin{bmatrix}
1 \\ -1 \\ 1 \\ -1 
\end{bmatrix}
=
\begin{bmatrix}
2 \\ -2 \\ 0 \\ 0 
\end{bmatrix}
$$
with residual vector 
$$
\mathsf{z} = \mathsf{v} - \hat{\mathsf{v}}
= \begin{bmatrix}
2 \\ 2 \\ 4 \\ 2 
\end{bmatrix}
-
\begin{bmatrix}
2 \\ -2 \\ 0 \\ 0 
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 6 \\ 4 \\ 2 
\end{bmatrix}
$$
and the distance is $\| \mathsf{z} \| = \sqrt{36 + 16 + 4} = \sqrt{56}$.

###

a. $\mathsf{v}_1 \cdot \mathsf{v}_2 = 1 +2 - 3 =0$.

b. We must find $\mbox{Nul}(A)$ where 
$A = \begin{bmatrix} \mathsf{v}_1^{\top} \\ \mathsf{v}_2^{\top}\end{bmatrix}$.

$$
\begin{bmatrix}
1 & 1 & -1 \\
1 & 2 & 3
\end{bmatrix}
\longrightarrow
\begin{bmatrix}
1 & 1 & -1 \\
0 & 1 & 4
\end{bmatrix}
\longrightarrow
\begin{bmatrix}
1 & 0 & -5 \\
0 & 1 & 4
\end{bmatrix}
$$
so the vector $\begin{bmatrix} 5 \\ -4 \\ 1 \end{bmatrix}$ is a basis for $W^{\perp}$

c. We have
\begin{align}
\hat{\mathsf{y}} &=
\frac{\mathsf{y} \cdot \mathsf{v_1}}{\mathsf{v_1} \cdot \mathsf{v_1}} \, \mathsf{v_1} +
\frac{\mathsf{y} \cdot \mathsf{v_2}}{\mathsf{v_2} \cdot \mathsf{v_2}} \, \mathsf{v_2}
=
\frac{8-2}{1+1+1} \mathsf{v_1} + \frac{8+6}{1+4+9} \mathsf{v_2} \\
&=
2\mathsf{v_1} +\mathsf{v_2}
= \begin{bmatrix} 2 \\ 2 \\ -2 \end{bmatrix} + \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
= \begin{bmatrix} 3 \\ 4 \\ 1 \end{bmatrix}
\end{align}
and so 
$$
\mathsf{z} = \mathsf{y} - \hat{\mathsf{y}}
= \begin{bmatrix} 8 \\ 0 \\ 2 \end{bmatrix} - \begin{bmatrix} 3 \\ 4 \\ 1 \end{bmatrix}
= \begin{bmatrix} 5 \\ -4 \\ 1 \end{bmatrix}.
$$

###

a .We will answer this one using RStudio.

```{r, echo=TRUE}

A = cbind(c(1,-2,1,0,1), c(-1,3,-1,1,-1), c(0,0,1,3,1), c(0,2,0,0,4))
rref(A)

```
So we need all four vectors to span the column space.
  
b. We obtain a basis for $W^{\perp}$ by finding $\mbox{Nul(A^{\top})}$ So let's row reduce $A^{\top}$

```{r, echo=TRUE}
rref(t(A))
```

The vector  $\begin{bmatrix} 2 \\ -2 \\ -7 \\ 2 \\ 1\end{bmatrix}$ spans $W^{\perp}$


###

We will show that 
$\| \mathsf{v} \|^2 =  ( \mathsf{v} \cdot \mathsf{u}_1)^2 + (\mathsf{v} \cdot \mathsf{u}_2)^2 + \cdots +(\mathsf{v} \cdot \mathsf{u}_n)^2.$

Let's write $\mathsf{v}$ in terms of the orthonormal  basis: $\mathsf{v} = c_1 \mathsf{u}_1 + c_2 \mathsf{u}_2 + \cdots + c_n \mathsf{u}_n$. We then have 

\begin{align}
\| \mathsf{v} \|^2 &= \mathsf{v} \cdot \mathsf{v} \\
&= (c_1 \mathsf{u}_1 + c_2 \mathsf{u}_2 + \cdots + c_n \mathsf{u}_n) \cdot (c_1 \mathsf{u}_1 + c_2 \mathsf{u}_2 + \cdots + c_n \mathsf{u}_n) \\
&= c_1^2 + c_2 + \cdots + c_n^2
\end{align}
because $\mathsf{u}_i \cdot \mathsf{u}_i  =1$ and $\mathsf{u}_i \cdot \mathsf{u}_j=0$ for $i \neq j$. 

Finally, we note that $\mathsf{v} \cdot \mathsf{u_i} = c_i$ using the same facts about the dot projects for the orthonormal basis. So we have verified the claim above.


###

a.

```{r echo=TRUE}
A = cbind(c(1,1,1,1), c(1,2,1,2),c(1,-1,-1,1))
b  = c(4,1,-2,-1)
rref(cbind(A,b))
```

There is a pivot in the last column of this augmented matrix, so this system is inconsistent.

b.

Here is the least squares calculation.

```{r, echo=TRUE}
#solve the normal equation
(xhat = solve(t(A) %*% A, t(A) %*% b))

# find the projection
(bhat = A %*% xhat)

# find the residual vector
(z = b - bhat)

# check that z is orthogonal to Col(A)
t(A) %*% z

# measure the distance between bhat and b
sqrt( t(z) %*% z)

```

The projection is $\hat{\mathsf{b}} = [2,-1,0,1]^{\top}$. 
The residual is $\mathsf{z} = [2,2,-2,-2]^{\top}$

c. The distance of between $\mathsf{b}$ and  $\hat{\mathsf{b}}$ is 
$$
\| = \| \mathsf{z} \| = \sqrt{4+4+4+4} = \sqrt{16} = 4.
$$
###
a. 

```{r echo=TRUE}


x = 1:length(covid.mn)
y = log(covid.mn)

A = cbind(x^0, x)

(xhat = solve(t(A) %*% A, t(A) %*% y))





```

b.

```{r q4-covid, echo=TRUE}
a = exp(xhat[1])
k = xhat[2]

f=function(y){a * exp(k*(y))}

plot(x,f(x)+ covid.start,type="l",lwd=3,ylab="new positive COVID-19 cases", xlab="days since July 31, 2020", main="best fit exponential function")
points(x,covid.mn + covid.start,pch=20,cex=.7,col="red")

```

The curve is a pretty good fit (unfortunately?). However, it does look like the additional restrictions of the last two weeks are slowing the COVID spread.


###


a.


```{r, echo=TRUE}
A = cbind(c(3,0,34,3), c(0,6,-34,0), c(34,-34,74,34),c(3,0,34,3))
A
syst = eigen(A)

(P = syst$vectors)



zapsmall(P %*% t(P))


```

The eigenvectors are the columns of the second matrix $P$ shown above. The last matrix shows that $P P^{\top} = I$, so the columns are orthonormal.


b.

```{r}

zapsmall(syst$values)

```

$A$ is not invertible because 0 is an eigenvalue.


c.

```{r, echo=TRUE}

v1 = syst$vectors[,1]
v2 = syst$vectors[,2]
v3 = syst$vectors[,3]
v4 = syst$vectors[,4]

syst$values[1] * v1 %*% t(v1) + syst$values[2] * v2 %*% t(v2) + syst$values[3] * v3 %*% t(v3) + syst$values[4] * v4 %*% t(v4)

```


d. We must remember to use the eigenvalues of largest magnitude: these are
$\lambda_1 = 108$ and $\lambda_4 = -28$.

```{r echo=TRUE}
syst$values[1] * v1 %*% t(v1)  + syst$values[4] * v4 %*% t(v4)
```


###

a. This SVD factorization is $U \Sigma V^{\top}$.

    + $\mbox{Nul}(A) = \{ 0 \}$ so it has no basis.
    + An orthonormal basis for $\mbox{Col}(A)$  is the first three columns of $U$.
    + An orthonormal basis for $\mbox{Row}(A)$  is the three rows of $V^{\top}$.
    +An orthonormal basis for $\mbox{Nul}(A^{\top})$  is the  last two columns of $U$.
    
b. This is true because the zero vector is orthogonal to every vector.

c. The matrix $\Sigma$ has 3 pivots. So the nullspace of $A$ is trivial.

d. The 3D volume expands because the product of the singular values is greater than 1.

e.

```{r echo=TRUE}

11.4 * cbind(c(0.48,0.61,0.30,0.032,0.56)) %*% rbind(c(0.16,-0.98,0.062))

```



###

a. The null space is the span of $[-2.5,-1,-1.5,1,0]^{\top}$ and $[-1.5,-2,-2.5,0,1]^{\top}$.
The column space is the span of $[1,2,3,-1]^{\top}$ and $[3,1,3,4]^{\top}$ and $[-3,0,-3,-3]^{\top}$

b. This mapping is not one-to-one because the null space is two-dimensional. The mapping is not onto because there is no pivot in the final row of $B$.

c. This is a conceptual question without a particular "right answer." Here are some observations.
    + The nullspace of $A$ is the same as the nullspace of $B$, so an orthogonal basis for $\mbox{Nul}(A)$ is also an orthogonal basis for $\mbox{Nul}(B)$. However, the orthonormal basis vector in the SVD for $A$ do not need to be the same as the orthonormal basis in the SVD for $B$.
    + The columnspace of $A$ is different from the columnspace of $B$. However, each of them is three-dimensional.
    + The singular values for $A$ do not need to equal the singular values of $B$. Row reduction change the determinant of a square matrix. So it is safe to assume that it will have a similar effect on singular values. However, we konw that both will have 3 singular values since  $A$ and $B$ have the same rank.
    
d.

```{r echo=TRUE}

A = rbind(c(1,3,-3,1,0),c(2,1,0,6,5),c(3,3,-3,6,3),c(-1,4,-3,-3,-1))
B = rref(A)

A
B

```

```{r, echo=TRUE}

(Asvd = svd(A))
(Bsvd = svd(B))

# check that the singular values are different
Asvd$d
Bsvd$d


Arow = Asvd$v[,1:3]
Acol = Asvd$u[,1:3]

Brow = Bsvd$v[,1:3]
Bcol = Bsvd$u[,1:3]

# check that the rowspaces are the same
# this also means that the nullspaces are the same
rref(cbind(Arow, Brow))
rref(cbind(Brow, Arow))

# check that the columnspaces are different
rref(cbind(Acol, Bcol))



```



###


a.


<!--chapter:end:q4-review.Rmd-->


# Week 1 Learning Goals



Here are the knowledge and skills you should master by the end of this first, shorter week.


## Solving Linear Equations

I should be able to do the following tasks:

* Identify linear systems from nonlinear systems
* Create a linear system to solve a variety of applied scenarios
* Convert between a linear system and an augmented matrix
* Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF)
* Use REF to determine whether a linear system is consistent or inconsistent
* Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions
* Use RREF to find explicit equations for the solution set of a consistent system

## RStudio

I should be able to do the following tasks:

* Log in to [Macalester's RStudio server](http://rstudio.macalester.edu/)
* Upload R Markdown files to RStudio
* Knit R Markdown to produce HTML
* Use RStudio to create vectors and matrices
* Use the ``rref`` command from ``pracma`` to solve a linear system



## Vocabulary

I should know and be able to use and explain the following terms:

* elementary row operation (and be able to state them)
* augmented matrix
* REF and RREF
* pivot position
* basic variable (pivot variable)
* free variable
* consistent system and inconsistent system

## Conceptual Thinking

I should understand and be able to perform the following conceptual tasks:

* Model 2-dimensional linear systems as the intersections of lines
* Model 3-dimensional linear systems as the intersections of planes







<!--chapter:end:w1-solving-linear-equations.Rmd-->

# Week 2 Learning Goals

Here are the knowledge and skills you should master by the end the second week.


## Solution Sets, Span and Linear Independence

I should be able to do the following **tasks**:

* Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation $Ax = b$.
* Compute and understand the matrix-vector product $A x$ both as a linear combination of the columns of A and as the dot product of $x$ with the rows of $A$.
* Write the solution set to $Ax=b$ as a parametric vector equation.
* Determine whether a set of vectors is linearly dependent or independent
* Find a dependence relation among a set of vectors
* Decide if a set of vectors span $\mathbb{R}^n$

## Vocabulary

I should know and be able to use and explain the following **terms** or **properties**. 

* $A(x + y) = Ax + Ay$ and $A(c x) = c A x$
* homogeneous and nonhomogeneous equations
* parametric vector equations
* linear independence and linear dependence

## Conceptual Thinking

I should understand and be able to explain the following **concepts**:

* Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an $m \times n$ matrix $A$
    * For each $b \in  \mathbb{R}^m$, the system $A x = b$ has at least one solution
    * Each $b \in  \mathbb{R}^m$ is a linear combination of the columns of $A$
    * The columns of $A$ span $\mathbb{R}^m$
    * $A$ has a pivot in every row.
* Understand the relation between homogeneous solutions and nonhomogeneous solutions.
* Linear independence
* Span
* More than $n$ vectors in $\mathbb{R}^n$ must be linearly dependent.

<!--chapter:end:w2-solution-sets.Rmd-->

# Week 3 Learning Goals

Here are the knowledge and skills you should master by the end the third week.


## Linear Transformations and Matrix Inverses

I should be able to do the following **tasks**:

* Determine whether a mapping from $\mathbb{R}^n$ to $\mathbb{R}^n$ is a linear transformation.
* Use the RREF of the corresponding matrix to determine whether $T(\mathsf{x})$ is one-to-one and/or onto.
* Describe 2D linear transformations as a mixture of geometric operations, including expansion, contraction, reflection, rotation, shearing  and dimension reduction. 
* Perform a 2D translation using 3D homogeneous coordinates.
* Multiply an $m \times n$ matrix with an $n \times p$ matrix to get an $m \times p$ matrix.
* Determine whether a $2 \times 2$ matrix is invertible.
* Find the inverse of a $2 \times 2$ matrix by hand.
* Use RStudio to check for invertiblity and to find the inverse of an $n \times n$ square matrix.
* Explain the connection between Gaussian Elimination, elementary matrices, and the matrix inverse.



## Vocabulary

I should know and be able to use and explain the following **terms** or **properties**. 

* linear transformation: $T(a \mathsf{u}  + b \mathsf{v}) = a T(\mathsf{u})  + b T(\mathsf{v})$
* domain, codomain (aka target) and range (aka image)
* $T$ *maps* vector $\mathsf{x}$ to its *image* $T(\mathsf{x})$
* one-to-one
* onto
* standard matrix for a linear transformation
* homogeneous coordinates
* transpose of a matix
* invertible matrix
* elementary matrices

## Conceptual Thinking

I should understand and be able to explain the following **concepts**:

* A linear transformation $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ corresponds to multiplication by an $m \times n$ matrix $A$.
* $T(\mathsf{x})=\mathsf{A} \mathsf{x}$ is a one-to-one linear transformations if and only $\mathsf{A}$ has linearly independent columns
* $T(\mathsf{x})=\mathsf{A} \mathsf{x}$ is an onto linear transformations if  and only if the  columns of $\mathsf{A}$ span $\mathbb{R}^m$.
* The Invertible Matrix Theorem (Section 2.3, Theorem 8, page 112) is one of the highlights of the course! It gives 12 different conditions that all equivalent! You should think deeply about why everything comes together like this for square matrices.




<!--chapter:end:w3-learning-goals.Rmd-->


# Week 4 Learning Goals



Here are the knowledge and skills you should master by the end of the fourth week.


## Vector Spaces and the Determinant

I should be able to do the following **tasks**:

* Prove/disprove that a subset of a vector space is a subspace.
* Prove/disprove that a set of vectors is linearly dependent.
* Prove/disprove that a set of vectors span a vector space (or a subspace).
* Find the kernel and image of $T(\mathsf{x}) = Ax$.
* Determine whether a set of vectors is a basis.
* Find a basis for $\mathrm{Nul}(A)$ and a basis for $\mathrm{Col}(A)$.
* Find the change-of-coordinate matrix $P_{\mathcal{B}}$ from basis ${\mathcal{B}}$ to the standard basis $\mathcal{S}$.
* Use matrix inverses (and RStudio) to find the change-of-coordinate matrix $P_{\mathcal{B}}^{-1}$ from basis ${\mathcal{S}}$ to the standard basis $\mathcal{B}$.
* Find the coordinate vector with respect to a given basis.
* Find the dimension of a vector space (or subspace) by finding or verifying a basis.
* Find the determinant of a $2 \times 2$ matrix by hand.
* Find the determinant of a $3 \times 3$ matrix by using row operations/cofactor expansion/permutation method.
* Use RStudio to calculate the determinant of a square matrix.
* Use $\det(A)$ to decide whether the square matrix $A$ is invertible.

## Vocabulary

I should know and be able to use and explain the following **terms** or **properties**. 

* every one of [these Important Definitions][Important Definitions]
* subspace
* null space and column space of a matrix
* kernel and image of a linear transformation
* basis
* coordinate vector with respect to a basis
* change-of-coordinates matrix
* the coordinate vector with respect to a basis
* the dimension of a vector space (or a subspace)
* determinant

## Conceptual Thinking

I should understand and be able to explain the following **concepts**:

* A vector space consists of a collection of vectors and all of their linear combinations.
* A subspace is a subset of a vector space that is *also* a vector space by itself (closed under linear combinations).
* The solutions to $A \mathsf{x} = \mathbb{0}$ form a subspace.
* The span of the columns of $A$ form a subspace.
* How the kernel and image of $T(\mathsf{x}) = Ax$ correspond to the nullspace and columnspace of $A$.
* Every basis of a given vector space (or subspace) contains the same number of vectors.
* Why every vector in a vector space has a unique representation as a linear combination of a given basis ${\mathcal{B}}$.
* How dimension relates to span and linear independence.
* Interpret $\det(A)$ as a measure the expansion/contraction of "volumes" in $\mathbb{R}^n$ under the linear transformation $T(\mathsf{x})=A\mathsf{x}$.


<!--chapter:end:w4-learning-goals.Rmd-->


# Week 5-6 Learning Goals



Here are the knowledge and skills you should master by the end of the fifth and sixth weeks.


## Eigensystems

I should be able to do the following **tasks**:

* Check whether a given vector $\mathsf{v}$ is an  eigenvector for square matrix $A$.
* Find the eigenvalues of a matrix $2 \times 2$ matrix by hand, using the  characteristic equation
* Find the eigenvalues of a triangular matix by inspection.
* Given the eigenvalues of matrix $A$, find the eigenvectors by solving $(A - \lambda I) = \mathbf{0}$.
* Find the eigenvalues and eigenvectors of an  $n \times n$ matrix $A$ using `eigen(A)` on RStudio.
* Determine whether a matrix is diagonalizable.
* Factor a diagonalizable $n \times n$ matrix as $A = PDP^{-1}$ where $D$ is a diagonal matrix of eigenvalues and $P$ is the matrix whose columns are the corresponding eignvectors.
* Compute matrix powers using the diagonalization.
* Use RStudio to find complex eigenvalues and eigenvectors of a square matrix.

* Factor a $2 \times 2$ scaling-rotation matrix as $A = P C P^{-1}$ where $C$ is a  scaling-rotation matrix $\begin{bmatrix} a & -b \\ b & a \end{bmatrix}$  and $P = [ \mathsf{w}, \mathsf{u}]$ where $\mathsf{v} = \mathsf{u} + i \mathsf{w}$ is the eigenvector for $\lambda = a + b i$.

* Find the angle of rotation and the scaling factor in a $2 \times 2$ matrix with complex eigenvalues.

* Use the dominant eigenvalue and dominant eigenvector to determine the long-term behavior of a dynamical system.

* Use eigenvalues to investigate a population modeled with a Leslie matrix.

* Give a close-formula for a dynamical system using the eigen decomposition of a matrix

## Vocabulary

I should know and be able to use and explain the following **terms** or **properties**. 

* eigenvalue, eigenvector  and eigenspace
* characteristic equation
* diagonalizable matrix
* similar matrices
* algebraic multiplicity of an eigenvalue
* geometric multiplicity of an eigenvalue
* scaling-rotation matrix
* discrete dynamical system
* trajectory
* dominant eigenvalue  and dominant eigenvector
* population model
* Leslie matrix

## Conceptual Thinking

I should understand and be able to explain the following **concepts**:

* An eigenspace of $A$ is a subspace that is fixed under the linear transformation $T(\mathsf{x}) = A \mathsf{x}$.
* An eigenvalue $\lambda$ with $1 <| \lambda |$ corresponds to expansion.
* An eigenvalue $\lambda$ with $0 < | \lambda | < 1$ corresponds to contraction.
* A complex  eigenvalue  corresponds  to a rotation in a 2D subspace.
* The eigenspace for $\lambda$ is the subspace $E_\lambda = \mathrm{Nul}(A  - \lambda I)$.
* A matrix is  not diagonalizable when it has complex eigenvalues.
* A matrix is not diagonalizable when it has an eigenvalue whose algebraic mutiplicity is strictly larger than its geometrix multiplicity.

* The long-term behavior of a  dynamical system is determined by its dominant eigenvalue and eigenvector.
* Population model predicts one of: long term growth, extinction, convergence to a stable population.




<!--chapter:end:w5-learning-goals.Rmd-->


# Week 7-8 Learning Goals


Here are the knowledge and skills you should master by the end of the seventh and eighth weeks.


## Orthogonality and SVD


I should be able to do the following **tasks**:

* Find the length of a vector
* Find the distance between two vectors
* Normalize a vector
* Find the cosine of the angle between two vectors
* Find the orthogonal projection of one vector onto another
* Find the orthogonal projection of one vector onto a subspace (using an orthogonal basis)
* Find the orthogonal complement of a subspace
* Find the least squares approximation for an inconsistent system
* Formulate a curve fitting problem as an inconsistent linear system $A \mathsf{x} = \mathsf{b}$
* Orthogonally diagonalize a symmetric matrix as $A=PDP^{\top}$.
* Find the spectral decomposition $A = \lambda_1 \mathsf{v}_1 \mathsf{v}_1^{\top} + 
\lambda_2 \mathsf{v}_2 \mathsf{v}_2^{\top} + \cdots + \lambda_n \mathsf{v}_n \mathsf{v}_n^{\top}$ of a symmetric matrix $A$
* Use an orthogonal diagonalization to find the best rank $k$ approximation of symmetric matrix $A$

## Vocabulary

I should know and be able to use and explain the following **terms** or **properties**. 


* dot product of two vectors $\mathsf{v} \cdot \mathsf{w} = \mathsf{v}^{\top} \mathsf{w}$  (aka scalar product, inner product)
* length (magnitude) of a vector
* angle between vectors
* normalize
* unit vector
* orthogonal vectors
* orthogonal complement of a subspace
* orthogonal projection
* orthogonal basis
* orthonormal basis
* normal equations for a least squares approximation
* least squares solution
* residual vector
* symmetric matrix
* orthogonally diagonalizable
* outer product of two vectors $\mathsf{v} \, \mathsf{w}^{\top}$
* spectral decomposition of a symmetric matrix

## Conceptual Thinking

I should understand and be able to explain the following **concepts**:

* The dot product gives an algebraic encoding of the geometry (lengths and angles) of $\mathbb{R}^n$
* If two vectors are orthogonal, then they are perpendicular, or one of them is the zero vector
* An orthogonal projection is a linear transformation
* The row space of a matrix is orthogonal to its nullspace
* The inverse of orthogonal matrix $A$ is the transpose $A^{\top}$
* Cosine similarity is a useful way to compare vectors, especially in high-dimensional vector spaces.
* The residual vector measures the quality of fit of a least squares solution
* The outer product $\mathsf{v}\,  \mathsf{w}^{\top}$ is a square matrix with rank 1


<!--chapter:end:w7-learning-goals.Rmd-->

